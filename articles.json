[{"content": "Python is an interpreted high-level programming language for general-purpose programming. Created by Guido van Rossum and first released in 1991, Python has a design philosophy that emphasizes code readability, notably using significant whitespace. It provides constructs that enable clear programming on both small and large scales.[26]Python features a dynamic type system and automatic memory management. It supports multiple programming paradigms, including object-oriented, imperative, functional and procedural, and has a large and comprehensive standard library.[27]Python interpreters are available for many operating systems. CPython, the reference implementation of Python, is open source software[28] and has a community-based development model, as do nearly all of its variant implementations. CPython is managed by the non-profit Python Software Foundation.History[edit]Python was conceived in the late 1980s,[29] and its implementation began in December 1989[30] by Guido van Rossum at Centrum Wiskunde & Informatica (CWI) in the Netherlands as a successor to the ABC language (itself inspired by SETL)[31] capable of exception handling and interfacing with the Amoeba operating system.[7] Van Rossum remains Python's principal author. His continuing central role in Python's development is reflected in the title given to him by the Python community: Benevolent Dictator For Life (BDFL).On the origins of Python, Van Rossum wrote in 1996:[32]\n...In December 1989, I was looking for a hobby programming project that would keep me occupied during the week around Christmas. My office ... would be closed, but I had a home computer, and not much else on my hands. I decided to write an interpreter for the new scripting language I had been thinking about lately: a descendant of ABC that would appeal to Unix/C hackers. I chose Python as a working title for the project, being in a slightly irreverent mood (and a big fan of Monty Python's Flying Circus).\n\u2014 Guido van Rossum\nPython 2.0 was released on 16 October 2000 and had many major new features, including a cycle-detecting garbage collector and support for Unicode. With this release, the development process became more transparent and community-backed.[33]Python 3.0 (initially called Python 3000 or py3k) was released on 3 December 2008 after a long testing period. It is a major revision of the language that is not completely backward-compatible with previous versions.[34] However, many of its major features have been backported to the Python 2.6.x[35] and 2.7.x version series, and releases of Python 3 include the 2to3 utility, which automates the translation of Python 2 code to Python 3.[36]Python 2.7's end-of-life date was initially set at 2015, then postponed to 2020 out of concern that a large body of existing code could not easily be forward-ported to Python 3.[37][38]Python 3.6 had changes regarding UTF-8 (in Windows, PEP 528 and PEP 529) and Python 3.7.0b1 (PEP 540) adds a new UTF-8 Mode (and overrides POSIX locale).In January 2017, Google announced work on a Python 2.7 to Go transcompiler to improve performance under concurrent workloads.[39]Features and philosophy[edit]Python is a multi-paradigm programming language. Object-oriented programming and structured programming are fully supported, and many of its features support functional programming and aspect-oriented programming (including by metaprogramming[40] and metaobjects (magic methods)).[41] Many other paradigms are supported via extensions, including design by contract[42][43] and logic programming.[44]Python uses dynamic typing, and a combination of reference counting and a cycle-detecting garbage collector for memory management. It also features dynamic name resolution (late binding), which binds method and variable names during program execution.Python's design offers some support for functional programming in the Lisp tradition. It has filter(), map(), and reduce() functions; list comprehensions, dictionaries, and sets; and generator expressions.[45] The standard library has two modules (itertools and functools) that implement functional tools borrowed from Haskell and Standard ML.[46]The language's core philosophy is summarized in the document The Zen of Python (PEP 20), which includes aphorisms such as:[47]\nBeautiful is better than ugly\nExplicit is better than implicit\nSimple is better than complex\nComplex is better than complicated\nReadability counts\nRather than having all of its functionality built into its core, Python was designed to be highly extensible. This compact modularity has made it particularly popular as a means of adding programmable interfaces to existing applications. Van Rossum's vision of a small core language with a large standard library and easily extensible interpreter stemmed from his frustrations with ABC, which espoused the opposite approach.[29]While offering choice in coding methodology, the Python philosophy rejects exuberant syntax (such as that of Perl) in favor of a simpler, less-cluttered grammar. As Alex Martelli put it: To describe something as 'clever' is not considered a compliment in the Python culture.[48] Python's philosophy rejects the Perl there is more than one way to do it approach to language design in favor of there should be one\u2014and preferably only one\u2014obvious way to do it.[47]Python's developers strive to avoid premature optimization, and reject patches to non-critical parts of CPython that would offer marginal increases in speed at the cost of clarity.[49] When speed is important, a Python programmer can move time-critical functions to extension modules written in languages such as C, or use PyPy, a just-in-time compiler. Cython is also available, which translates a Python script into C and makes direct C-level API calls into the Python interpreter.An important goal of Python's developers is keeping it fun to use. This is reflected in the language's name\u2014a tribute to the British comedy group Monty Python[50]\u2014and in occasionally playful approaches to tutorials and reference materials, such as examples that refer to spam and eggs (from a famous Monty Python sketch) instead of the standard foo and bar.[51][52]A common neologism in the Python community is pythonic, which can have a wide range of meanings related to program style. To say that code is pythonic is to say that it uses Python idioms well, that it is natural or shows fluency in the language, that it conforms with Python's minimalist philosophy and emphasis on readability. In contrast, code that is difficult to understand or reads like a rough transcription from another programming language is called unpythonic.Users and admirers of Python, especially those considered knowledgeable or experienced, are often referred to as Pythonists, Pythonistas, and Pythoneers.[53][54]Syntax and semantics[edit]Python is meant to be an easily readable language. Its formatting is visually uncluttered, and it often uses English keywords where other languages use punctuation. Unlike many other languages, it does not use curly brackets to delimit blocks, and semicolons after statements are optional. It has fewer syntactic exceptions and special cases than C or Pascal.[55]Indentation[edit]Python uses whitespace indentation, rather than curly brackets or keywords, to delimit blocks. An increase in indentation comes after certain statements; a decrease in indentation signifies the end of the current block.[56] Thus, the program's visual structure accurately represents the program's semantic structure.[1] This feature is also sometimes termed the off-side rule.Statements and control flow[edit]Python's statements include (among others):\nThe assignment statement (token '=', the equals sign). This operates differently than in traditional imperative programming languages, and this fundamental mechanism (including the nature of Python's version of variables) illuminates many other features of the language. Assignment in C, e.g., x = 2, translates to typed variable name x receives a copy of numeric value 2. The (right-hand) value is copied into an allocated storage location for which the (left-hand) variable name is the symbolic address. The memory allocated to the variable is large enough (potentially quite large) for the declared type. In the simplest case of Python assignment, using the same example, x = 2, translates to (generic) name x receives a reference to a separate, dynamically allocated object of numeric (int) type of value 2. This is termed binding the name to the object. Since the name's storage location doesn't contain the indicated value, it is improper to call it a variable. Names may be subsequently rebound at any time to objects of greatly varying types, including strings, procedures, complex objects with data and methods, etc. Successive assignments of a common value to multiple names, e.g., x = 2; y = 2; z = 2 result in allocating storage to (at most) three names and one numeric object, to which all three names are bound. Since a name is a generic reference holder it is unreasonable to associate a fixed data type with it. However at a given time a name will be bound to some object, which will have a type; thus there is dynamic typing.\nThe if statement, which conditionally executes a block of code, along with else and elif (a contraction of else-if).\nThe for statement, which iterates over an iterable object, capturing each element to a local variable for use by the attached block.\nThe while statement, which executes a block of code as long as its condition is true.\nThe try statement, which allows exceptions raised in its attached code block to be caught and handled by except clauses; it also ensures that clean-up code in a finally block will always be run regardless of how the block exits.\nThe class statement, which executes a block of code and attaches its local namespace to a class, for use in object-oriented programming.\nThe def statement, which defines a function or method.\nThe with statement, from Python 2.5 released on September 2006,[57] which encloses a code block within a context manager (for example, acquiring a lock before the block of code is run and releasing the lock afterwards, or opening a file and then closing it), allowing Resource Acquisition Is Initialization (RAII)-like behavior and replaces a common try/finally idiom.[58]\nThe pass statement, which serves as a NOP. It is syntactically needed to create an empty code block.\nThe assert statement, used during debugging to check for conditions that ought to apply.\nThe yield statement, which returns a value from a generator function. From Python 2.5, yield is also an operator. This form is used to implement coroutines.\nThe import statement, which is used to import modules whose functions or variables can be used in the current program. There are four ways of using import: import <module name> or from <module name> import * or import numpy as np or from numpy import pi as Pie.\nThe print statement was changed to the print() function in Python 3.[59]\nPython does not support tail call optimization or first-class continuations, and, according to Guido van Rossum, it never will.[60][61] However, better support for coroutine-like functionality is provided in 2.5, by extending Python's generators.[62] Before 2.5, generators were lazy iterators; information was passed unidirectionally out of the generator. From Python 2.5, it is possible to pass information back into a generator function, and from Python 3.3, the information can be passed through multiple stack levels.[63]Expressions[edit]Some Python expressions are similar to languages such as C and Java, while some are not:\nAddition, subtraction, and multiplication are the same, but the behavior of division differs. There are two types of divisions in Python. They are floor division and integer division.[64] Python also added the ** operator for exponentiation.\nFrom Python 3.5, the new @ infix operator was introduced. It is intended to be used by libraries such as NumPy for matrix multiplication.[65][66]\nIn Python, == compares by value, versus Java, which compares numerics by value[67] and objects by reference.[68] (Value comparisons in Java on objects can be performed with the equals() method.) Python's is operator may be used to compare object identities (comparison by reference). In Python, comparisons may be chained, for example a <= b <= c.\nPython uses the words and, or, not for its boolean operators rather than the symbolic &&, ||, ! used in Java and C.\nPython has a type of expression termed a list comprehension. Python 2.4 extended list comprehensions into a more general expression termed a generator expression.[45]\nAnonymous functions are implemented using lambda expressions; however, these are limited in that the body can only be one expression.\nConditional expressions in Python are written as x if c else y[69] (different in order of operands from the c ? x : y operator common to many other languages).\nPython makes a distinction between lists and tuples. Lists are written as [1, 2, 3], are mutable, and cannot be used as the keys of dictionaries (dictionary keys must be immutable in Python). Tuples are written as (1, 2, 3), are immutable and thus can be used as the keys of dictionaries, provided all elements of the tuple are immutable. The + operator can be used to concatenate two tuples, which does not directly modify their contents, but rather produces a new tuple containing the elements of both provided tuples. Thus, given the variable t initially equal to (1, 2, 3), executing t = t + (4, 5) first evaluates t + (4, 5), which yields (1, 2, 3, 4, 5), which is then assigned back to t, thereby effectively modifying the contents of t, while conforming to the immutable nature of tuple objects. Parentheses are optional for tuples in unambiguous contexts.[70]\nPython features sequence unpacking where multiple expressions, each evaluating to anything that can be assigned to (a variable, a writable property, etc.), are associated in the identical manner to that forming tuple literals and, as a whole, are put on the left hand side of the equal sign in an assignment statement. The statement expects an iterable object on the right hand side of the equal sign that produces the same number of values as the provided writable expressions when iterated through, and will iterate through it, assigning each of the produced values to the corresponding expression on the left.[citation needed]\nPython has a string format operator %. This functions analogous to printf format strings in C, e.g. spam=%s eggs=%d % (blah, 2) evaluates to spam=blah eggs=2. In Python 3 and 2.6+, this was supplemented by the format() method of the str class, e.g. spam={0} eggs={1}.format(blah, 2). Python 3.6 added f-strings: blah = blah; eggs = 2; f'spam={blah} eggs={eggs}'.[71]\nPython has various kinds of string literals:\n\nStrings delimited by single or double quote marks. Unlike in Unix shells, Perl and Perl-influenced languages, single quote marks and double quote marks function identically. Both kinds of string use the backslash (\\) as an escape character. String interpolation became available in Python 3.6 as formatted string literals.[71]\nTriple-quoted strings, which begin and end with a series of three single or double quote marks. They may span multiple lines and function like here documents in shells, Perl and Ruby.\nRaw string varieties, denoted by prefixing the string literal with an r. Escape sequences are not interpreted; hence raw strings are useful where literal backslashes are common, such as regular expressions and Windows-style paths. Compare @-quoting in C#.\n\n\nPython has array index and array slicing expressions on lists, denoted as a[key], a[start:stop] or a[start:stop:step]. Indexes are zero-based, and negative indexes are relative to the end. Slices take elements from the start index up to, but not including, the stop index. The third slice parameter, called step or stride, allows elements to be skipped and reversed. Slice indexes may be omitted, for example a[:] returns a copy of the entire list. Each element of a slice is a shallow copy.\nIn Python, a distinction between expressions and statements is rigidly enforced, in contrast to languages such as Common Lisp, Scheme, or Ruby. This leads to duplicating some functionality. For example:\nList comprehensions vs. for-loops\nConditional expressions vs. if blocks\nThe eval() vs. exec() built-in functions (in Python 2, exec is a statement); the former is for expressions, the latter is for statements.\nStatements cannot be a part of an expression, so list and other comprehensions or lambda expressions, all being expressions, cannot contain statements. A particular case of this is that an assignment statement such as a = 1 cannot form part of the conditional expression of a conditional statement. This has the advantage of avoiding a classic C error of mistaking an assignment operator = for an equality operator == in conditions: if (c = 1) { ... } is syntactically valid (but probably unintended) C code but if c = 1: ... causes a syntax error in Python.Methods[edit]Methods on objects are functions attached to the object's class; the syntax instance.method(argument) is, for normal methods and functions, syntactic sugar for Class.method(instance, argument). Python methods have an explicit self parameter to access instance data, in contrast to the implicit self (or this) in some other object-oriented programming languages (e.g., C++, Java, Objective-C, or Ruby).[72]Typing[edit]Python uses duck typing and has typed objects but untyped variable names. Type constraints are not checked at compile time; rather, operations on an object may fail, signifying that the given object is not of a suitable type. Despite being dynamically typed, Python is strongly typed, forbidding operations that are not well-defined (for example, adding a number to a string) rather than silently attempting to make sense of them.Python allows programmers to define their own types using classes, which are most often used for object-oriented programming. New instances of classes are constructed by calling the class (for example, SpamClass() or EggsClass()), and the classes are instances of the metaclass type (itself an instance of itself), allowing metaprogramming and reflection.Before version 3.0, Python had two kinds of classes: old-style and new-style.[73] The syntax of both styles is the same, the difference being whether the class object is inherited from, directly or indirectly (all new-style classes inherit from object and are instances of type). In versions of Python 2 from Python 2.2 onwards, both kinds of classes can be used. Old-style classes were eliminated in Python 3.0.The long term plan is to support gradual typing[74] and from Python 3.5, the syntax of the language allows specifying static types but they are not checked in the default implementation, CPython. An experimental optional static type checker named mypy supports compile-time type checking.[75]Mathematics[edit]Python has the usual C arithmetic operators (+, -, *, /, %). It also has ** for exponentiation, e.g. 5**3 == 125 and 9**0.5 == 3.0, and a new matrix multiply @ operator is included in version 3.5.[77] Additionally, it has a unary operator (~), which essentially inverts all the bits of its one argument. For integers, this means ~x=-x-1.[78] Other operators include bitwise shift operators x << y, which shifts x to the left y places, the same as x*(2**y) , and x >> y, which shifts x to the right y places, the same as x/(2**y) .[79]The behavior of division has changed significantly over time:[80]\nPython 2.1 and earlier use the C division behavior. The / operator is integer division if both operands are integers, and floating-point division otherwise. Integer division rounds towards 0, e.g. 7/3 == 2 and -7/3 == -2.\nPython 2.2 changes integer division to round towards negative infinity, e.g. 7/3 == 2 and -7/3 == -3. The floor division // operator is introduced. So 7//3 == 2, -7//3 == -3, 7.5//3 == 2.0 and -7.5//3 == -3.0. Adding from __future__ import division causes a module to use Python 3.0 rules for division (see next).\nPython 3.0 changes / to be always floating-point division. In Python terms, the pre-3.0 / is classic division, the version-3.0 / is real division, and // is floor division.\nRounding towards negative infinity, though different from most languages, adds consistency. For instance, it means that the equation (a + b)//b == a//b + 1 is always true. It also means that the equation b*(a//b) + a%b == a is valid for both positive and negative values of a. However, maintaining the validity of this equation means that while the result of a%b is, as expected, in the half-open interval [0, b), where b is a positive integer, it has to lie in the interval (b, 0] when b is negative.[81]Python provides a round function for rounding a float to the nearest integer. For tie-breaking, versions before 3 use round-away-from-zero: round(0.5) is 1.0, round(-0.5) is \u22121.0.[82] Python 3 uses round to even: round(1.5) is 2, round(2.5) is 2.[83]Python allows boolean expressions with multiple equality relations in a manner that is consistent with general use in mathematics. For example, the expression a < b < c tests whether a is less than b and b is less than c. C-derived languages interpret this expression differently: in C, the expression would first evaluate a < b, resulting in 0 or 1, and that result would then be compared with c.[84][page needed]Python has extensive built-in support for arbitrary precision arithmetic. Integers are transparently switched from the machine-supported maximum fixed-precision (usually 32 or 64 bits), belonging to the python type int, to arbitrary precision, belonging to the python type long, where needed. The latter have an L suffix in their textual representation.[85] (In Python 3, the distinction between the int and long types was eliminated; this behavior is now entirely contained by the int class.) The Decimal type/class in module decimal (since version 2.4) provides decimal floating point numbers to arbitrary precision and several rounding modes.[86] The Fraction type in module fractions (since version 2.6) provides arbitrary precision for rational numbers.[87]Due to Python's extensive mathematics library, and the third-party library NumPy that further extends the native capabilities, it is frequently used as a scientific scripting language to aid in problems such as numerical data processing and manipulation.Libraries[edit]Python's large standard library, commonly cited as one of its greatest strengths,[88] provides tools suited to many tasks. For Internet-facing applications, many standard formats and protocols such as MIME and HTTP are supported. It includes modules for creating graphical user interfaces, connecting to relational databases, generating pseudorandom numbers, arithmetic with arbitrary precision decimals,[89] manipulating regular expressions, and unit testing.Some parts of the standard library are covered by specifications (for example, the Web Server Gateway Interface (WSGI) implementation wsgiref follows PEP 333[90]), but most modules are not. They are specified by their code, internal documentation, and test suites (if supplied). However, because most of the standard library is cross-platform Python code, only a few modules need altering or rewriting for variant implementations.As of March 2018,[update] the Python Package Index (PyPI), the official repository for third-party Python software, contains over 130,000[91] packages with a wide range of functionality, including:\nGraphical user interfaces\nWeb frameworks\nMultimedia\nDatabases\nNetworking\nTest frameworks\nAutomation\nWeb scraping[92]\nDocumentation\nSystem administration\nScientific computing\nText processing\nImage processing\nDevelopment environments[edit]Most Python implementations (including CPython) include a read\u2013eval\u2013print loop (REPL), permitting them to function as a command line interpreter for which the user enters statements sequentially and receives results immediately.Other shells, including IDLE and IPython, add further abilities such as auto-completion, session state retention and syntax highlighting.As well as standard desktop integrated development environments (see Wikipedia's Python IDE article), there are Web browser-based IDEs; SageMath (intended for developing science and math-related Python programs); PythonAnywhere, a browser-based IDE and hosting environment; and Canopy IDE, a commercial Python IDE emphasizing scientific computing.[93]Implementations[edit]Reference implementation[edit]CPython is the reference implementation of Python. It is written in C, meeting the C89 standard with several select C99 features.[94] It compiles Python programs into an intermediate bytecode[95] which is then executed by its virtual machine.[96] CPython is distributed with a large standard library written in a mixture of C and native Python. It is available for many platforms, including Windows and most modern Unix-like systems. Platform portability was one of its earliest priorities.[97]Other implementations[edit]PyPy is a fast, compliant[98] interpreter of Python 2.7 and 3.5. Its just-in-time compiler brings a significant speed improvement over CPython.[99]Stackless Python is a significant fork of CPython that implements microthreads; it does not use the C memory stack, thus allowing massively concurrent programs. PyPy also has a stackless version.[100]MicroPython is a Python 3 variant optimised for microcontrollers.Unsupported implementations[edit]Other just-in-time Python compilers have been developed, but are now unsupported:\nGoogle began a project named Unladen Swallow in 2009 with the aim of speeding up the Python interpreter fivefold by using the LLVM, and of improving its multithreading ability to scale to thousands of cores.[101]\nPsyco is a just-in-time specialising compiler that integrates with CPython and transforms bytecode to machine code at runtime. The emitted code is specialised for certain data types and is faster than standard Python code.\nIn 2005, Nokia released a Python interpreter for the Series 60 mobile phones named PyS60. It includes many of the modules from the CPython implementations and some additional modules to integrate with the Symbian operating system. The project has been kept up-to-date to run on all variants of the S60 platform, and several third-party modules are available. The Nokia N900 also supports Python with GTK widget libraries, enabling programs to be written and run on the target device.[102]Cross-compilers to other languages[edit]There are several compilers to high-level object languages, with either unrestricted Python, a restricted subset of Python, or a language similar to Python as the source language:\nJython compiles into Java byte code, which can then be executed by every Java virtual machine implementation. This also enables the use of Java class library functions from the Python program.\nIronPython follows a similar approach in order to run Python programs on the .NET Common Language Runtime.\nThe RPython language can be compiled to C, Java bytecode, or Common Intermediate Language, and is used to build the PyPy interpreter of Python.\nPyjs compiles Python to JavaScript.\nCython compiles Python to C and C++.\nPythran compiles Python to C++.\nSomewhat dated Pyrex (latest release in 2010) and Shed Skin (latest release in 2013) compile to C and C++ respectively.\nGoogle's Grumpy compiles Python to Go.\nNuitka compiles Python into C++ [103]\nPerformance[edit]A performance comparison of various Python implementations on a non-numerical (combinatorial) workload was presented at EuroSciPy '13.[104]Development[edit]Python's development is conducted largely through the Python Enhancement Proposal (PEP) process, the primary mechanism for proposing major new features, collecting community input on issues and documenting Python design decisions.[105] Outstanding PEPs are reviewed and commented on by the Python community and Guido Van Rossum, Python's Benevolent Dictator For Life.[105]Enhancement of the language corresponds with development of the CPython reference implementation. The mailing list python-dev is the primary forum for the language's development. Specific issues are discussed in the Roundup bug tracker maintained at python.org.[106] Development originally took place on a self-hosted source-code repository running Mercurial, until Python moved to GitHub in January 2017.[107]CPython's public releases come in three types, distinguished by which part of the version number is incremented:\nBackward-incompatible versions, where code is expected to break and need to be manually ported. The first part of the version number is incremented. These releases happen infrequently\u2014for example, version 3.0 was released 8 years after 2.0.\nMajor or feature releases, about every 18 months, are largely compatible but introduce new features. The second part of the version number is incremented. Each major version is supported by bugfixes for several years after its release.[108]\nBugfix releases, which introduce no new features, occur about every 3 months and are made when a sufficient number of bugs have been fixed upstream since the last release. Security vulnerabilities are also patched in these releases. The third and final part of the version number is incremented.[109]\nMany alpha, beta, and release-candidates are also released as previews and for testing before final releases. Although there is a rough schedule for each release, they are often delayed if the code is not ready. Python's development team monitors the state of the code by running the large unit test suite during development, and using the BuildBot continuous integration system.[110]The community of Python developers has also contributed over 86,000[111] software modules (as of 20 August 2016[update]) to the Python Package Index (PyPI), the official repository of third-party Python libraries.The major academic conference on Python is PyCon. There are also special Python mentoring programmes, such as Pyladies.Naming[edit]Python's name is derived from the British comedy group Monty Python, whom Python creator Guido van Rossum enjoyed while developing the language. Monty Python references appear frequently in Python code and culture;[112] for example, the metasyntactic variables often used in Python literature are spam and eggs instead of the traditional foo and bar.[112][113] The official Python documentation also contains various references to Monty Python routines.[114][115]The prefix Py- is used to show that something is related to Python. Examples of the use of this prefix in names of Python applications or libraries include Pygame, a binding of SDL to Python (commonly used to create games); PyQt and PyGTK, which bind Qt and GTK to Python respectively; and PyPy, a Python implementation originally written in Python.Uses[edit]Since 2003, Python has consistently ranked in the top ten most popular programming languages in the TIOBE Programming Community Index where, as of January 2018[update], it is the fourth most popular language (behind Java, C, and C++).[116] It was selected Programming Language of the Year in 2007 and 2010.[117]An empirical study found that scripting languages, such as Python, are more productive than conventional languages, such as C and Java, for programming problems involving string manipulation and search in a dictionary, and determined that memory consumption was often better than Java and not much worse than C or C++.[118]Large organizations that use Python include Wikipedia, Google,[119] Yahoo!,[120] CERN,[121] NASA,[122] Facebook, Amazon, Instagram, Spotify[citation needed] and some smaller entities like ILM[123] and ITA.[124] The social news networking site Reddit is written entirely in Python.Python can serve as a scripting language for web applications, e.g., via mod_wsgi for the Apache web server.[125] With Web Server Gateway Interface, a standard API has evolved to facilitate these applications. Web frameworks like Django, Pylons, Pyramid, TurboGears, web2py, Tornado, Flask, Bottle and Zope support developers in the design and maintenance of complex applications. Pyjs and IronPython can be used to develop the client-side of Ajax-based applications. SQLAlchemy can be used as data mapper to a relational database. Twisted is a framework to program communications between computers, and is used (for example) by Dropbox.Libraries such as NumPy, SciPy and Matplotlib allow the effective use of Python in scientific computing,[126][127] with specialized libraries such as Biopython and Astropy providing domain-specific functionality. SageMath is a mathematical software with a notebook programmable in Python: its library covers many aspects of mathematics, including algebra, combinatorics, numerical mathematics, number theory, and calculus. The Python language re-implemented in Java platform is used for numeric and statistical calculations with 2D/3D visualization by the DMelt project.[128][129]Python has been successfully embedded in many software products as a scripting language, including in finite element method software such as Abaqus, 3D parametric modeler like FreeCAD, 3D animation packages such as 3ds Max, Blender, Cinema 4D, Lightwave, Houdini, Maya, modo, MotionBuilder, Softimage, the visual effects compositor Nuke, 2D imaging programs like GIMP,[130] Inkscape, Scribus and Paint Shop Pro,[131] and musical notation programs like scorewriter and capella. GNU Debugger uses Python as a pretty printer to show complex structures such as C++ containers. Esri promotes Python as the best choice for writing scripts in ArcGIS.[132] It has also been used in several video games,[133][134] and has been adopted as first of the three available programming languages in Google App Engine, the other two being Java and Go.[135] Python is also used in algorithmic trading and quantitative finance.[136] Python can also be implemented in APIs of online brokerages that run on other languages by using wrappers.[137]Python has been used in artificial intelligence projects.[138][139][140][141] As a scripting language with modular architecture, simple syntax and rich text processing tools, Python is often used for natural language processing.[142]Many operating systems include Python as a standard component. It ships with most Linux distributions, AmigaOS 4, FreeBSD, NetBSD, OpenBSD and macOS, and can be used from the command line (terminal). Many Linux distributions use installers written in Python: Ubuntu uses the Ubiquity installer, while Red Hat Linux and Fedora use the Anaconda installer. Gentoo Linux uses Python in its package management system, Portage.Python is used extensively in the information security industry, including in exploit development.[143][144]Most of the Sugar software for the One Laptop per Child XO, now developed at Sugar Labs, is written in Python.[145]The Raspberry Pi single-board computer project has adopted Python as its main user-programming language.LibreOffice includes Python, and intends to replace Java with Python. Its Python Scripting Provider is a core feature[146] since Version 4.0 from 7 February 2013.Languages influenced by Python[edit]Python's design and philosophy have influenced many other programming languages:\nBoo uses indentation, a similar syntax, and a similar object model.[147]\nCobra uses indentation and a similar syntax, and its Acknowledgements document lists Python first among languages that influenced it.[148] However, Cobra directly supports design-by-contract, unit tests, and optional static typing.[149]\nCoffeeScript, a programming language that cross-compiles to JavaScript, has Python-inspired syntax.\nECMAScript borrowed iterators, generators and list comprehensions from Python.[150]\nGo is described as incorporating the development speed of working in a dynamic language like Python.[151]\nGroovy was motivated by the desire to bring the Python design philosophy to Java.[152]\nJulia was designed with true macros [.. and to be] as usable for general programming as Python [and] should be as fast as C.[22] Calling to or from Julia is possible; to with PyCall.jl and a Python package pyjulia allows calling, in the other direction, from Python.\nKotlin (programming language) is a functional programming language with an interactive shell similar to python. However, Kotlin is strongly typed with access to standard Java libraries.[153]\nOCaml has an optional syntax named twt (The Whitespace Thing), inspired by Python and Haskell.[154]\nRuby's creator, Yukihiro Matsumoto, has said: I wanted a scripting language that was more powerful than Perl, and more object-oriented than Python. That's why I decided to design my own language.[155]\nSwift, a programming language developed by Apple, has some Python-inspired syntax.[156]\nPython's development practices have also been emulated by other languages. For example, the practice of requiring a document describing the rationale for, and issues surrounding, a change to the language (in Python, a PEP) is also used in Tcl[157] and Erlang.[158]Python received TIOBE's Programming Language of the Year awards in 2007 and 2010. The award is given to the language with the greatest growth in popularity over the year, as measured by the TIOBE index.[159]See also[edit]\nComparison of integrated development environments for Python\nComparison of programming languages\nList of programming languages\npip (package manager)\nOff-side rule\nReferences[edit]Further reading[edit]\nDowney, Allen B. (May 2012). Think Python: How to Think Like a Computer Scientist (Version 1.6.6 ed.). ISBN 978-0-521-72596-5. \nHamilton, Naomi (5 August 2008). The A-Z of Programming Languages: Python. Computerworld. Archived from the original on 29 December 2008. Retrieved 31 March 2010. \nLutz, Mark (2013). Learning Python (5th ed.). O'Reilly Media. ISBN 978-0-596-15806-4. \nPilgrim, Mark (2004). Dive Into Python. Apress. ISBN 978-1-59059-356-1. \nPilgrim, Mark (2009). Dive Into Python 3. Apress. ISBN 978-1-4302-2415-0. [permanent dead link]\nSummerfield, Mark (2009). Programming in Python 3 (2nd ed.). Addison-Wesley Professional. ISBN 978-0-321-68056-3. \nExternal links[edit]\nOfficial website\nPython at Curlie (based on DMOZ)\n", "subtitles": ["History", "Features and philosophy", "Syntax and semantics", "Libraries", "Development environments", "Implementations", "Development", "Naming", "Uses", "Languages influenced by Python", "See also", "References", "Further reading", "External links"], "title": "Python (programming language)"},
{"content": "In object-oriented programming, a class is an extensible program-code-template for creating objects, providing initial values for state (member variables) and implementations of behavior (member functions or methods).[1][2] In many languages, the class name is used as the name for the class (the template itself), the name for the default constructor of the class (a subroutine that creates objects), and as the type of objects generated by instantiating the class; these distinct concepts are easily conflated.[2]When an object is created by a constructor of the class, the resulting object is called an instance of the class, and the member variables specific to the object are called instance variables, to contrast with the class variables shared across the class.In some languages, classes are only a compile-time feature (new classes cannot be declared at runtime), while in other languages classes are first-class citizens, and are generally themselves objects (typically of type Class or similar). In these languages, a class that creates classes is called a metaclass.Class vs. type[edit]In casual use, people often refer to the class of an object, but narrowly speaking objects have type: the interface, namely the types of member variables, the signatures of member functions (methods), and properties these satisfy. At the same time, a class has an implementation (specifically the implementation of the methods), and can create objects of a given type, with a given implementation.[3] In the terms of type theory, a class is an implementation\u200d\u2014\u200ca concrete data structure and collection of subroutines\u200d\u2014\u200cwhile a type is an interface. Different (concrete) classes can produce objects of the same (abstract) type (depending on type system); for example, the type Stack might be implemented with two classes \u2013  SmallStack (fast for small stacks, but scales poorly) and ScalableStack (scales well but high overhead for small stacks). Similarly, a given class may have several different constructors.Types generally represent nouns, such as a person, place or thing, or something nominalized, and a class represents an implementation of these. For example, a Banana type might represent the properties and functionality of bananas in general, while the ABCBanana and XYZBanana classes would represent ways of producing bananas (say, banana suppliers or data structures and functions to represent and draw bananas in a video game). The ABCBanana class could then produce particular bananas: instances of the ABCBanana class would be objects of type Banana. Often only a single implementation of a type is given, in which case the class name is often identical with the type name.Design and implementation[edit]Classes are composed from structural and behavioral constituents.[4] Programming languages that include classes as a programming construct offer support, for various class-related features, and the syntax required to use these features varies greatly from one programming language to another.Structure[edit]A class contains data field descriptions (or properties, fields, data members, or attributes). These are usually field types and names that will be associated with state variables at program run time; these state variables either belong to the class or specific instances of the class. In most languages, the structure defined by the class determines the layout of the memory used by its instances. Other implementations are possible: for example, objects in Python use associative key-value containers.[5]Some programming languages support specification of invariants as part of the definition of the class, and enforce them through the type system. Encapsulation of state is necessary for being able to enforce the invariants of the class.Behavior[edit]The behavior of class or its instances is defined using methods. Methods are subroutines with the ability to operate on objects or classes. These operations may alter the state of an object or simply provide ways of accessing it.[6] Many kinds of methods exist, but support for them varies across languages. Some types of methods are created and called by programmer code, while other special methods\u2014such as constructors, destructors, and conversion operators\u2014are created and called by compiler-generated code. A language may also allow the programmer to define and call these special methods.[7][8]The concept of class interface[edit]Every class implements (or realizes) an interface by providing structure and behavior. Structure consists of data and state, and behavior consists of code that specifies how methods are implemented.[9] There is a distinction between the definition of an interface and the implementation of that interface; however, this line is blurred in many programming languages because class declarations both define and implement an interface. Some languages, however, provide features that separate interface and implementation. For example, an abstract class can define an interface without providing implementation.Languages that support class inheritance also allow classes to inherit interfaces from the classes that they are derived from.[example needed] In languages that support access specifiers, the interface of a class is considered to be the set of public members of the class, including both methods and attributes (via implicit getter and setter methods); any private members or internal data structures are not intended to be depended on by external code and thus are not part of the interface.Object-oriented programming methodology dictates that the operations of any interface of a class are to be independent of each other. It results in a layered design where clients of an interface use the methods declared in the interface. An interface places no requirements for clients to invoke the operations of one interface in any particular order. This approach has the benefit that client code can assume that the operations of an interface are available for use whenever the client has access to the object.[citation needed]Example[edit]The buttons on the front of your television set are the interface between you and the electrical wiring on the other side of its plastic casing. You press the power button to toggle the television on and off. In this example, your particular television is the instance, each method is represented by a button, and all the buttons together comprise the interface. (Other television sets that are the same model as yours would have the same interface.) In its most common form, an interface is a specification of a group of related methods without any associated implementation of the methods.A television set also has a myriad of attributes, such as size and whether it supports color, which together comprise its structure. A class represents the full description of a television, including its attributes (structure) and buttons (interface).Getting the total number of televisions manufactured could be a static method of the television class. This method is clearly associated with the class, yet is outside the domain of each individual instance of the class. Another example would be a static method that finds a particular instance out of the set of all television objects.Member accessibility[edit]The following is a common set of access specifiers:[10]\nPrivate (or class-private) restricts the access to the class itself. Only methods that are part of the same class can access private members.\nProtected (or class-protected) allows the class itself and all its subclasses to access the member.\nPublic means that any code can access the member by its name.\nAlthough many object-oriented languages support the above access specifiers, their semantics may differ.Object-oriented design uses the access specifiers in conjunction with careful design of public method implementations to enforce class invariants\u2014constraints on the state of the objects. A common usage of access specifiers is to separate the internal data of a class from its interface: the internal structure is made private, while public accessor methods can be used to inspect or alter such private data.Access specifiers do not necessarily control visibility, in that even private members may be visible to client external code. In some languages, an inaccessible but visible member may be referred to at run-time (for example, by a pointer returned from a member function), but an attempt to use it by referring to the name of the member from client code will be prevented by the type checker.[11]The various object-oriented programming languages enforce member accessibility and visibility to various degrees, and depending on the language's type system and compilation policies, enforced at either compile-time or run-time. For example, the Java language does not allow client code that accesses the private data of a class to compile. [12] In the C++ language, private methods are visible, but not accessible in the interface; however, they may be made invisible by explicitly declaring fully abstract classes that represent the interfaces of the class.[13]Some languages feature other accessibility schemes:\nInstance vs. class accessibility: Ruby supports instance-private and instance-protected access specifiers in lieu of class-private and class-protected, respectively. They differ in that they restrict access based on the instance itself, rather than the instance's class.[14]\nFriend: C++ supports a mechanism where a function explicitly declared as a friend function of the class may access the members designated as private or protected.[15]\nPath-based: Java supports restricting access to a member within a Java package, which is the logical path of the file. However, it is a common practice when extending a Java framework to implement classes in the same package as a framework class in order to access protected members. The source file may exist in a completely different location, and may be deployed to a different .jar file, yet still be in the same logical path as far as the JVM is concerned.[10]\nInter-class relationships[edit]In addition to the design of standalone classes, programming languages may support more advanced class design based upon relationships between classes. The inter-class relationship design capabilities commonly provided are compositional and hierarchical.Compositional[edit]Classes can be composed of other classes, thereby establishing a compositional relationship between the enclosing class and its embedded classes. Compositional relationship between classes is also commonly known as a has-a relationship.[16] For example, a class Car could be composed of and contain a class Engine. Therefore, a Car has an Engine. One aspect of composition is containment, which is the enclosure of component instances by the instance that has them. If an enclosing object contains component instances by value, the components and their enclosing object have a similar lifetime. If the components are contained by reference, they may not have a similar lifetime.[17] For example, in Objective-C 2.0:This Car class has an instance of NSString (a string object), Engine, and NSArray (an array object).Hierarchical[edit]Classes can be derived from one or more existing classes, thereby establishing a hierarchical relationship between the derived-from classes (base classes, parent classes or superclasses) and the derived class (child class or subclass) . The relationship of the derived class to the derived-from classes is commonly known as an is-a relationship.[18] For example, a class 'Button' could be derived from a class 'Control'. Therefore, a Button is a Control. Structural and behavioral members of the parent classes are inherited by the child class. Derived classes can define additional structural members (data fields) and behavioral members (methods) in addition to those that they inherit and are therefore specializations of their superclasses. Also, derived classes can override inherited methods if the language allows.Not all languages support multiple inheritance. For example, Java allows a class to implement multiple interfaces, but only inherit from one class.[19] If multiple inheritance is allowed, the hierarchy is a directed acyclic graph (or DAG for short), otherwise it is a tree. The hierarchy has classes as nodes and inheritance relationships as links. Classes in the same level are more likely to be associated than classes in different levels. The levels of this hierarchy are called layers or levels of abstraction.Example (Simplified Objective-C 2.0 code, from iPhone SDK):In this example, a UITableView is a UIScrollView is a UIView is a UIResponder is an NSObject.Definitions of subclass[edit]Conceptually, a superclass is a superset of its subclasses. For example, a common class hierarchy would involve GraphicObject as a superclass of Rectangle and Elipse, while Square would be a subclass of Rectangle. These are all subset relations in set theory as well, i.e., all squares are rectangles but not all rectangles are squares.A common conceptual error is to mistake a part of relation with a subclass. For example, a car and truck are both kinds of vehicles and it would be appropriate to model them as subclasses of a vehicle class. However, it would be an error to model the component parts of the car as subclass relations. For example, a car is composed of an engine and body, but it would not be appropriate to model engine or body as a subclass of car.In object-oriented modeling these kinds of relations are typically modeled as object properties. In this example the Car class would have a property called parts. parts would be typed to hold a collection of objects such as instances of Body, Engine, Tires,.... Object modeling languages such as UML include capabilities to model various aspects of part of and other kinds of relations. Data such as the cardinality of the objects, constraints on input and output values, etc. This information can be utilized by developer tools to generate additional code beside the basic data definitions for the objects. Things such as error checking on get and set methods.[20]One important question when modeling and implementing a system of object classes is whether a class can have one or more superclasses. In the real world with actual sets it would be rare to find sets that didn't intersect with more than one other set. However, while some systems such as Flavors and CLOS provide a capability for more than one parent to do so at run time introduces complexity that many in the object-oriented community consider antithetical to the goals of using object classes in the first place. Understanding which class will be responsible for handling a message can get complex when dealing with more than one superclass. If used carelessly this feature can introduce some of the same system complexity and ambiguity classes were designed to avoid.[21]Most modern object-oriented languages such as Smalltalk and Java require single inheritance at run time. For these languages, multiple inheritance may be useful for modeling but not for an implementation.However, semantic web application objects do have multiple superclasses. The volatility of the Internet requires this level of flexibility and the technology standards such as the Web Ontology Language (OWL) are designed to support it.A similar issue is whether or not the class hierarchy can be modified at run time. Languages such as Flavors, CLOS, and Smalltalk all support this feature as part of their meta-object protocols. Since classes are themselves first-class objects, it is possible to have them dynamically alter their structure by sending them the appropriate messages. Other languages that focus more on strong typing such as Java and C++ do not allow the class hierarchy to be modified at run time. Semantic web objects have the capability for run time changes to classes. The rational is similar to the justification for allowing multiple superclasses, that the Internet is so dynamic and flexible that dynamic changes to the hierarchy are required to manage this volatility.[22]Orthogonality of the class concept and inheritance[edit]Although class-based languages are commonly assumed to support inheritance, inheritance is not an intrinsic aspect of the concept of classes. Some languages, often referred to as object-based languages, support classes yet do not support inheritance. Examples of object-based languages include earlier versions of Visual Basic.Within object-oriented analysis[edit]In object-oriented analysis and in UML, an association between two classes represents a collaboration between the classes or their corresponding instances. Associations have direction; for example, a bi-directional association between two classes indicates that both of the classes are aware of their relationship.[23] Associations may be labeled according to their name or purpose.[24]An association role is given end of an association and describes the role of the corresponding class. For example, a subscriber role describes the way instances of the class Person participate in a subscribes-to association with the class Magazine. Also, a Magazine has the subscribed magazine role in the same association. Association role multiplicity describes how many instances correspond to each instance of the other class of the association. Common multiplicities are 0..1, 1..1, 1..* and 0..*, where the * specifies any number of instances.[23]Taxonomy of classes[edit]There are many categories of classes, some of which overlap.Abstract and concrete[edit]In a language that supports inheritance, an abstract class, or abstract base class (ABC), is a class that cannot be instantiated because it is either labeled as abstract or it simply specifies abstract methods (or virtual methods). An abstract class may provide implementations of some methods, and may also specify virtual methods via signatures that are to be implemented by direct or indirect descendants of the abstract class. Before a class derived from an abstract class can be instantiated, all abstract methods of its parent classes must be implemented by some class in the derivation chain.[25]Most object-oriented programming languages allow the programmer to specify which classes are considered abstract and will not allow these to be instantiated. For example, in Java, C# and PHP, the keyword abstract is used.[26][27] In C++, an abstract class is a class having at least one abstract method given by the appropriate syntax in that language (a pure virtual function in C++ parlance).[25]A class consisting of only virtual methods is called a Pure Abstract Base Class (or Pure ABC) in C++ and is also known as an interface by users of the language.[13] Other languages, notably Java and C#, support a variant of abstract classes called an interface via a keyword in the language. In these languages, multiple inheritance is not allowed, but a class can implement multiple interfaces. Such a class can only contain abstract publicly accessible methods.[19][28][29]A concrete class is a class that can be instantiated, as opposed to abstract classes, which cannot.Local and inner[edit]In some languages, classes can be declared in scopes other than the global scope. There are various types of such classes.An inner class is a class defined within another class. The relationship between an inner class and its containing class can also be treated as another type of class association. An inner class is typically neither associated with instances of the enclosing class nor instantiated along with its enclosing class. Depending on language, it may or may not be possible to refer to the class from outside the enclosing class. A related concept is inner types, also known as inner data type or nested type, which is a generalization of the concept of inner classes. C++ is an example of a language that supports both inner classes and inner types (via typedef declarations).[30][31]Another type is a local class, which is a class defined within a procedure or function. This limits references to the class name to within the scope where the class is declared. Depending on the semantic rules of the language, there may be additional restrictions on local classes compared to non-local ones. One common restriction is to disallow local class methods to access local variables of the enclosing function. For example, in C++, a local class may refer to static variables declared within its enclosing function, but may not access the function's automatic variables.[32]Metaclasses[edit]Metaclasses are classes whose instances are classes.[33] A metaclass describes a common structure of a collection of classes and can implement a design pattern or describe particular kinds of classes. Metaclasses are often used to describe frameworks.[34]In some languages, such as Python, Ruby or Smalltalk, a class is also an object; thus each class is an instance of a unique metaclass that is built into the language. [5] [35] [36] The Common Lisp Object System (CLOS) provides metaobject protocols (MOPs) to implement those classes and metaclasses. [37]Non-subclassable[edit]Non-subclassable classes allow programmers to design classes and hierarchies of classes where at some level in the hierarchy, further derivation is prohibited. (A stand-alone class may be also designated as non-subclassable, preventing the formation of any hierarchy). Contrast this to abstract classes, which imply, encourage, and require derivation in order to be used at all. A non-subclassable class is implicitly concrete.A non-subclassable class is created by declaring the class as sealed in C# or as final in Java or PHP.[38][39][40]For example, Java's String class is designated as final.[41]Non-subclassable classes may allow a compiler (in compiled languages) to perform optimizations that are not available for subclassable classes.[citation needed]Mixins[edit]Some languages have special support for mixins, though in any language with multiple inheritance a mixin is simply a class that does not represent an is-a-type-of relationship. Mixins are typically used to add the same methods to multiple classes; for example, a class UnicodeConversionMixin might provide a method called unicode_to_ascii when included in classes FileReader and WebPageScraper that do not share a common parent.Partial[edit]In languages supporting the feature, a partial class is a class whose definition may be split into multiple pieces, within a single source-code file or across multiple files. The pieces are merged at compile-time, making compiler output the same as for a non-partial class.The primary motivation for introduction of partial classes is to facilitate the implementation of code generators, such as visual designers. It is otherwise a challenge or compromise to develop code generators that can manage the generated code when it is interleaved within developer-written code. Using partial classes, a code generator can process a separate file or coarse-grained partial class within a file, and is thus alleviated from intricately interjecting generated code via extensive parsing, increasing compiler efficiency and eliminating the potential risk of corrupting developer code. In a simple implementation of partial classes, the compiler can perform a phase of precompilation where it unifies all the parts of a partial class. Then, compilation can proceed as usual.Other benefits and effects of the partial class feature include:\nEnables separation of a class's interface and implementation code in a unique way.\nEases navigation through large classes within an editor.\nEnables separation of concerns, in a way similar to aspect-oriented programming but without using any extra tools.\nEnables multiple developers to work on a single class concurrently without the need to merge individual code into one file at a later time.\nPartial classes have existed in Smalltalk under the name of Class Extensions for considerable time. With the arrival of the .NET framework 2, Microsoft introduced partial classes, supported in both C# 2.0 and Visual Basic 2005. WinRT also supports partial classes.Example in VB.NET[edit]This simple example, written in Visual Basic .NET, shows how parts of the same class are defined in two different files.\nfile1.vb\n\nfile2.vb\nWhen compiled, the result is the same as if the two files were written as one, like this:Example in Objective-C[edit]In Objective-C, partial classes, also known as categories, may even spread over multiple libraries and executables, like this example:In Foundation, header file NSData.h:In user-supplied library, a separate binary from Foundation framework, header file NSData+base64.h:And in an app, yet another separate binary file, source code file main.m:The dispatcher will find both methods called over the NSData instance and invoke both of them correctly.Uninstantiable[edit]Uninstantiable classes allow programmers to group together per-class fields and methods that are accessible at runtime without an instance of the class. Indeed, instantiation is prohibited for this kind of class.For example, in C#, a class marked static can not be instantiated, can only have static members (fields, methods, other), may not have instance constructors, and is sealed. [42]Unnamed[edit]An unnamed class or anonymous class is a class that is not bound to a name or identifier upon definition. This is analogous to named versus unnamed functions.Benefits[edit]The benefits of organizing software into object classes fall into three categories:[43]\nRapid development\nEase of maintenance\nReuse of code and designs\nObject classes facilitate rapid development because they lessen the semantic gap between the code and the users. System analysts can talk to both developers and users using essentially the same vocabulary, talking about accounts, customers, bills, etc. Object classes often facilitate rapid development because most object-oriented environments come with powerful debugging and testing tools. Instances of classes can be inspected at run time to verify that the system is performing as expected. Also, rather than get dumps of core memory, most object-oriented environments have interpreted debugging capabilities so that the developer can analyze exactly where in the program the error occurred and can see which methods were called to which arguments and with what arguments.[44]Object classes facilitate ease of maintenance via encapsulation. When developers need to change the behavior of an object they can localize the change to just that object and its component parts. This reduces the potential for unwanted side effects from maintenance enhancements.Software re-use is also a major benefit of using Object classes. Classes facilitate re-use via inheritance and interfaces. When a new behavior is required it can often be achieved by creating a new class and having that class inherit the default behaviors and data of its superclass and then tailor some aspect of the behavior or data accordingly. Re-use via interfaces (also known as methods) occurs when another object wants to invoke (rather than create a new kind of) some object class. This method for re-use removes many of the common errors that can make their way into software when one program re-uses code from another.[45]These benefits come with a cost of course. One of the most serious obstacles to using object classes has been performance. Interpreted environments that support languages such as Smalltalk and CLOS provided rapid development but the resulting code was not nearly as fast as what could be achieved in some procedural languages such as C. This has been partly addressed by the development of object-oriented languages that are not interpreted such as C++ and Java.[46] Also, due to Moore's law the processing power of computers has increased to the point where efficient code is not as critical for most systems as it was in the past.[citation needed]Still, no matter how well designed the language, there will always be an inevitable bit of required extra overhead to create a class rather than use procedural code and in some circumstances, especially where performance or memory are required to be optimal, that using object classes may not be the best approach. Also, getting the benefits of object classes requires that they be used appropriately and that requires training. Without the proper training developers may simply code procedural programs in an object-oriented environment and end up with the worst of both worlds.[47]Run-time representation[edit]As a data type, a class is usually considered as a compile-time construct. A language may also support prototype or factory metaobjects that represent run-time information about classes, or even represent metadata that provides access to reflection facilities and ability to manipulate data structure formats at run-time. Many languages distinguish this kind of run-time type information about classes from a class on the basis that the information is not needed at run-time. Some dynamic languages do not make strict distinctions between run-time and compile-time constructs, and therefore may not distinguish between metaobjects and classes.For example, if Human is a metaobject representing the class Person, then instances of class Person can be created by using the facilities of the Human metaobject.See also[edit]\nClass-based programming\nClass diagram (UML)\nList of object-oriented programming languages\nMixin\nObject-oriented programming\nPrototype-based programming\nTrait (computer programming)\nNotes[edit]References[edit]Further reading[edit]\nAbadi; Cardelli: A Theory of Objects\nISO/IEC 14882:2003 Programming Language C++, International standard\nClass Warfare: Classes vs. Prototypes, by Brian Foote\nMeyer, B.: Object-oriented software construction, 2nd edition, Prentice Hall, 1997, ISBN 0-13-629155-4\nRumbaugh et al.: Object-oriented modeling and design, Prentice Hall, 1991, ISBN 0-13-630054-5\nExternal links[edit]\nDias, Tiago (October 2006). Programming demo - .NET using Partial Types for better code. Hyper/Net. Youtube. \n", "subtitles": ["Class vs. type", "Design and implementation", "Inter-class relationships", "Taxonomy of classes", "Benefits", "Run-time representation", "See also", "Notes", "References", "Further reading", "External links"], "title": "Class (computer programming)"},
{"content": "Exception handling syntax varies between programming languages, partly to cover semantic differences but largely to fit into each language's overall syntactic structure. Some languages do not call the relevant concept 'exception handling'; others may not have direct facilities for it, but can still provide means to implement it.Most commonly, error handling uses a try...[catch...][finally...] block, and errors are created via a throw statement, but there is significant variation in naming and syntax.Catalogue of exception handling syntaxes[edit]Ada[edit]\nException declarations\n\nRaising exceptions\n\nException handling and propagation\nAssembly language[edit]Most assembly languages will have a macro instruction or an interrupt address available for the particular system to intercept events such as illegal op codes, program check, data errors, overflow, divide by zero, and other such. IBM and Univac mainframes had the STXIT macro. Digital Equipment Corporation RT11 systems had trap vectors for program errors, i/o interrupts, and such. DOS has certain interrupt addresses. Microsoft Windows has specific module calls to trap program errors.Bash[edit]One can set a trap for multiple errors, responding to any signal with syntax like:\ntrap 'echo Error at line ${LINENO}' ERR\nBASIC[edit]An On Error goto/gosub structure is used in BASIC and is quite different from modern exception handling; in BASIC there is only one global handler whereas in modern exception handling, exception handlers are stacked.C[edit]The most common way to implement exception handling in standard C is to use setjmp/longjmp functions:Microsoft-specific[edit]Two types exist:\nStructured Exception Handling (SEH)\nVectored Exception Handling (VEH, introduced in Windows XP)\nExample of SEH in C programming language:C#[edit]C++[edit]In C++, a resource acquisition is initialization technique can be used to clean up resources in exceptional situations. C++ intentionally does not support finally.[1] The outer braces for the method are optional.ColdFusion Markup Language (CFML)[edit]Script syntax[edit]Adobe ColdFusion documentation[2]Tag syntax[edit]Adobe ColdFusion documentation[3]Railo-Lucee specific syntax[edit]Added to the standard syntax above, CFML dialects of Railo and Lucee allow a retry statement.[4]This statement returns processing to the start of the prior try block.CFScript example:Tag-syntax example:D[edit]In D, a finally clause or the resource acquisition is initialization technique can be used to clean up resources in exceptional situations.Delphi[edit]\nException declarations\n\nRaising exceptions\n\nException handling and propagation[5]\nErlang[edit]Haskell[edit]Haskell does not have special syntax for exceptions. Instead, a try/catch/finally/etc. interface is provided by functions.prints\n(1,42)\nin analogy with this C++Another example isIn purely functional code, if only one error condition exists, the Maybe type may be sufficient, and is an instance of Haskell's Monad class by default. More complex error propagation can be achieved using the Error or ErrorT monads, for which similar functionality (using `catch`) is supported.Java[edit]JavaScript[edit]Lisp[edit]Common Lisp[edit]Lua[edit]Lua uses the pcall and xpcall functions, with xpcall taking a function to act as a catch block.\nPredefined function\n\nAnonymous function\nObjective-C[edit]\nException declarations\n\nRaising exceptions\n\nException handling and propagation\nOCaml[edit]Perl[edit]The Perl mechanism for exception handling uses die to throw an exception when wrapped inside an eval { ... }; block. After the eval, the special variable $@ contains the value passed from die. However, scoping issues can make doing this correctly quite ugly:Perl 5.005 added the ability to throw objects as well as strings. This allows better introspection and handling of types of exceptions.The __DIE__ pseudo-signal can be trapped to handle calls to die. This is not suitable for exception handling since it is global. However it can be used to convert string-based exceptions from third-party packages into objects.The forms shown above can sometimes fail if the global variable $@ is changed between when the exception is thrown and when it is checked in the if ($@) statement. This can happen in multi-threaded environments, or even in single-threaded environments when other code (typically called in the destruction of some object) resets the global variable before the checking code. The following example shows a way to avoid this problem (see [1]). But at the cost of not being able to use return values:Several modules in the Comprehensive Perl Archive Network (CPAN) expand on the basic mechanism:\nError provides a set of exception classes and allows use of the try/throw/catch/finally syntax.\nTryCatch and Try::Tiny both allow use of try/catch/finally syntax instead of boilerplate to handle exceptions correctly.\nException::Class is a base class and class-maker for derived exception classes. It provides a full structured stack trace in $@->trace and $@->trace->as_string.\nFatal overloads previously defined functions that return true/false e.g., open, close, read, write, etc. This allows built-in functions and others to be used as if they threw exceptions.\nPHP[edit]PowerBuilder[edit]Exception handling is available in PowerBuilder versions 8.0 and above.\nTRY\n   // Normal execution path\nCATCH (ExampleException ee)\n   //  deal with the ExampleException\nFINALLY\n   // This optional section is executed upon termination of any of the try or catch blocks above\nEND TRY\nPowerShell[edit]Version 1.0[edit]Version 2.0[edit]Python[edit]R[edit]Rebol[edit]Rexx[edit]Ruby[edit]S-Lang[edit]\n try \n {\n    % code that might throw an exception\n }\n catch SomeError: \n { \n    % code that handles this exception\n }\n catch SomeOtherError:\n {  \n    % code that handles this exception\n }\n finally   % optional block\n {\n    % This code will always get executed\n }\nNew exceptions may be created using the new_exception function, e.g.,\n new_exception (MyIOError, IOError, My I/O Error);\nwill create an exception called MyIOError as a subclass of IOError. Exceptions may be generated using the throw statement, which can throw arbitrary S-Lang objects.Smalltalk[edit]The general mechanism is provided by the message on:do:.[6] Exceptions are just normal objects that subclass Error, you throw one by creating an instance and sending it a #signal message, e.g., MyException new signal. The handling mechanism (#on:do:) is again just a normal message implemented by BlockClosure. The thrown exception is passed as a parameter to the handling block closure, and can be queried, as well as potentially sending #resume to it, to allow execution flow to continue.Swift[edit]Exception handling is supported since Swift 2.Tcl[edit]Since Tcl 8.6, there is also a try command:VBScript[edit][7]Visual Basic[edit][7]Visual Basic .NET[edit]Visual Prolog[edit]http://wiki.visual-prolog.com/index.php?title=Language_Reference/Terms#Try-catch-finallyX++[edit]References[edit]See also[edit]\nException handling for the semantics of exception handling\nSyntax for definition of syntax in computer science\n", "subtitles": ["Catalogue of exception handling syntaxes", "References", "See also"], "title": "Exception handling syntax"},
{"content": "In most computer programming languages, a while loop is a control flow statement that allows code to be executed repeatedly based on a given Boolean condition. The while loop can be thought of as a repeating if statement.Overview[edit]The while construct consists of a block of code and a condition/expression.[1] The condition/expression is evaluated, and if the condition/expression is true,[1] the code within the block is executed. This repeats until the condition/expression becomes false. Because the while loop checks the condition/expression before the block is executed, the control structure is often also known as a pre-test loop. Compare this with the do while loop, which tests the condition/expression after the loop has executed.For example, in the C programming language (as well as Java, C#,[2] Objective-C, and C++, which use the same syntax in this case), the code fragmentfirst checks whether x is less than 5, which it is, so then the {loop body} is entered, where the printf function is run and x is incremented by 1. After completing all the statements in the loop body, the condition, (x < 5), is checked again, and the loop is executed again, this process repeating until the variable x has the value 5.Note that it is possible, and in some cases desirable, for the condition to always evaluate to true, creating an infinite loop. When such a loop is created intentionally, there is usually another control structure (such as a break statement) that controls termination of the loop. For example:Equivalent constructs[edit]In the C programming language,is equivalent toorororThose last two are not recommended because the use of goto statements makes it hard for a programmer to understand the flow of control, and is generally regarded as a last resort.Also, in C and its descendants, a while loop is a for loop with no initialization or counting expressions, i.e.,Demonstrating while loops[edit]These while loops will calculate the factorial of the number 5:ActionScript 3[edit]Ada[edit]Microsoft Small Basic[edit]Visual Basic[edit]Bourne (Unix) shell[edit]\ninclude <stdio.h>\nint main() {\n int i=10;\n while (i==10);\n {\n printf(%d,i);\n i++;\n }\n}Fortran[edit]Java, C#, D[edit]The code for the loop is the same for Java, C# and D:For Java the result is printed as follows:The same in C#And finally in DJavaScript[edit]Lua[edit]MATLAB[edit]Mathematica[edit]Oberon, Oberon-2 (programming language), Oberon-07, or Component Pascal[edit]Maya Embedded Language[edit]Pascal[edit]Perl[edit]While loops are frequently used for reading data line by line (as defined by the $/ line separator) from open filehandles:PHP[edit]PL/I[edit]Python[edit]Non-terminating while loop:Racket[edit]In Racket, as in other Scheme implementations, a named-let is a popular way to implement loops:Using a macro system, implementing a while loop is a trivial exercise (commonly used to introduce macros):But note that an imperative programming style is often discouraged in Racket (as in Scheme).Ruby[edit]Smalltalk[edit]Contrary to other languages, in Smalltalk a while loop is not a language construct but defined in the class BlockClosure as a method with one parameter, the body as a closure, using self as the condition.Smalltalk also has a corresponding whileFalse: method.Swift[edit]Tcl[edit]VEX[edit]Windows PowerShell[edit]While programming language[edit]The While programming language[3] is a simple programming language constructed from assignments, sequential composition, conditionals and while statements, used in the theoretical analysis of imperative programming language semantics.[4][5]See also[edit]\nDo while loop\nFor loop\nForeach\nLOOP (programming language) \u2013 a programming language with the property that the functions it can compute are exactly the primitive recursive functions\nReferences[edit]", "subtitles": ["Overview", "Equivalent constructs", "Demonstrating ", " loops", "See also", "References"], "title": "While loop"},
{"content": "Resource acquisition is initialization (RAII)[1] is a programming idiom[2] used in several object-oriented languages to describe a particular language behavior. In RAII, holding a resource is a class invariant, and is tied to object lifetime: resource allocation (or acquisition) is done during object creation (specifically initialization), by the constructor, while resource deallocation (release) is done during object destruction (specifically finalization), by the destructor. Thus the resource is guaranteed to be held between when initialization finishes and finalization starts (holding the resources is a class invariant), and to be held only when the object is alive. Thus if there are no object leaks, there are no resource leaks.RAII is associated most prominently with C++ where it originated, but also D, Ada, Vala, and Rust. The technique was developed for exception-safe resource management in C++[3] during 1984\u201389, primarily by Bjarne Stroustrup and Andrew Koenig,[4] and the term itself was coined by Stroustrup.[5] RAII is generally pronounced as an initialism, sometimes pronounced as R, A, double I.[6]Other names for this idiom include Constructor Acquires, Destructor Releases (CADRe) [7] and one particular style of use is called Scope-based Resource Management (SBRM).[8] This latter term is for the special case of automatic variables. RAII ties resources to object lifetime, which may not coincide with entry and exit of a scope. (Notably variables allocated on the free store have lifetimes unrelated to any given scope.) However, using RAII for automatic variables (SBRM) is the most common use case.C++11 example[edit]The following C++11 example demonstrates usage of RAII for file access and mutex locking:This code is exception-safe because C++ guarantees that all stack objects are destroyed at the end of the enclosing scope, known as stack unwinding. The destructors of both the lock and file objects are therefore guaranteed to be called when returning from the function, whether an exception has been thrown or not.[9]Local variables allow easy management of multiple resources within a single function: they are destroyed in the reverse order of their construction, and an object is destroyed only if fully constructed\u2014that is, if no exception propagates from its constructor.[10]Using RAII greatly simplifies resource management, reduces overall code size and helps ensure program correctness. RAII is therefore highly recommended in C++, and most of the C++ standard library follows the idiom.[11]Benefits[edit]The advantages of RAII as a resource management technique are that it provides encapsulation, exception safety (for stack resources), and locality (it allows acquisition and release logic to be written next to each other).Encapsulation is provided because resource management logic is defined once in the class, not at each call site. Exception safety is provided for stack resources (resources that are released in the same scope as they are acquired) by tying the resource to the lifetime of a stack variable (a local variable declared in a given scope): if an exception is thrown, and proper exception handling is in place, the only code that will be executed when exiting the current scope are the destructors of objects declared in that scope. Finally, locality of definition is provided by writing the constructor and destructor definitions next to each other in the class definition.Resource management therefore needs to be tied to the lifespan of suitable objects in order to gain automatic allocation and reclamation. Resources are acquired during initialization, when there is no chance of them being used before they are available, and released with the destruction of the same objects, which is guaranteed to take place even in case of errors.Comparing RAII with the finally construct used in Java, Stroustrup wrote that \u201cIn realistic systems, there are far more resource acquisitions than kinds of resources, so the resource acquisition is initialization technique leads to less code than use of a finally construct.\u201d[1]Typical uses[edit]The RAII design is often used for controlling mutex locks in multi-threaded applications. In that use, the object releases the lock when destroyed. Without RAII in this scenario the potential for deadlock would be high and the logic to lock the mutex would be far from the logic to unlock it. With RAII, the code that locks the mutex essentially includes the logic that the lock will be released when execution leaves the scope of the RAII object.Another typical example is interacting with files: We could have an object that represents a file that is open for writing, wherein the file is opened in the constructor and closed when execution leaves the object's scope. In both cases, RAII ensures only that the resource in question is released appropriately; care must still be taken to maintain exception safety. If the code modifying the data structure or file is not exception-safe, the mutex could be unlocked or the file closed with the data structure or file corrupted.Ownership of dynamically allocated objects (memory allocated with new in C++) can also be controlled with RAII, such that the object is released when the RAII (stack-based) object is destroyed. For this purpose, the C++11 standard library defines the smart pointer classes std::unique_ptr for single-owned objects and std::shared_ptr for objects with shared ownership. Similar classes are also available through std::auto_ptr in C++98, and boost::shared_ptr in the Boost libraries.Clang and GCC cleanup extension for C[edit]Both Clang and GNU Compiler Collection implement a non-standard extension to the C language to support RAII: the cleanup variable attribute.[12] The following macro annotates a variable with a given destructor function that it will call when the variable goes out of scope:This macro can then be used as follows:In this example, the compiler arranges for the fclosep function to be called on logfile before example_usage returns.Limitations[edit]RAII only works for resources acquired and released (directly or indirectly) by stack-allocated objects, where there is a well-defined static object lifetime. Heap-allocated objects which themselves acquire and release resources are common in many languages, including C++. RAII depends on heap-based objects to be implicitly or explicitly deleted along all possible execution paths, in order to trigger its resource-releasing destructor (or equivalent).[13]:8:27 This can be achieved by using smart pointers to manage all heap objects, with weak-pointers for cyclically referenced objects.In C++, stack unwinding is only guaranteed to occur if the exception is caught somewhere. This is because If no matching handler is found in a program, the function terminate() is called; whether or not the stack is unwound before this call to terminate() is implementation-defined (15.5.1). (C++03 standard, \u00a715.3/9).[14] This behavior is usually acceptable, since the operating system releases remaining resources like memory, files, sockets, etc. at program termination.Reference counting[edit]Perl, Python (in the CPython implementation),[15] and PHP[16] manage object lifetime by reference counting, which makes it possible to use RAII. Objects that are no longer referenced are immediately destroyed or finalized and released, so a destructor or finalizer can release the resource at that time. However, it is not always idiomatic in such languages, and is specifically discouraged in Python (in favor of context managers and finalizers from the weakref package).However, object lifetimes are not necessarily bound to any scope, and objects may be destroyed non-deterministically or not at all. This makes it possible to accidentally leak resources that should have been released at the end of some scope. Objects stored in a static variable (notably a global variable) may not be finalized when the program terminates, so their resources are not released; CPython makes no guarantee of finalizing such objects, for instance. Further, objects with circular references will not be collected by a simple reference counter, and will live indeterminately long; even if collected (by more sophisticated garbage collection), destruction time and destruction order will be non-deterministic. In CPython there is a cycle detector which detects cycles and finalizes the objects in the cycle, though prior to CPython 3.4, cycles are not collected if any object in the cycle has a finalizer.[17]References[edit]Further reading[edit]External links[edit]\nSample Chapter Gotcha #67: Failure to Employ Resource Acquisition Is Initialization by Stephen Dewhurst\nInterview A Conversation with Bjarne Stroustrup by Bill Venners\nArticle The Law of The Big Two by Bjorn Karlsson and Matthew Wilson\nArticle Implementing the 'Resource Acquisition is Initialization' Idiom by Danny Kalev\nArticle RAII, Dynamic Objects, and Factories in C++ by Roland Pibinger\nRAII in Delphi One-liner RAII in Delphi by Barry Kelly\nRAII in Java by Yegor Bugayenko\n", "subtitles": ["C++11 example", "Benefits", "Typical uses", "Clang and GCC \"cleanup\" extension for C", "Limitations", "Reference counting", "References", "Further reading", "External links"], "title": "Resource acquisition is initialization"},
{"content": "In computer science, a lock or mutex (from mutual exclusion) is a synchronization mechanism for enforcing limits on access to a resource in an environment where there are many threads of execution. A lock is designed to enforce a mutual exclusion concurrency control policy.Types[edit]Generally, locks are advisory locks, where each thread cooperates by acquiring the lock before accessing the corresponding data. Some systems also implement mandatory locks, where attempting unauthorized access to a locked resource will force an exception in the entity attempting to make the access.The simplest type of lock is a binary semaphore. It provides exclusive access to the locked data. Other schemes also provide shared access for reading data. Other widely implemented access modes are exclusive, intend-to-exclude and intend-to-upgrade.Another way to classify locks is by what happens when the lock strategy prevents progress of a thread. Most locking designs block the execution of the thread requesting the lock until it is allowed to access the locked resource. With a spinlock, the thread simply waits (spins) until the lock becomes available. This is efficient if threads are blocked for a short time, because it avoids the overhead of operating system process re-scheduling. It is inefficient if the lock is held for a long time, or if the progress of the thread that is holding the lock depends on preemption of the locked thread.Locks typically require hardware support for efficient implementation. This support usually takes the form of one or more atomic instructions such as test-and-set, fetch-and-add or compare-and-swap. These instructions allow a single process to test if the lock is free, and if free, acquire the lock in a single atomic operation.Uniprocessor architectures have the option of using uninterruptable sequences of instructions\u2014using special instructions or instruction prefixes to disable interrupts temporarily\u2014but this technique does not work for multiprocessor shared-memory machines. Proper support for locks in a multiprocessor environment can require quite complex hardware or software support, with substantial synchronization issues.The reason an atomic operation is required is because of concurrency, where more than one task executes the same logic. For example, consider the following C code:The above example does not guarantee that the task has the lock, since more than one task can be testing the lock at the same time. Since both tasks will detect that the lock is free, both tasks will attempt to set the lock, not knowing that the other task is also setting the lock. Dekker's or Peterson's algorithm are possible substitutes if atomic locking operations are not available.Careless use of locks can result in deadlock or livelock. A number of strategies can be used to avoid or recover from deadlocks or livelocks, both at design-time and at run-time. (The most common strategy is to standardize the lock acquisition sequences so that combinations of inter-dependent locks are always acquired in a specifically defined cascade order.)Some languages do support locks syntactically. An example in C# follows:The code lock(this) can lead to problems if the instance can be accessed publicly.[1]Similar to Java, C# can also synchronize entire methods, by using the MethodImplOptions.Synchronized attribute.[2][3]Granularity[edit]Before being introduced to lock granularity, one needs to understand three concepts about locks:\nlock overhead: the extra resources for using locks, like the memory space allocated for locks, the CPU time to initialize and destroy locks, and the time for acquiring or releasing locks. The more locks a program uses, the more overhead associated with the usage;\nlock contention: this occurs whenever one process or thread attempts to acquire a lock held by another process or thread. The more fine-grained the available locks, the less likely one process/thread will request a lock held by the other. (For example, locking a row rather than the entire table, or locking a cell rather than the entire row.);\ndeadlock: the situation when each of at least two tasks is waiting for a lock that the other task holds. Unless something is done, the two tasks will wait forever.\nThere is a tradeoff between decreasing lock overhead and decreasing lock contention when choosing the number of locks in synchronization.An important property of a lock is its granularity. The granularity is a measure of the amount of data the lock is protecting. In general, choosing a coarse granularity (a small number of locks, each protecting a large segment of data) results in less lock overhead when a single process is accessing the protected data, but worse performance when multiple processes are running concurrently. This is because of increased lock contention. The more coarse the lock, the higher the likelihood that the lock will stop an unrelated process from proceeding. Conversely, using a fine granularity (a larger number of locks, each protecting a fairly small amount of data) increases the overhead of the locks themselves but reduces lock contention. Granular locking where each process must hold multiple locks from a common set of locks can create subtle lock dependencies. This subtlety can increase the chance that a programmer will unknowingly introduce a deadlock.[citation needed]In a database management system, for example, a lock could protect, in order of increasing granularity, part of a field, a field, a record, a data page, or an entire table. Coarse granularity, such as using table locks, tends to give the best performance for a single user, whereas fine granularity, such as record locks, tends to give the best performance for multiple users.Database locks[edit]Database locks can be used as a means of ensuring transaction synchronicity. i.e. when making transaction processing concurrent (interleaving transactions), using 2-phased locks ensures that the concurrent execution of the transaction turns out equivalent to some serial ordering of the transaction. However, deadlocks become an unfortunate side-effect of locking in databases. Deadlocks are either prevented by pre-determining the locking order between transactions or are detected using waits-for graphs. An alternate to locking for database synchronicity while avoiding deadlocks involves the use of totally ordered global timestamps.There are mechanisms employed to manage the actions of multiple concurrent users on a database\u2014the purpose is to prevent lost updates and dirty reads. The two types of locking are pessimistic locking and optimistic locking:\nPessimistic locking: a user who reads a record with the intention of updating it places an exclusive lock on the record to prevent other users from manipulating it. This means no one else can manipulate that record until the user releases the lock. The downside is that users can be locked out for a very long time, thereby slowing the overall system response and causing frustration.\n\n\n\nWhere to use pessimistic locking: this is mainly used in environments where data-contention (the degree of users request to the database system at any one time) is heavy; where the cost of protecting data through locks is less than the cost of rolling back transactions, if concurrency conflicts occur. Pessimistic concurrency is best implemented when lock times will be short, as in programmatic processing of records. Pessimistic concurrency requires a persistent connection to the database and is not a scalable option when users are interacting with data, because records might be locked for relatively large periods of time. It is not appropriate for use in Web application development.\n\n\n\nOptimistic locking: this allows multiple concurrent users access to the database whilst the system keeps a copy of the initial-read made by each user. When a user wants to update a record, the application determines whether another user has changed the record since it was last read. The application does this by comparing the initial-read held in memory to the database record to verify any changes made to the record. Any discrepancies between the initial-read and the database record violates concurrency rules and hence causes the system to disregard any update request. An error message is generated and the user is asked to start the update process again. It improves database performance by reducing the amount of locking required, thereby reducing the load on the database server. It works efficiently with tables that require limited updates since no users are locked out. However, some updates may fail. The downside is constant update failures due to high volumes of update requests from multiple concurrent users - it can be frustrating for users.\n\n\n\nWhere to use optimistic locking: this is appropriate in environments where there is low contention for data, or where read-only access to data is required. Optimistic concurrency is used extensively in .NET to address the needs of mobile and disconnected applications,[4] where locking data rows for prolonged periods of time would be infeasible. Also, maintaining record locks requires a persistent connection to the database server, which is not possible in disconnected applications.\n\n\nDisadvantages[edit]Lock-based resource protection and thread/process synchronization have many disadvantages:\nContention: some threads/processes have to wait until a lock (or a whole set of locks) is released. If one of the threads holding a lock dies, stalls, blocks, or enters an infinite loop, other threads waiting for the lock may wait forever.\nOverhead: the use of locks adds overhead for each access to a resource, even when the chances for collision are very rare. (However, any chance for such collisions is a race condition.)\nDebugging: bugs associated with locks are time dependent and can be very subtle and extremely hard to replicate, such as deadlocks.\nInstability: the optimal balance between lock overhead and lock contention can be unique to the problem domain (application) and sensitive to design, implementation, and even low-level system architectural changes. These balances may change over the life cycle of an application and may entail tremendous changes to update (re-balance).\nComposability: locks are only composable (e.g., managing multiple concurrent locks in order to atomically delete item X from table A and insert X into table B) with relatively elaborate (overhead) software support and perfect adherence by applications programming to rigorous conventions.\nPriority inversion: a low-priority thread/process holding a common lock can prevent high-priority threads/processes from proceeding. Priority inheritance can be used to reduce priority-inversion duration. The priority ceiling protocol can be used on uniprocessor systems to minimize the worst-case priority-inversion duration, as well as prevent deadlock.\nConvoying: all other threads have to wait if a thread holding a lock is descheduled due to a time-slice interrupt or page fault.\nSome concurrency control strategies avoid some or all of these problems. For example, a funnel or serializing tokens can avoid the biggest problem: deadlocks. Alternatives to locking include non-blocking synchronization methods, like lock-free programming techniques and transactional memory. However, such alternative methods often require that the actual lock mechanisms be implemented at a more fundamental level of the operating software. Therefore, they may only relieve the application level from the details of implementing locks, with the problems listed above still needing to be dealt with beneath the application.In most cases, proper locking depends on the CPU providing a method of atomic instruction stream synchronization (for example, the addition or deletion of an item into a pipeline requires that all contemporaneous operations needing to add or delete other items in the pipe be suspended during the manipulation of the memory content required to add or delete the specific item). Therefore, an application can often be more robust when it recognizes the burdens it places upon an operating system and is capable of graciously recognizing the reporting of impossible demands.[citation needed]Lack of composability[edit]One of lock-based programming's biggest problems is that locks don't compose: it is hard to combine small, correct lock-based modules into equally correct larger programs without modifying the modules or at least knowing about their internals. Simon Peyton Jones (an advocate of software transactional memory) gives the following example of a banking application:[5] design a class Account that allows multiple concurrent clients to deposit or withdraw money to an account; and give an algorithm to transfer money from one account to another. The lock-based solution to the first part of the problem is:\nclass Account:\n    member balance : Integer\n    member mutex : Lock\n    method deposit(n : Integer)\n           mutex.lock()\n           balance \u2190 balance + n\n           mutex.unlock()\n    method withdraw(n : Integer)\n           deposit(\u2212n)\nThe second part of the problem is much more complicated. A transfer routine that is correct for sequential programs would be\nfunction transfer(from : Account, to : Account, amount : integer)\n    from.withdraw(amount)\n    to.deposit(amount)\nIn a concurrent program, this algorithm is incorrect because when one thread is halfway through transfer, another might observe a state where amount has been withdrawn from the first account, but not yet deposited into the other account: money has gone missing from the system. This problem can only be fixed completely by taking locks on both account prior to changing any of the two accounts, but then the locks have to be taken according to some arbitrary, global ordering to prevent deadlock:\nfunction transfer(from : Account, to : Account, amount : integer)\n    if from < to    // arbitrary ordering on the locks\n        from.lock()\n        to.lock()\n    else\n        to.lock()\n        from.lock()\n    from.withdraw(amount)\n    to.deposit(amount)\n    from.unlock()\n    to.unlock()\nThis solution gets more complicated when more locks are involved, and the transfer function needs to know about all of the locks, so they cannot be hidden.Language support[edit]Programming languages vary in their support for synchronization:\nThe ISO/IEC C standard provides a standard mutual exclusion (locks) API since C11. The current ISO/IEC C++ standard supports threading facilities since C++11. The OpenMP standard is supported by some compilers, and allows critical sections to be specified using pragmas. The POSIX pthread API provides lock support.[6] Visual C++ provides the synchronize attribute of methods to be synchronized, but this is specific to COM objects in the Windows architecture and Visual C++ compiler.[7] C and C++ can easily access any native operating system locking features.\nObjective-C provides the keyword @synchronized[8] to put locks on blocks of code and also provides the classes NSLock,[9] NSRecursiveLock,[10] and NSConditionLock[11] along with the NSLocking protocol[12] for locking as well.\nC# provides the lock keyword on a thread to ensure its exclusive access to a resource.\nVB.NET provides a SyncLock keyword like C#'s lock keyword.\nJava provides the keyword synchronized to lock code blocks, methods or objects[13] and libraries featuring concurrency-safe data structures.\nPython provides a low-level mutex mechanism without a keyword.[14]\nRuby provides a low-level mutex object and no keyword.[15]\nAda provides protected objects that have visible protected subprograms or entries[16] as well as rendezvous.[17]\nx86 assembly provides the LOCK prefix on certain operations to guarantee their atomicity.\nPHP provides a file based locking [18] as well as a Mutex class in the pthreads extension. [19]\nSee also[edit]\nCritical section\nDouble-checked locking\nFile locking\nLock-free and wait-free algorithms\nMonitor (synchronization)\nMutual exclusion\nRead/write lock pattern\nSemaphore (programming)\nReferences[edit]External links[edit]\nTutorial on Locks and Critical Sections\n", "subtitles": ["Types", "Granularity", "Database locks", "Disadvantages", "Language support", "See also", "References", "External links"], "title": "Lock (computer science)"},
{"content": "A method in object-oriented programming (OOP) is a procedure associated with a message and an object. An object is mostly made up of data and behavior, which form the interface that an object presents to the outside world. Data is represented as properties of the object and behavior as methods. For example, a Window object would have methods such as open and close, while its state (whether it is opened or closed) would be a property.In class-based programming, methods are defined in a class, and objects are instances of a given class. One of the most important capabilities that a method provides is method overriding. The same name (e.g., area) can be used for multiple different kinds of classes. This allows the sending objects to invoke behaviors and to delegate the implementation of those behaviors to the receiving object. A method in Java programming sets the behavior of a class object. For example, an object can send an area message to another object and the appropriate formula is invoked whether the receiving object is a rectangle, circle, triangle, etc.Methods also provide the interface that other classes use to access and modify the data properties of an object. This is known as encapsulation. Encapsulation and overriding are the two primary distinguishing features between methods and procedure calls.[1]Overriding and overloading[edit]Method overriding and overloading are two of the most significant ways that a method differs from a conventional procedure or function call. Overriding refers to a subclass redefining the implementation of a method of its superclass. For example, findArea may be a method defined on a shape class. The various subclasses: rectangle, circle, triangle, etc. would each define the appropriate formula to calculate their area. The idea is to look at objects as black boxes so that changes to the internals of the object can be made with minimal impact on the other objects that use it. This is known as encapsulation and is meant to make code easier to maintain and re-use.Method overloading, on the other hand, refers to differentiating the code used to handle a message based on the parameters of the method. If one views the receiving object as the first parameter in any method then overriding is just a special case of overloading where the selection is based only on the first argument.[2] The following simple Java example illustrates the difference:[3]Accessor, mutator and manager methods[edit]Accessor methods are used to read data values of an object. Mutator methods are used to modify the data of an object. Manager methods are used to initialize and destroy objects of a class, e.g. constructors and destructors.These methods provide an abstraction layer that facilitates encapsulation and modularity. For example, if a bank-account class provides a getBalance() accessor method to retrieve the current balance (rather than directly accessing the balance data fields), then later revisions of the same code can implement a more complex mechanism for balance retrieval (e.g., a database fetch), without the dependent code needing to be changed. The concepts of encapsulation and modularity are not unique to object-oriented programming. Indeed, in many ways the object-oriented approach is simply the logical extension of previous paradigms such as abstract data types and structured programming.[4]Constructors[edit]A constructor is a method that is called at the beginning of an object's lifetime to create and initialize the object, a process called construction (or instantiation). Initialization may include an acquisition of resources. Constructors may have parameters but usually, do not return values in most languages. See the following example in Java:Destructors[edit]A destructor is a method that is called automatically at the end of an object's lifetime, a process called destruction. Destruction in most languages does not allow destructor method arguments nor return values. Destruction can be implemented so as to perform cleanup chores and other tasks at object destruction.Finalizers[edit]In garbage-collected languages, such as Java, C#, and Python, destructors are known as finalizers. They have a similar purpose and function to destructors, but because of the differences between languages that utilize garbage-collection and languages with manual memory management, the sequence in which they are called is different.Abstract methods[edit]An abstract method is one with only a signature and no implementation body. It is often used to specify that a subclass must provide an implementation of the method. Abstract methods are used to specify interfaces in some computer languages.[5]Example[edit]The following Java code shows an abstract class that needs to be extended:The following subclass extends the main class:Class methods[edit]Class methods are methods that are called on a class rather than an instance. They are typically used as part of an object meta-model. I.e, for each class, defined an instance of the class object in the meta-model is created. Meta-model protocols allow classes to be created and deleted. In this sense, they provide the same functionality as constructors and destructors described above. But in some languages such as the Common Lisp Object System (CLOS) the meta-model allows the developer to dynamically alter the object model at run time: e.g., to create new classes, redefine the class hierarchy, modify properties, etc.Special methods[edit]Special methods are very language-specific and a language may support none, some, or all of the special methods defined here. A language's compiler may automatically generate default special methods or a programmer may be allowed to optionally define special methods. Most special methods cannot be directly called, but rather the compiler generates code to call them at appropriate times.Static methods[edit]Static methods are meant to be relevant to all the instances of a class rather than to any specific instance. They are similar to static variables in that sense. An example would be a static method to sum the values of all the variables of every instance of a class. For example, if there were a Product class it might have a static method to compute the average price of all products.In Java, a commonly used static method is:\nMath.max(double a, double b)\nThis static method has no owning object and does not run on an instance. It receives all information from its arguments.[6]A static method can be invoked even if no instances of the class exist yet. Static methods are called static because they are resolved at compile time based on the class they are called on and not dynamically as in the case with instance methods, which are resolved polymorphically based on the runtime type of the object.Copy-assignment operators[edit]Copy-assignment operators define actions to be performed by the compiler when a class object is assigned to a class object of the same type.Operator methods[edit]Operator methods define or redefine operator symbols and define the operations to be performed with the symbol and the associated method parameters. C++ Example:Member functions in C++[edit]Some procedural languages were extended with object-oriented capabilities to leverage the large skill sets and legacy code for those languages but still provide the benefits of object-oriented development. Perhaps the most well-known example is C++, an object-oriented extension of the C programming language. Due to the design requirements to add the object-oriented paradigm on to an existing procedural language, message passing in C++ has some unique capabilities and terminologies. For example, in C++ a method is known as a member function. C++ also has the concept of virtual functions which are member functions that can be overridden in derived classes and allow for dynamic dispatch.Virtual functions[edit]Virtual functions are the means by which a C++ class can achieve polymorphic behavior. Non-virtual member functions, or regular methods, are those that do not participate in polymorphism.C++ Example:See also[edit]\nProperty (programming)\nRemote method invocation\nSubroutine, also called subprogram, routine, procedure, or function\nNotes[edit]References[edit]", "subtitles": ["Overriding and overloading", "Accessor, mutator and manager methods", "Abstract methods", "Class methods", "Special methods", "Member functions in C++", "See also", "Notes", "References"], "title": "Method (computer programming)"},
{"content": "For each (or foreach) is a control flow statement for traversing items in a collection. Foreach is usually used in place of a standard for statement. Unlike other for loop constructs, however, foreach loops[1] usually maintain no explicit counter: they essentially say do this to everything in this set, rather than do this x times. This avoids potential off-by-one errors and makes code simpler to read. In object-oriented languages an iterator, even if implicit, is often used as the means of traversal.The foreach statement in some languages has some defined order, processing each item in the collection from the first to the last. The foreach statement in many other languages does not have any particular order, especially array programming languages, in order to support loop optimization in general and in particular to allow vector processing to process some or all of the items in the collection simultaneously.Syntax[edit]Syntax varies among languages. Most use the simple word for, roughly as follows:\nfor each item in collection:\n  do something to item\nLanguage support[edit]Programming languages which support foreach loops include ABC, ActionScript, Ada, C++11, C#, ColdFusion Markup Language (CFML), Cobra, D, Daplex (query language), ECMAScript, Erlang, Java (since 1.5, using the reserved word for for the for loop and the foreach loop), JavaScript, Lua, Objective-C (since 2.0), ParaSail, Perl, PHP, Python, REALbasic, Ruby, Scala, Smalltalk, Swift, Tcl, tcsh, Unix shells, Visual Basic .NET, and Windows PowerShell. Notable languages without foreach are C, and C++ pre-C++11.ActionScript[edit]ActionScript supports foreach loops by key/index and by value:Typical usage is as shown, but someArray could be any object, and someObject could be an array.Ada[edit]Ada supports foreach loops as part of the normal for loop. Say X is an array:This syntax is used on mostly arrays, but will also work with other types when a full iteration is needed.Ada 2012 has generalized loops to foreach loops on any kind of container (array, lists, maps...):C[edit]The language C does not have collections or a foreach construct. However, it has several standard data structures that can be used as collections, and foreach can be made easily with a macro.However, two obvious problems occur:\nThe macro is unhygienic: it declares a new variable in the existing scope which remains after the loop.\nOne foreach macro cannot be defined that works with different collection types (e.g., array and linked list) or that is extensible to user types.\nC string as a collection of charC int array as a collection of int (array size known at compile-time)Most general: string or array as collection (collection size known at run-time)\nNote: idxtype can be removed and typeof(col[0]) used in its place with GCC\nC#[edit]In C#, assuming that myArray is an array of integers:Language Integrated Query (LINQ) provides the following syntax, accepting a delegate or lambda expression:C++[edit]C++11 provides a foreach loop. The syntax is similar to that of Java:Currently, C++11 range-based for statements have been implemented in GNU Compiler Collection (GCC) (since version 4.6), Clang (since version 3.0) and Visual C++ 2012 (version 11 [2])The C++ STL also supports for_each[3], that applies each element to a function, which can be any predefined function or a lambda expression. While the direction of the range-based for is only from the beginning to the end, you can change the direction or range by altering the first two parameters.Qt, a C++ framework, offers a macro providing foreach loops[4] using the STL iterator interface:Boost, a set of free peer-reviewed portable C++ libraries also provides foreach loops:[5]C++/CLI[edit]The C++/CLI language proposes a construct similar to C#.Assuming that myArray is an array of integers:ColdFusion Markup Language (CFML)[edit]Script syntax[edit]Tag syntax[edit]CFML incorrectly identifies the value as index in this construct; the index variable does receive the actual value of the array element, not its index.Common Lisp[edit]Common Lisp provides foreach ability either with the dolist macro:or the powerful loop macro to iterate on more data typesand even with the mapcar function:D[edit]Dart[edit]Object Pascal, Delphi[edit]Foreach support was added in Delphi 2005, and uses an enumerator variable that must be declared in the var section.Eiffel[edit]The iteration (foreach) form of the Eiffel loop construct is introduced by the keyword across.In this example, every element of the structure my_list is printed:The local entity ic is an instance of the library class ITERATION_CURSOR. The cursor's feature item provides access to each structure element. Descendants of class ITERATION_CURSOR can be created to handle specialized iteration algorithms. The types of objects that can be iterated across (my_list in the example) are based on classes that inherit from the library class ITERABLE.The iteration form of the Eiffel loop can also be used as a boolean expression when the keyword loop is replaced by either all (effecting universal quantification) or some (effecting existential quantification).This iteration is a boolean expression which is true if all items in my_list have counts greater than three:The following is true if at least one item has a count greater than three:Go[edit]Go's foreach loop can be used to loop over an array, slice, string, map, or channel.Using the two-value form, we get the index/key (first element) and the value (second element):Using the one-value form, we get the index/key (first element):[6]Groovy[edit]Groovy supports for loops over collections like arrays, lists and ranges:Groovy also supports a C-style for loop with an array index:Collections in Groovy can also be iterated over using the each keyword and a closure. By default, the loop dummy is named itHaskell[edit]Haskell allows looping over lists with monadic actions using mapM_ and forM_ (mapM_ with its arguments flipped) from Control.Monad:It's also possible to generalize those functions to work on applicative functors rather than monads and any data structure that is traversable using traverse (for with its arguments flipped) and mapM (forM with its arguments flipped) from Data.Traversable.Haxe[edit]Java[edit]In Java, a foreach-construct was introduced in Java Development Kit (JDK) 1.5.0.[7]Official sources use several names for the construct. It is referred to as the Enhanced for Loop,[7] the For-Each Loop,[8] and the foreach statement.[9]JavaScript[edit]For unordered iteration over the keys in an Object, JavaScript features the for...in loop:To limit the iteration to the object's own properties, excluding those inherited through the prototype chain, it is sometimes useful to add a hasOwnProperty() test, if supported by the JavaScript engine (for WebKit/Safari, this means in version 3 or later).In ECMAScript 5 it is possible to use the keys method of the Object function to iterate over the own keys of an object more naturally. [10]In ECMAScript 5 it's also possible to use the forEach method of a native array.[11]Gecko\u2019s JavaScript engine also has a for each...in statement, which iterates over the values in the object, not the keys.[12]Also note that it is inadvisable to use either a for...in or for each...in statement on an Array object in JavaScript, due to the above issue of properties inherited from prototypes, and also because it only iterates over existent keys and is not guaranteed to iterate over the elements in any particular order.[13] A regular C-style for loop should be used instead. The EcmaScript 6 standard has for..of for index-less iteration over generators, arrays and more.Lua[14][edit]Iterate only through numerical index values:Iterate through all index values:Mathematica[edit]In Mathematica, Do will simply evaluate an expression for each element of a list, without returning any value.It is more common to use Table, which returns the result of each evaluation in a new list.MATLAB[edit]Mint[edit]For each loops are supported in Mint, possessing the following syntax:The for (;;) or while (true) infinite loop in Mint can be written using a for each loop and an infinitely long list.[15]Objective-C[edit]Foreach loops, called Fast enumeration, are supported starting in Objective-C 2.0. They can be used to iterate over any object that implements the NSFastEnumeration protocol, including NSArray, NSDictionary (iterates over keys), NSSet, etc.NSArrays can also broadcast a message to their members:Where blocks are available, an NSArray can automatically perform a block on every contained item:The type of collection being iterated will dictate the item returned with each iteration. For example:OCaml[edit]OCaml is a functional language. Thus, the equivalent of a foreach loop can be achieved as a library function over lists and arrays.For lists:or in short way:For arrays:or in short way:ParaSail[edit]The ParaSail parallel programming language supports several kinds of iterators, including a general for each iterator over a container:ParaSail also supports filters on iterators, and the ability to refer to both the key and the value of a map. Here is a forward iteration over the elements of My_Map selecting only elements where the keys are in My_Set:Pascal[edit]In Pascal, ISO standard 10206:1990 introduced iteration over set types, thus:Perl[edit]In Perl, foreach (which is equivalent to the shorter for) can be used to traverse elements of a list. The expression which denotes the collection to loop over is evaluated in list-context and each item of the resulting list is, in turn, aliased to the loop variable.List literal example:Array examples:Hash example:Direct modification of collection members:Perl 6[edit]In Perl 6, a distinct language from Perl 5, for must be used to traverse elements of a list. (foreach is no longer allowed.) The expression which denotes the collection to loop over is evaluated in list-context, but not flattened by default, and each item of the resulting list is, in turn, aliased to the loop variable(s).List literal example:Array examples:Hash example:ororDirect modification of collection members:PHP[edit]It is also possible to extract both keys and values using the alternate syntax:Direct modification of collection members:\nMore information\nPython[edit]Python's tuple assignment, fully available in its foreach loop, also makes it trivial to iterate on (key, value) pairs in associative arrays:As for ... in is the only kind of for loop in Python, the equivalent to the counter loop found in other languages is...... though using the enumerate function is considered more Pythonic:Racket[edit]or using the conventional Scheme for-each function:do-something-with is a one-argument function.Ruby[edit]orThis can also be used with a hash.Scala[edit]Scheme[edit]do-something-with is a one-argument function.Smalltalk[edit]Swift[edit]Swift uses the for...in construct to iterate over members of a collection.[16]The for...in loop is often used with the closed and half-open range constructs to iterate over the loop body a certain number of times.SystemVerilog[edit]SystemVerilog supports iteration over any vector or array type of any dimensionality using the foreach keyword.A trivial example iterates over an array of integers:A more complex example iterates over an associative array of arrays of integers:Tcl[edit]Tcl uses foreach to iterate over lists. It is possible to specify more than one iterator variable, in which case they are assigned sequential values from the list.It is also possible to iterate over more than one list simultaneously. In the following i assumes sequential values of the first list, j sequential values of the second list:Visual Basic .NET[edit]or without type inferenceWindows PowerShell[edit]From a pipelineExtensible Stylesheet Language (XSL)[edit][17]See also[edit]\nDo while loop\nFor loop\nWhile loop\nMap (higher-order function)\nReferences[edit]", "subtitles": ["Syntax", "Language support", "See also", "References"], "title": "Foreach loop"},
{"content": "In computer science, a NOP, no-op, or NOOP (pronounced no op; short for no operation) is an assembly language instruction, programming language statement, or computer protocol command that does nothing.Machine set of directions[edit]Some computer instruction sets include an instruction whose explicit purpose is to not change the state of any of the programmer-accessible registers, status flags, or memory. It often takes a well-defined number of clock cycles to execute. In other instruction sets, a NOP can be simulated by executing an instruction having operands that cause the same effect; e.g., on the SPARC processor, the instruction sethi 0, %g0 is the recommended solution.A NOP is most commonly used for timing purposes, to force memory alignment, to prevent hazards, to occupy a branch delay slot, to render void an existing instruction such as a jump, or as a place-holder to be replaced by active instructions later on in program development (or to replace removed instructions when reorganizing would be problematic or time-consuming). In some cases, a NOP can have minor side effects; for example, on the Motorola 68000 series of processors, the NOP opcode causes a synchronization of the pipeline.[1]Listed below are the NOP instruction for some CPU architectures:From a hardware design point of view, unmapped areas of a bus are often designed to return zeroes; since the NOP slide behavior is often desirable, it gives a bias to coding it with the all-zeroes opcode.Code[edit]NOP is sometimes used as a description for the action performed by a function or a sequence of programming language statements if the function or code has no effect (it might also be called redundant code). A common compiler optimization is the detection and removal of this kind of code. Such code may be required by the grammar of the programming language, which does not allow a blank.Ada[edit]In Ada, the null statement serves as a NOP.[6] As the syntax forbids that control statements or functions be empty, the null statement must be used to specify that no action is required. (Thus, if the programmer forgets to write a sequence of statements, the program will fail to compile.)C and derivatives[edit]The second line below is an example of a single C statement that behaves like a NOP. In practice, most compilers will not generate code for this statement:\n  int i = 0;\n  i+1;\nThis statement performs an addition and discards the result. Indeed, any statement without side effects (and that does not affect control flow, e.g., break, return) can be removed, as the result of the computation is discarded.The simplest possible statement in C that behaves like a NOP is the so-called null statement, which is just a semi-colon in a context requiring a statement. (A compiler is not required to generate a NOP instruction in this case; typically, no instructions whatsoever would be generated.)\n  ;\nAlternatively, an empty block (compound statement) may be used, and may be more legible:\n  {}\nIn some cases, such as the body of a function, a block must be used, but this can be empty. In C, statements cannot be empty \u2013 simple statements must end with a ; (semicolon) while compound statements are enclosed in {} (braces), which does not itself need a following semicolon. Thus in contexts where a statement is grammatically required, some such null statement can be used.The null statement is useless by itself, but it can have a syntactic use in a wider context, e.g., within the context of a loop:alternatively,or more tersely:(note that the last form may be confusing, and as such generates a warning with some compilers or compiler options, as semicolon usually indicates an end of function call instruction when placed after a round parenthesis on the end of line).The above code continues calling the function getchar() until it returns a \n (newline) character, essentially fast-forwarding the current reading location of standard input to the beginning of next line.Fortran[edit]In Fortran, the CONTINUE statement is used in some contexts such as the last statement in a DO loop, although it can be used anywhere, and does not have any functionality.Pascal[edit]As with C, the ; used by itself can be used as a null statement. In fact, due to the specification of the language, in a BEGIN / END block, the semicolon is optional before the END statement, thus a semicolon used there is superfluous.Also, a block consisting of BEGIN END; may be used as a placeholder to indicate no action, even if placed inside another BEGIN / END block.Python[edit]The Python programming language has a pass statement which has no effect when executed and thus serves as a NOP. It is primarily used to ensure correct syntax due to Python's indentation-sensitive syntax; for example the syntax for definition of a class requires an indented block with the class logic, which has to be expressed as pass when it should be empty.jQuery[edit]The jQuery library provides a function jQuery.noop(), which does nothing.[7]Angular[edit]The Angular framework provides angular.noop function that performs no operations.Shell Scripting (bash, zsh, etc)[edit]The ':' [colon] character is a shell builtin that has similar effect to a NOP (a do-nothing operation). It's not technically an NOP, as it changes the special parameter $? (exit status of last command) to 0. It may be considered a synonym for the shell builtin 'true', and its exit status is true (0).[8][9][10]Lodash[edit]The Lodash library provides a function _.noop(), which returns undefined and does nothing.[11]NOP protocol commands[edit]Many computer protocols, such as telnet, include a NOP command that a client can issue to request a response from the server without requesting any other actions. Such a command can be used to ensure the connection is still alive or that the server is responsive. A NOOP command is part of the following protocols (this is a partial list):\ntelnet\nFTP\nSMTP\nX11\nPOP3\nNNTP\nfinger\nIMAP4\nBitTorrent\nCharacter encoding: the null control character\nNote that unlike the other protocols listed, the IMAP4 NOOP command has a specific purpose - it allows the server to send any pending notifications to the client.While most telnet or FTP servers respond to a NOOP command with OK or +OK, some programmers have added quirky responses to the client. For example, the ftpd daemon of MINIX responds to NOOP with the message:[12]\n200 NOOP to you too!\nCracking[edit]NOPs are often involved when cracking software that checks for serial numbers, specific hardware or software requirements, presence or absence of hardware dongles, etc. This is accomplished by altering functions and subroutines to bypass security checks and instead simply return the expected value being checked for. Because most of the instructions in the security check routine will be unused, these would be replaced with NOPs, thus removing the software's security functionality without attracting any attention.Security exploits[edit]The NOP opcode can be used to form a NOP slide, which allows code to execute when the exact value of the instruction pointer is indeterminate (e.g., when a buffer overflow causes a function's return address on the stack to be overwritten).See also[edit]\nComputer architecture\nHLT (x86 instruction)\nIdentity function \u2013 the functional programming equivalent to NOOP\nxyzzy (command) \u2013 a command sometimes used instead of NOOP\nIEFBR14\nFiller text\nComment (computer programming) \u2013 annotations generally for programmers that are ignored by compilers and interpreters\nReferences[edit]", "subtitles": ["Machine set of directions", "Code", "NOP protocol commands", "Cracking", "Security exploits", "See also", "References"], "title": "NOP"},
{"content": "In computer programming, a subroutine is a sequence of program instructions that perform a specific task, packaged as a unit. This unit can then be used in programs wherever that particular task should be performed.Subprograms may be defined within programs, or separately in libraries that can be used by multiple programs. In different programming languages, a subroutine may be called a procedure, a function, a routine, a method, or a subprogram. The generic term callable unit is sometimes used.[1]The name subprogram suggests a subroutine behaves in much the same way as a computer program that is used as one step in a larger program or another subprogram. A subroutine is often coded so that it can be started (called) several times and from several places during one execution of the program, including from other subroutines, and then branch back (return) to the next instruction after the call, once the subroutine's task is done. Maurice Wilkes, David Wheeler, and Stanley Gill are credited with the invention of this concept, which they termed a closed subroutine,[2][3] contrasted with an open subroutine or macro.[4]Subroutines are a powerful programming tool,[5] and the syntax of many programming languages includes support for writing and using them. Judicious use of subroutines (for example, through the structured programming approach) will often substantially reduce the cost of developing and maintaining a large program, while increasing its quality and reliability.[6] Subroutines, often collected into libraries, are an important mechanism for sharing and trading software. The discipline of object-oriented programming is based on objects and methods (which are subroutines attached to these objects or object classes).In the compiling method called threaded code, the executable program is basically a sequence of subroutine calls.Main concepts[edit]The content of a subroutine is its body, which is the piece of program code that is executed when the subroutine is called or invoked.A subroutine may be written so that it expects to obtain one or more data values from the calling program (to replace its parameters or formal parameters). The calling program provides actual values for these parameters, called arguments. Different programming languages may use different conventions for passing arguments:The subroutine may return a computed value to its caller (its return value), or provide various result values or output parameters. Indeed, a common use of subroutines is to implement mathematical functions, in which the purpose of the subroutine is purely to compute one or more results whose values are entirely determined by the arguments passed to the subroutine. (Examples might include computing the logarithm of a number or the determinant of a matrix.)A subroutine call may also have side effects such as modifying data structures in a computer memory, reading from or writing to a peripheral device, creating a file, halting the program or the machine, or even delaying the program's execution for a specified time. A subprogram with side effects may return different results each time it is called, even if it is called with the same arguments. An example is a random number function, available in many languages, that returns a different pseudo-random number each time it is called. The widespread use of subroutines with side effects is a characteristic of imperative programming languages.A subroutine can be coded so that it may call itself recursively, at one or more places, to perform its task. This method allows direct implementation of functions defined by mathematical induction and recursive divide and conquer algorithms.A subroutine whose purpose is to compute one boolean-valued function (that is, to answer a yes/no question) is sometimes called a predicate. In logic programming languages, often[vague] all subroutines are called predicates, since they primarily[vague] determine success or failure.[citation needed]Language support[edit]High-level programming languages usually include specific constructs to:\ndelimit the part of the program (body) that makes up the subroutine\nassign an identifier (name) to the subroutine\nspecify the names and data types of its parameters and return values\nprovide a private naming scope for its temporary variables\nidentify variables outside the subroutine that are accessible within it\ncall the subroutine\nprovide values to its parameters\nthe main program contains the address of the subprogram\nthe sub program contains the address of next instruction of the function call in main program\nspecify the return values from within its body\nreturn to the calling program\ndispose of the values returned by a call\nhandle any exceptional conditions encountered during the call\npackage subroutines into a module, library, object, class, etc.\nSome programming languages, such as Pascal, Fortran, Ada and many dialects of BASIC, distinguish between functions or function subprograms, which provide an explicit return value to the calling program, and subroutines or procedures, which do not. In those languages, function calls are normally embedded in expressions (e.g., a sqrt function may be called as y = z + sqrt(x)). Procedure calls either behave syntactically as statements (e.g., a print procedure may be called as if x > 0 then print(x) or are explicitly invoked by a statement such as CALL or GOSUB (e.g. call print(x)). Other languages, such as C and Lisp, do not distinguish between functions and subroutines.In strictly functional programming languages such as Haskell, subprograms can have no side effects, which means that various internal states of the program will not change. Functions will always return the same result if repeatedly called with the same arguments. Such languages typically only support functions, since subroutines that do not return a value have no use unless they can cause a side effect.In programming languages such as C, C++, and C#, subroutines may also simply be called functions, not to be confused with mathematical functions or functional programming, which are different concepts.A language's compiler will usually translate procedure calls and returns into machine instructions according to a well-defined calling convention, so that subroutines can be compiled separately from the programs that call them. The instruction sequences corresponding to call and return statements are called the procedure's prologue and epilogue.Advantages[edit]The advantages of breaking a program into subroutines include:\nDecomposing a complex programming task into simpler steps: this is one of the two main tools of structured programming, along with data structures\nReducing duplicate code within a program\nEnabling reuse of code across multiple programs\nDividing a large programming task among various programmers, or various stages of a project\nHiding implementation details from users of the subroutine\nImproving traceability (i.e. most languages offer ways to obtain the call trace which includes the names of the involved subroutines and perhaps even more information such as file names and line numbers); by not decomposing the code into subroutines, debugging would be impaired severely\nDisadvantages[edit]Invoking a subroutine (versus using in-line code) imposes some computational overhead in the call mechanism.A subroutine typically requires standard housekeeping code \u2013 both at entry to, and exit from, the function (function prologue and epilogue \u2013 usually saving general purpose registers and return address as a minimum).History[edit]The idea of a subroutine was worked out after computing machines had already existed for some time. The arithmetic and conditional jump instructions were planned ahead of time and have changed relatively little; but the special instructions used for procedure calls have changed greatly over the years. The earliest computers and microprocessors, such as the Small-Scale Experimental Machine and the RCA 1802, did not have a single subroutine call instruction. Subroutines could be implemented, but they required programmers to use the call sequence\u2014a series of instructions\u2014at each call site. Some very early computers and microprocessors, such as the IBM 1620, the Intel 8008, and the PIC microcontrollers, have a single-instruction subroutine call that uses dedicated hardware stack to store return addresses\u2014such hardware supports only a few levels of subroutine nesting, but can support recursive subroutines. Machines before the mid 1960s\u2014such as the UNIVAC I, the PDP-1, and the IBM 1130\u2014typically use a calling convention which saved the instruction counter in the first memory location of the called subroutine. This allows arbitrarily deep levels of subroutine nesting, but does not support recursive subroutines. The PDP-11 (1970) is one of the first computers with a stack-pushing subroutine call instruction; this feature supports both arbitrarily deep subroutine nesting and also supports recursive subroutines.[7]Language support[edit]In the very early assemblers, subroutine support was limited. Subroutines were not explicitly separated from each other or from the main program, and indeed the source code of a subroutine could be interspersed with that of other subprograms. Some assemblers would offer predefined macros to generate the call and return sequences. By the 1960s, assemblers usually had much more sophisticated support for both inline and separately assembled subroutines that could be linked together.Subroutine libraries[edit]Even with this cumbersome approach, subroutines proved very useful. For one thing they allowed use of the same code in many different programs. Moreover, memory was a very scarce resource on early computers, and subroutines allowed significant savings in the size of programs.Many early computers loaded the program instructions into memory from a punched paper tape. Each subroutine could then be provided by a separate piece of tape, loaded or spliced before or after the main program (or mainline[8]); and the same subroutine tape could then be used by many different programs. A similar approach applied in computers which used punched cards for their main input. The name subroutine library originally meant a library, in the literal sense, which kept indexed collections of tapes or card-decks for collective use.Return by indirect jump[edit]To remove the need for self-modifying code, computer designers eventually provided an indirect jump instruction, whose operand, instead of being the return address itself, was the location of a variable or processor register containing the return address.On those computers, instead of modifying the subroutine's return jump, the calling program would store the return address in a variable so that when the subroutine completed, it would execute an indirect jump that would direct execution to the location given by the predefined variable.Jump to subroutine[edit]Another advance was the jump to subroutine instruction, which combined the saving of the return address with the calling jump, thereby minimizing overhead significantly.In the IBM System/360, for example, the branch instructions BAL or BALR, designed for procedure calling, would save the return address in a processor register specified in the instruction. To return, the subroutine had only to execute an indirect branch instruction (BR) through that register. If the subroutine needed that register for some other purpose (such as calling another subroutine), it would save the register's contents to a private memory location or a register stack.In systems such as the HP 2100, the JSB instruction would perform a similar task, except that the return address was stored in the memory location that was the target of the branch. Execution of the procedure would actually begin at the next memory location. In the HP 2100 assembly language, one would write, for example\n       ...\n       JSB MYSUB    (Calls subroutine MYSUB.)\n BB    ...          (Will return here after MYSUB is done.)\nto call a subroutine called MYSUB from the main program. The subroutine would be coded as\n MYSUB NOP          (Storage for MYSUB's return address.)\n AA    ...          (Start of MYSUB's body.)\n       ...\n       JMP MYSUB,I  (Returns to the calling program.)\nThe JSB instruction placed the address of the NEXT instruction (namely, BB) into the location specified as its operand (namely, MYSUB), and then branched to the NEXT location after that (namely, AA = MYSUB + 1). The subroutine could then return to the main program by executing the indirect jump JMP MYSUB,I which branched to the location stored at location MYSUB.Compilers for Fortran and other languages could easily make use of these instructions when available. This approach supported multiple levels of calls; however, since the return address, parameters, and return values of a subroutine were assigned fixed memory locations, it did not allow for recursive calls.Incidentally, a similar method was used by Lotus 1-2-3, in the early 1980s, to discover the recalculation dependencies in a spreadsheet. Namely, a location was reserved in each cell to store the return address. Since circular references are not allowed for natural recalculation order, this allows a tree walk without reserving space for a stack in memory, which was very limited on small computers such as the IBM PC.Call stack[edit]Most modern implementations of a subroutine call use a call stack, a special case of the stack data structure, to implement subroutine calls and returns. Each procedure call creates a new entry, called a stack frame, at the top of the stack; when the procedure returns, its stack frame is deleted from the stack, and its space may be used for other procedure calls. Each stack frame contains the private data of the corresponding call, which typically includes the procedure's parameters and internal variables, and the return address.The call sequence can be implemented by a sequence of ordinary instructions (an approach still used in reduced instruction set computing (RISC) and very long instruction word (VLIW) architectures), but many traditional machines designed since the late 1960s have included special instructions for that purpose.The call stack is usually implemented as a contiguous area of memory. It is an arbitrary design choice whether the bottom of the stack is the lowest or highest address within this area, so that the stack may grow forwards or backwards in memory; however, many architectures chose the latter.[citation needed]Some designs, notably some Forth implementations, used two separate stacks, one mainly for control information (like return addresses and loop counters) and the other for data. The former was, or worked like, a call stack and was only indirectly accessible to the programmer through other language constructs while the latter was more directly accessible.When stack-based procedure calls were first introduced, an important motivation was to save precious memory.[citation needed] With this scheme, the compiler does not have to reserve separate space in memory for the private data (parameters, return address, and local variables) of each procedure. At any moment, the stack contains only the private data of the calls that are currently active (namely, which have been called but haven't returned yet). Because of the ways in which programs were usually assembled from libraries, it was (and still is) not uncommon to find programs that include thousands of subroutines, of which only a handful are active at any given moment.[citation needed] For such programs, the call stack mechanism could save significant amounts of memory. Indeed, the call stack mechanism can be viewed as the earliest and simplest method for automatic memory management.However, another advantage of the call stack method is that it allows recursive subroutine calls, since each nested call to the same procedure gets a separate instance of its private data.Delayed stacking [edit]One disadvantage of the call stack mechanism is the increased cost of a procedure call and its matching return.[clarification needed] The extra cost includes incrementing and decrementing the stack pointer (and, in some architectures, checking for stack overflow), and accessing the local variables and parameters by frame-relative addresses, instead of absolute addresses. The cost may be realized in increased execution time, or increased processor complexity, or both.This overhead is most obvious and objectionable in leaf procedures or leaf functions, which return without making any procedure calls themselves.[9][10][11] To reduce that overhead, many modern compilers try to delay the use of a call stack until it is really needed.[citation needed] For example, the call of a procedure P may store the return address and parameters of the called procedure in certain processor registers, and transfer control to the procedure's body by a simple jump. If procedure P returns without making any other call, the call stack is not used at all. If P needs to call another procedure Q, it will then use the call stack to save the contents of any registers (such as the return address) that will be needed after Q returns.C and C++ examples[edit]In the C and C++ programming languages, subprograms are termed functions (further classified as member functions when associated with a class, or free functions[12] when not). These languages use the special keyword void to indicate that a function takes no parameters (especially in C) or does not return any value. Note that C/C++ functions can have side-effects, including modifying any variables whose addresses are passed as parameters (i.e., passed by reference). Examples:The function does not return a value and has to be called as a stand-alone function, e.g., function1();This function returns a result (the number 5), and the call can be part of an expression, e.g., x + function2()This function converts a number between 0 and 6 into the initial letter of the corresponding day of the week, namely 0 to 'S', 1 to 'M', ..., 6 to 'S'. The result of calling it might be assigned to a variable, e.g., num_day = function3(number);.This function does not return a value but modifies the variable whose address is passed as the parameter; it would be called with function4(&variable_to_increment);.Small Basic example[edit]In the example above, Example() calls the subroutine[13].To define the actual subroutine, the Sub keyword must be used, with the subroutine name following Sub. After content has followed, EndSub must be typed.Visual Basic 6 examples[edit]In the Visual Basic 6 language, subprograms are termed functions or subs (or methods when associated with a class). Visual Basic 6 uses various terms called types to define what is being passed as a parameter. By default, an unspecified variable is registered as a variant type and can be passed as ByRef (default) or ByVal. Also, when a function or sub is declared, it is given a public, private, or friend designation, which determines whether it can be accessed outside the module or project that it was declared in.\nBy value [ByVal] \u2013 a way of passing the value of an argument to a procedure by passing a copy of the value, instead of passing the address. As a result, the variable's actual value can't be changed by the procedure to which it is passed.\nBy reference [ByRef] \u2013 a way of passing the value of an argument to a procedure by passing an address of the variable, instead of passing a copy of its value. This allows the procedure to access the actual variable. As a result, the variable's actual value can be changed by the procedure to which it is passed. Unless otherwise specified, arguments are passed by reference.\nPublic (optional) \u2013 indicates that the function procedure is accessible to all other procedures in all modules. If used in a module that contains an Option Private, the procedure is not available outside the project.\nPrivate (optional) \u2013 indicates that the function procedure is accessible only to other procedures in the module where it is declared.\nFriend (optional) \u2013 used only in a class module. Indicates that the Function procedure is visible throughout the project, but not visible to a controller of an instance of an object.\nThe function does not return a value and has to be called as a stand-alone function, e.g., Function1This function returns a result (the number 5), and the call can be part of an expression, e.g., x + Function2()This function converts a number between 0 and 6 into the initial letter of the corresponding day of the week, namely 0 to 'M', 1 to 'T', ..., 6 to 'S'. The result of calling it might be assigned to a variable, e.g., num_day = Function3(number).This function does not return a value but modifies the variable whose address is passed as the parameter; it would be called with Function4(variable_to_increment).PL/I example[edit]In PL/I a called procedure may be passed a descriptor providing information about the argument, such as string lengths and array bounds. This allows the procedure to be more general and eliminates the need for the programmer to pass such information. By default PL/I passes arguments by reference. A (trivial) subroutine to change the sign of each element of a two-dimensional array might look like:\n  change_sign: procedure(array);\n    declare array(*,*) float;\n    array = -array;\n    end change_sign;\nThis could be called with various arrays as follows:\n  /* first array bounds from -5 to +10 and 3 to 9 */\n  declare array1 (-5:10, 3:9)float;\n  /* second array bounds from 1 to 16 and 1 to 16 */\n  declare array2 (16,16) float;\n  call change_sign(array1);\n  call change_sign(array2);\nLocal variables, recursion and reentrancy[edit]A subprogram may find it useful to make use of a certain amount of scratch space; that is, memory used during the execution of that subprogram to hold intermediate results. Variables stored in this scratch space are termed local variables, and the scratch space is termed an activation record. An activation record typically has a return address that tells it where to pass control back to when the subprogram finishes.A subprogram may have any number and nature of call sites. If recursion is supported, a subprogram may even call itself, causing its execution to suspend while another nested execution of the same subprogram occurs. Recursion is a useful means to simplify some complex algorithms and break down complex problems. Recursive languages generally provide a new copy of local variables on each call. If the programmer desires the value of local variables to stay the same between calls, they can be declared static in some languages, or global values or common areas can be used. Here is an example of recursive subroutine in C/C++ to find Fibonacci numbers:Early languages like Fortran did not initially support recursion because variables were statically allocated, as well as the location for the return address. Most computers before the late 1960s such as the PDP-8 did not have support for hardware stack registers.[citation needed]Modern languages after ALGOL such as PL/1 and C almost invariably use a stack, usually supported by most modern computer instruction sets to provide a fresh activation record for every execution of a subprogram. That way, the nested execution is free to modify its local variables without concern for the effect on other suspended executions in progress. As nested calls accumulate, a call stack structure is formed, consisting of one activation record for each suspended subprogram. In fact, this stack structure is virtually ubiquitous, and so activation records are commonly termed stack frames.Some languages such as Pascal and Ada also support nested subroutines, which are subroutines callable only within the scope of an outer (parent) subroutine. Inner subroutines have access to the local variables of the outer subroutine that called them. This is accomplished by storing extra context information within the activation record, also termed a display.If a subprogram can be executed properly even when another execution of the same subprogram is already in progress, that subprogram is said to be reentrant. A recursive subprogram must be reentrant. Reentrant subprograms are also useful in multi-threaded situations, since multiple threads can call the same subprogram without fear of interfering with each other. In the IBM CICS transaction processing system, quasi-reentrant was a slightly less restrictive, but similar, requirement for application programs that were shared by many threads.In a multi-threaded environment, there is generally more than one stack. An environment that fully supports coroutines or lazy evaluation may use data structures other than stacks to store their activation records.Overloading[edit]In strongly typed languages, it is sometimes desirable to have a number of functions with the same name, but operating on different types of data, or with different parameter profiles. For example, a square root function might be defined to operate on reals, complex values or matrices. The algorithm to be used in each case is different, and the return result may be different. By writing three separate functions with the same name, the programmer has the convenience of not having to remember different names for each type of data. Further if a subtype can be defined for the reals, to separate positive and negative reals, two functions can be written for the reals, one to return a real when the parameter is positive, and another to return a complex value when the parameter is negative.In object-oriented programming, when a series of functions with the same name can accept different parameter profiles or parameters of different types, each of the functions is said to be overloaded.Here is an example of subroutine overloading in C++:In this code there are two functions of same name but they have different parameters.As another example, a subroutine might construct an object that will accept directions, and trace its path to these points on screen. There are a plethora of parameters that could be passed in to the constructor (colour of the trace, starting x and y co-ordinates, trace speed). If the programmer wanted the constructor to be able to accept only the color parameter, then he could call another constructor that accepts only color, which in turn calls the constructor with all the parameters passing in a set of default values for all the other parameters (X and Y would generally be centered on screen or placed at the origin, and the speed would be set to another value of the coder's choosing).Closures[edit]A closure is a subprogram together with the values of some of its variables captured from the environment in which it was created. Closures were a notable feature of the Lisp programming language, introduced by John McCarthy. Depending on the implementation, closures can serve as a mechanism for side-effects.Conventions[edit]A wide number of conventions for the coding of subroutines have been developed. Pertaining to their naming, many developers have adopted the approach that the name of a subroutine should be a verb when it does a certain task, an adjective when it makes some inquiry, and a noun when it is used to substitute variables.Some programmers suggest that a subroutine should perform only one task, and if a subroutine does perform more than one task, it should be split up into more subroutines. They argue that subroutines are key components in code maintenance, and their roles in the program must remain distinct.Proponents of modular programming (modularizing code) advocate that each subroutine should have minimal dependency on other pieces of code. For example, the use of global variables is generally deemed unwise by advocates for this perspective, because it adds tight coupling between the subroutine and these global variables. If such coupling is not necessary, their advice is to refactor subroutines to accept passed parameters instead. However, increasing the number of parameters passed to subroutines can affect code readability.Return codes[edit]Besides its main or normal effect, a subroutine may need to inform the calling program about exceptional conditions that may have occurred during its execution. In some languages and programming standards, this is often done through a return code, an integer value placed by the subroutine in some standard location, which encodes the normal and exceptional conditions.In the IBM System/360, where a return code was expected from the subroutine, the return value was often designed to be a multiple of 4\u2014so that it could be used as a direct branch table index into a branch table often located immediately after the call instruction to avoid extra conditional tests, further improving efficiency. In the System/360 assembly language, one would write, for example:\n           BAL  14,SUBRTN01    go to subroutine, storing return address in R14\n           B    TABLE(15)      use returned value in reg 15 to index the branch table, \n*                              branching to the appropriate branch instr.\nTABLE      B    OK             return code =00   GOOD                  }\n           B    BAD            return code =04   Invalid input         } Branch table\n           B    ERROR          return code =08   Unexpected condition  }\nOptimization of subroutine calls[edit]There is a significant runtime overhead in a calling a subroutine, including passing the arguments, branching to the subprogram, and branching back to the caller. The overhead often includes saving and restoring certain processor registers, allocating and reclaiming call frame storage, etc.. In some languages, each subroutine call also implies automatic testing of the subroutine's return code, or the handling of exceptions that it may raise. In object-oriented languages, a significant source of overhead is the intensively used dynamic dispatch for method calls.There are some seemingly obvious optimizations of procedure calls that cannot be applied if the procedures may have side effects. For example, in the expression (f(x)-1)/(f(x)+1), the function f must be called twice, because the two calls may return different results. Moreover, the value of x must be fetched again before the second call, since the first call may have changed it. Determining whether a subprogram may have a side effect is very difficult (indeed, undecidable).[citation needed] So, while those optimizations are safe in purely functional programming languages, compilers of typical imperative programming usually have to assume the worst.Inlining[edit]A method used to eliminate this overhead is inline expansion or inlining of the subprogram's body at each call site (versus branching to the subroutine and back). Not only does this avoid the call overhead, but it also allows the compiler to optimize the procedure's body more effectively by taking into account the context and arguments at that call. The inserted body can be optimized by the compiler. Inlining however, will usually increase the code size, unless the program contains only one call to the subroutine, or the subroutine body is less code than the call overhead.See also[edit]\nFunction (mathematics)\nMethod (computer programming)\nEvaluation strategy\nModular programming\nTransclusion\nOperator overloading\nFunctional programming\nCommand-query separation\nCoroutines, subprograms that call each other as if both were the main programs\nEvent handler, a subprogram that is called in response to an input event or interrupt\nReferences[edit]", "subtitles": ["Main concepts", "Language support", "Advantages", "Disadvantages", "History", "C and C++ examples", "Small Basic example", "Visual Basic 6 examples", "PL/I example", "Local variables, recursion and reentrancy", "Overloading", "Closures", "Conventions", "Optimization of subroutine calls", "See also", "References"], "title": "Subroutine"},
{"content": "A computer file is a computer resource for recording data discretely in a computer storage device. Just as words can be written to paper, so can information be written to a computer file.There are different types of computer files, designed for different purposes. A file may be designed to store a picture, a written message, a video, a computer program, or a wide variety of other kinds of data. Some types of files can store several types of information at once.By using computer programs, a person can open, read, change, and close a computer file. Computer files may be reopened, modified, and copied an arbitrary number of times.Typically, files are organised in a file system, which keeps track of where the files are located on disk and enables user access.Etymology[edit]The word file derives from the Latin filum (a thread).[1]File was used publicly in the context of computer storage as early as February 1950: In a Radio Corporation of America (RCA) advertisement in Popular Science Magazine[2] describing a new memory vacuum tube it had developed, RCA stated: the results of countless computations can be kept 'on file' and taken out again. Such a 'file' now exists in a 'memory' tube developed at RCA Laboratories. Electronically it retains figures fed into calculating machines, holds them in storage while it memorizes new ones - speeds intelligent solutions through mazes of mathematics.In 1952, file denoted, inter alia, information stored on punched cards.[3]In early use, the underlying hardware, rather than the contents stored on it, was denominated a file. For example, the IBM 350 disk drives were denominated disk files.[4] The introduction, circa 1961, by the Burroughs MCP and the MIT Compatible Time-Sharing System of the concept of a file system that managed several virtual files on one storage device is the origin of the contemporary denotation of the word. Although the contemporary register file demonstrates the early concept of files, its use has greatly decreased.File contents[edit]On most modern operating systems, files are organized into one-dimensional arrays of bytes. The format of a file is defined by its content since a file is solely a container for data, although, on some platforms the format is usually indicated by its filename extension, specifying the rules for how the bytes must be organized and interpreted meaningfully. For example, the bytes of a plain text file (.txt in Windows) are associated with either ASCII or UTF-8 characters, while the bytes of image, video, and audio files are interpreted otherwise. Most file types also allocate a few bytes for metadata, which allows a file to carry some basic information about itself.Some file systems can store arbitrary (not interpreted by the file system) file-specific data outside of the file format, but linked to the file, for example extended attributes or forks. On other file systems this can be done via sidecar files or software-specific databases. All those methods, however, are more susceptible to loss of metadata than are container and archive file formats.File size[edit]At any instant in time, a file might have a size, normally expressed as number of bytes, that indicates how much storage is associated with the file. In most modern operating systems the size can be any non-negative whole number of bytes up to a system limit. Many older operating systems kept track only of the number of blocks or tracks occupied by a file on a physical storage device. In such systems, software employed other methods to track the exact byte count (e.g., CP/M used a special control character, Ctrl-Z, to signal the end of text files).The general definition of a file does not require that its size have any real meaning, however, unless the data within the file happens to correspond to data within a pool of persistent storage. A special case is a zero byte file; these files can be newly created files that have not yet had any data written to them, or may serve as some kind of flag in the file system, or are accidents (the results of aborted disk operations). For example, the file to which the link /bin/ls points in a typical Unix-like system probably has a defined size that seldom changes. Compare this with /dev/null which is also a file, but its size may be obscure. (This is misleading because /dev/null is not really a file: in Unix-like systems, all resources, including devices, are accessed like files, but there is still a real distinction between files and devices\u2014at core, they behave differently\u2014and the obscurity of the size of /dev/null is one manifestation of this. As a character device, /dev/null has no size.)Organization of data in a file[edit]Information in a computer file can consist of smaller packets of information (often called records or lines) that are individually different but share some common traits. For example, a payroll file might contain information concerning all the employees in a company and their payroll details; each record in the payroll file concerns just one employee, and all the records have the common trait of being related to payroll\u2014this is very similar to placing all payroll information into a specific filing cabinet in an office that does not have a computer. A text file may contain lines of text, corresponding to printed lines on a piece of paper. Alternatively, a file may contain an arbitrary binary image (a BLOB) or it may contain an executable.The way information is grouped into a file is entirely up to how it is designed. This has led to a plethora of more or less standardized file structures for all imaginable purposes, from the simplest to the most complex. Most computer files are used by computer programs which create, modify or delete the files for their own use on an as-needed basis. The programmers who create the programs decide what files are needed, how they are to be used and (often) their names.In some cases, computer programs manipulate files that are made visible to the computer user. For example, in a word-processing program, the user manipulates document files that the user personally names. Although the content of the document file is arranged in a format that the word-processing program understands, the user is able to choose the name and location of the file and provide the bulk of the information (such as words and text) that will be stored in the file.Many applications pack all their data files into a single file called an archive file, using internal markers to discern the different types of information contained within. The benefits of the archive file are to lower the number of files for easier transfer, to reduce storage usage, or just to organize outdated files. The archive file must often be unpacked before next using.Operations[edit]The most basic operations that programs can perform on a file are:\nCreate a new file\nChange the access permissions and attributes of a file\nOpen a file, which makes the file contents available to the program\nRead data from a file\nWrite data to a file\nClose a file, terminating the association between it and the program\nFiles on a computer can be created, moved, modified, grown, shrunk, and deleted. In most cases, computer programs that are executed on the computer handle these operations, but the user of a computer can also manipulate files if necessary. For instance, Microsoft Word files are normally created and modified by the Microsoft Word program in response to user commands, but the user can also move, rename, or delete these files directly by using a file manager program such as Windows Explorer (on Windows computers) or by command lines (CLI).In Unix-like systems, user-space programs do not operate directly, at a low level, on a file. Only the kernel deals with files, and it handles all user-space interaction with files in a manner that is transparent to the user-space programs. The operating system provides a level of abstraction, which means that interaction with a file from user-space is simply through its filename (instead of its filehandle). For example, rm filename will not delete the file itself, but only a link to the file. There can be many links to a file, but when they are all removed, the kernel considers that file's memory space free to be reallocated. This free space is commonly considered a security risk (due to the existence of file recovery software). Any secure-deletion program uses kernel-space (system) functions to wipe the file's data.Identifying and organizing[edit]In modern computer systems, files are typically accessed using names (filenames). In some operating systems, the name is associated with the file itself. In others, the file is anonymous, and is pointed to by links that have names. In the latter case, a user can identify the name of the link with the file itself, but this is a false analogue, especially where there exists more than one link to the same file.Files (or links to files) can be located in directories. However, more generally, a directory can contain either a list of files or a list of links to files. Within this definition, it is of paramount importance that the term file includes directories. This permits the existence of directory hierarchies, i.e., directories containing sub-directories. A name that refers to a file within a directory must be typically unique. In other words, there must be no identical names within a directory. However, in some operating systems, a name may include a specification of type that means a directory can contain an identical name for more than one type of object such as a directory and a file.In environments in which a file is named, a file's name and the path to the file's directory must uniquely identify it among all other files in the computer system\u2014no two files can have the same name and path. Where a file is anonymous, named references to it will exist within a namespace. In most cases, any name within the namespace will refer to exactly zero or one file. However, any file may be represented within any namespace by zero, one or more names.Any string of characters may or may not be a well-formed name for a file or a link depending upon the context of application. Whether or not a name is well-formed depends on the type of computer system being used. Early computers permitted only a few letters or digits in the name of a file, but modern computers allow long names (some up to 255 characters) containing almost any combination of unicode letters or unicode digits, making it easier to understand the purpose of a file at a glance. Some computer systems allow file names to contain spaces; others do not. Case-sensitivity of file names is determined by the file system. Unix file systems are usually case sensitive and allow user-level applications to create files whose names differ only in the case of characters. Microsoft Windows supports multiple file systems, each with different policies[which?] regarding case-sensitivity. The common FAT file system can have multiple files whose names differ only in case if the user uses a disk editor to edit the file names in the directory entries. User applications, however, will usually not allow the user to create multiple files with the same name but differing in case.Most computers organize files into hierarchies using folders, directories, or catalogs. The concept is the same irrespective of the terminology used. Each folder can contain an arbitrary number of files, and it can also contain other folders. These other folders are referred to as subfolders. Subfolders can contain still more files and folders and so on, thus building a tree-like structure in which one master folder (or root folder \u2014 the name varies from one operating system to another) can contain any number of levels of other folders and files. Folders can be named just as files can (except for the root folder, which often does not have a name). The use of folders makes it easier to organize files in a logical way.When a computer allows the use of folders, each file and folder has not only a name of its own, but also a path, which identifies the folder or folders in which a file or folder resides. In the path, some sort of special character\u2014such as a slash\u2014is used to separate the file and folder names. For example, in the illustration shown in this article, the path /Payroll/Salaries/Managers uniquely identifies a file called Managers in a folder called Salaries, which in turn is contained in a folder called Payroll. The folder and file names are separated by slashes in this example; the topmost or root folder has no name, and so the path begins with a slash (if the root folder had a name, it would precede this first slash).Many (but not all) computer systems use extensions in file names to help identify what they contain, also known as the file type. On Windows computers, extensions consist of a dot (period) at the end of a file name, followed by a few letters to identify the type of file. An extension of .txt identifies a text file; a .doc extension identifies any type of document or documentation, commonly in the Microsoft Word file format; and so on. Even when extensions are used in a computer system, the degree to which the computer system recognizes and heeds them can vary; in some systems, they are required, while in other systems, they are completely ignored if they are presented.Protection[edit]Many modern computer systems provide methods for protecting files against accidental and deliberate damage. Computers that allow for multiple users implement file permissions to control who may or may not modify, delete, or create files and folders. For example, a given user may be granted only permission to read a file or folder, but not to modify or delete it; or a user may be given permission to read and modify files or folders, but not to execute them. Permissions may also be used to allow only certain users to see the contents of a file or folder. Permissions protect against unauthorized tampering or destruction of information in files, and keep private information confidential from unauthorized users.Another protection mechanism implemented in many computers is a read-only flag. When this flag is turned on for a file (which can be accomplished by a computer program or by a human user), the file can be examined, but it cannot be modified. This flag is useful for critical information that must not be modified or erased, such as special files that are used only by internal parts of the computer system. Some systems also include a hidden flag to make certain files invisible; this flag is used by the computer system to hide essential system files that users should not alter.Storage[edit]Any file that has any useful purpose, must have some physical manifestation. That is, a file (an abstract concept) in a real computer system must have a real physical analogue if it is to exist at all.In physical terms, most computer files are stored on some type of data storage device. For example, most operating systems store files on a hard disk. Hard disks have been the ubiquitous form of non-volatile storage since the early 1960s.[5] Where files contain only temporary information, they may be stored in RAM. Computer files can be also stored on other media in some cases, such as magnetic tapes, compact discs, Digital Versatile Discs, Zip drives, USB flash drives, etc. The use of solid state drives is also beginning to rival the hard disk drive.In Unix-like operating systems, many files have no associated physical storage device. Examples are /dev/null and most files under directories /dev, /proc and /sys. These are virtual files: they exist as objects within the operating system kernel.As seen by a running user program, files are usually represented either by a File control block or by a file handle. A File control block (FCB) is an area of memory which is manipulated to establish a filename etc. and then passed to the operating system as a parameter, it was used by older IBM operating systems and early PC operating systems including CP/M and early versions of MS-DOS. A file handle is generally either an opaque data type or an integer, it was introduced in around 1961 by the ALGOL-based Burroughs MCP running on the Burroughs B5000 but is now ubiquitous.Back up[edit]When computer files contain information that is extremely important, a back-up process is used to protect against disasters that might destroy the files. Backing up files simply means making copies of the files in a separate location so that they can be restored if something happens to the computer, or if they are deleted accidentally.There are many ways to back up files. Most computer systems provide utility programs to assist in the back-up process, which can become very time-consuming if there are many files to safeguard. Files are often copied to removable media such as writable CDs or cartridge tapes. Copying files to another hard disk in the same computer protects against failure of one disk, but if it is necessary to protect against failure or destruction of the entire computer, then copies of the files must be made on other media that can be taken away from the computer and stored in a safe, distant location.The grandfather-father-son backup method automatically makes three back-ups; the grandfather file is the oldest copy of the file and the son is the current copy.File systems and file managers[edit]The way a computer organizes, names, stores and manipulates files is globally referred to as its file system. Most computers have at least one file system. Some computers allow the use of several different file systems. For instance, on newer MS Windows computers, the older FAT-type file systems of MS-DOS and old versions of Windows are supported, in addition to the NTFS file system that is the normal file system for recent versions of Windows. Each system has its own advantages and disadvantages. Standard FAT allows only eight-character file names (plus a three-character extension) with no spaces, for example, whereas NTFS allows much longer names that can contain spaces. You can call a file Payroll records in NTFS, but in FAT you would be restricted to something like payroll.dat (unless you were using VFAT, a FAT extension allowing long file names).File manager programs are utility programs that allow users to manipulate files directly. They allow you to move, create, delete and rename files and folders, although they do not actually allow you to read the contents of a file or store information in it. Every computer system provides at least one file-manager program for its native file system. For example, File Explorer (formerly Windows Explorer) is commonly used in Microsoft Windows operating systems, and Nautilus is common under several distributions of Linux.See also[edit]\nBlock (data storage)\nComputer file management\nData hierarchy\nFile camouflage\nFile copying\nFile conversion\nFile deletion\nFile directory\nFile manager\nFile system\nFilename\nFlat file database\nObject composition\nSoft copy\nNotes[edit]External links[edit]\nData Formats Computer file at Curlie (based on DMOZ)\n", "subtitles": ["Etymology", "File contents", "Identifying and organizing", "Protection", "Storage", "Back up", "File systems and file managers", "See also", "Notes", "External links"], "title": "Computer file"},
{"content": "In computer programming, an assertion is a statement that a predicate (Boolean-valued function, i.e. a true\u2013false expression) is expected to always be true at that point in the code. If an assertion evaluates to false at run time, an assertion failure results, which typically causes the program to crash, or to throw an assertion exception.Details[edit]The following code contains two assertions, x > 0 and x > 1, and they are indeed true at the indicated points during execution:Programmers can use assertions to help specify programs and to reason about program correctness. For example, a precondition\u2014an assertion placed at the beginning of a section of code\u2014determines the set of states under which the programmer expects the code to execute. A postcondition\u2014placed at the end\u2014describes the expected state at the end of execution. For example: x > 0 { x++ } x > 1.The example above uses the notation for including assertions used by C. A. R. Hoare in his 1969 article.[1] That notation cannot be used in existing mainstream programming languages. However, programmers can include unchecked assertions using the comment feature of their programming language. For example, in C:The braces included in the comment help distinguish this use of a comment from other uses.Libraries may provide assertion features as well. For example, in C using glibc with C99 support:Several modern programming languages include checked assertions \u2013 statements that are checked at runtime or sometimes statically. If an assertion evaluates to false at runtime, an assertion failure results, which typically causes execution to abort. This draws attention to the location at which the logical inconsistency is detected and can be preferable to the behaviour that would otherwise result.The use of assertions helps the programmer design, develop, and reason about a program.Usage[edit]In languages such as Eiffel, assertions form part of the design process; other languages, such as C and Java, use them only to check assumptions at runtime. In both cases, they can be checked for validity at runtime but can usually also be suppressed.Assertions in design by contract[edit]Assertions can function as a form of documentation: they can describe the state the code expects to find before it runs (its preconditions), and the state the code expects to result in when it is finished running (postconditions); they can also specify invariants of a class. Eiffel integrates such assertions into the language and automatically extracts them to document the class. This forms an important part of the method of design by contract.This approach is also useful in languages that do not explicitly support it: the advantage of using assertion statements rather than assertions in comments is that the program can check the assertions every time it runs; if the assertion no longer holds, an error can be reported. This prevents the code from getting out of sync with the assertions.Assertions for run-time checking[edit]An assertion may be used to verify that an assumption made by the programmer during the implementation of the program remains valid when the program is executed. For example, consider the following Java code:In Java, % is the remainder operator (modulo), and in Java, if its first operand is negative, the result can also be negative (unlike the modulo used in mathematics). Here, the programmer has assumed that total is non-negative, so that the remainder of a division with 2 will always be 0 or 1. The assertion makes this assumption explicit: if countNumberOfUsers does return a negative value, the program may have a bug.A major advantage of this technique is that when an error does occur it is detected immediately and directly, rather than later through its often obscure side-effects. Since an assertion failure usually reports the code location, one can often pin-point the error without further debugging.Assertions are also sometimes placed at points the execution is not supposed to reach. For example, assertions could be placed at the default clause of the switch statement in languages such as C, C++, and Java. Any case which the programmer does not handle intentionally will raise an error and the program will abort rather than silently continuing in an erroneous state. In D such an assertion is added automatically when a switch statement doesn't contain a default clause.In Java, assertions have been a part of the language since version 1.4. Assertion failures result in raising an AssertionError when the program is run with the appropriate flags, without which the assert statements are ignored. In C, they are added on by the standard header assert.h defining assert (assertion) as a macro that signals an error in the case of failure, usually terminating the program. In C++, both assert.h and cassert headers provide the assert macro.The danger of assertions is that they may cause side effects either by changing memory data or by changing thread timing. Assertions should be implemented carefully so they cause no side effects on program code.Assertion constructs in a language allow for easy test-driven development (TDD) without the use of a third-party library.Assertions during the development cycle[edit]During the development cycle, the programmer will typically run the program with assertions enabled. When an assertion failure occurs, the programmer is immediately notified of the problem. Many assertion implementations will also halt the program's execution: this is useful, since if the program continued to run after an assertion violation occurred, it might corrupt its state and make the cause of the problem more difficult to locate. Using the information provided by the assertion failure (such as the location of the failure and perhaps a stack trace, or even the full program state if the environment supports core dumps or if the program is running in a debugger), the programmer can usually fix the problem. Thus assertions provide a very powerful tool in debugging.Assertions in production environment[edit]When a program is deployed to production, assertions are typically turned off, to avoid any overhead or side effects they may have. In some cases assertions are completely absent from deployed code, such as in C/C++ assertions via macros. In other cases, such as Java, assertions are present in the deployed code, and can be turned on in the field for debugging.[2]Assertions may also be used to promise the compiler that a given edge condition is not actually reachable, thereby permitting certain optimizations that would not otherwise be possible. In this case, disabling the assertions could actually reduce performance.Static assertions[edit]Assertions that are checked at compile time are called static assertions.Static assertions are particularly useful in compile time template metaprogramming, but can also be used in low-level languages like C by introducing illegal code if (and only if) the assertion fails. C11 and C++11 support static assertions directly through static_assert. In earlier C versions, a static assertion can be implemented, for example, like this:If the (BOOLEAN CONDITION) part evaluates to false then the above code will not compile because the compiler will not allow two case labels with the same constant. The boolean expression must be a compile-time constant value, for example (sizeof(int)==4) would be a valid expression in that context. This construct does not work at file scope (i.e. not inside a function), and so it must be wrapped inside a function.Another popular[3] way of implementing assertions in C is:If the (BOOLEAN CONDITION) part evaluates to false then the above code will not compile because arrays may not have a negative length. If in fact the compiler allows a negative length then the initialization byte (the '!' part) should cause even such over-lenient compilers to complain. The boolean expression must be a compile-time constant value, for example (sizeof(int)==4) would be a valid expression in that context.Both of these methods require a method of constructing unique names. Modern compilers support a __COUNTER__ preprocessor define that facilitates the construction of unique names, by returning monotonically increasing numbers for each compilation unit.[4]D provides static assertions through the use of static assert,.[5]Disabling assertions[edit]Most languages allow assertions to be enabled or disabled globally, and sometimes independently. Assertions are often enabled during development and disabled during final testing and on release to the customer. Not checking assertions avoids the cost of evaluating the assertions while (assuming the assertions are free of side effects) still producing the same result under normal conditions. Under abnormal conditions, disabling assertion checking can mean that a program that would have aborted will continue to run. This is sometimes preferable.Some languages, including C and C++, completely remove assertions at compile time using the preprocessor. Java requires an option to be passed to the run-time engine in order to enable assertions. Absent the option, assertions are bypassed, but they always remain in the code unless optimised away by a JIT compiler at run-time or excluded by an if(false) condition at compile time, thus they need not have a run-time space or time cost in Java either.Programmers can build checks into their code that are always active by bypassing or manipulating the language's normal assertion-checking mechanisms.Comparison with error handling[edit]Assertions are distinct from routine error-handling. Assertions document logically impossible situations and discover programming errors: if the impossible occurs, then something fundamental is clearly wrong with the program. This is distinct from error handling: most error conditions are possible, although some may be extremely unlikely to occur in practice. Using assertions as a general-purpose error handling mechanism is unwise: assertions do not allow for recovery from errors; an assertion failure will normally halt the program's execution abruptly; and assertions are often disabled in production code. Assertions also do not display a user-friendly error message.Consider the following example of using an assertion to handle an error:Here, the programmer is aware that malloc will return a NULL pointer if memory is not allocated. This is possible: the operating system does not guarantee that every call to malloc will succeed. If an out of memory error occurs the program will immediately abort. Without the assertion, the program would continue running until ptr was dereferenced, and possibly longer, depending on the specific hardware being used. So long as assertions are not disabled, an immediate exit is assured. But if a graceful failure is desired, the program has to handle the failure. For example, a server may have multiple clients, or may hold resources that will not be released cleanly, or it may have uncommitted changes to write to a datastore. In such cases it is better to fail a single transaction than to abort abruptly.Another error is to rely on side effects of expressions used as arguments of an assertion. One should always keep in mind that assertions might not be executed at all, since their sole purpose is to verify that a condition which should always be true does in fact hold true. Consequently, if the program is considered to be error-free and released, assertions may be disabled and will no longer be evaluated.Consider another version of the previous example:This might look like a smart way to assign the return value of malloc to ptr and check if it is NULL in one step, but the malloc call and the assignment to ptr is a side effect of evaluating the expression that forms the assert condition. When the NDEBUG parameter is passed to the compiler, as when the program is considered to be error-free and released, the assert() statement is removed, so malloc() isn't called, rendering ptr uninitialised. This could potentially result in a segmentation fault or similar null pointer error much further down the line in program execution, causing bugs that may be sporadic and/or difficult to track down. Programmers sometimes use a similar VERIFY(X) define to alleviate this problem.Modern compilers may issue a warning when encountering the above code.[6]History[edit]In 1947 reports by von Neumann and Goldstine[7] on their design for the IAS machine, they described algorithms using an early version of flow charts, in which they included assertions: It may be true, that whenever C actually reaches a certain point in the flow diagram, one or more bound variables will necessarily possess certain specified values, or possess certain properties, or satisfy certain properties with each other. Furthermore, we may, at such a point, indicate the validity of these limitations. For this reason we will denote each area in which the validity of such limitations is being asserted, by a special box, which we call an assertion box.The assertional method for proving correctness of programs was advocated by Alan Turing. In a talk Checking a Large Routine at Cambridge, June 24, 1949 Turing suggested: How can one check a large routine in the sense of making sure that it's right? In order that the man who checks may not have too difficult a task, the programmer should make a number of definite assertions which can be checked individually, and from which the correctness of the whole program easily follows.[8]See also[edit]\nAssertion definition language\nDesign by contract\nException handling\nHoare logic\nStatic code analysis\nJava Modeling Language\nInvariant (computer science)\nReferences[edit]External links[edit]\nProgramming With Assertions in Java by Qusay H. Mahmoud, (Oracle Corp.), 2005\nA historical perspective on runtime assertion checking in software development by Lori A. Clarke, David S. Rosenblum in: ACM SIGSOFT Software Engineering Notes 31(3):25-37, 2006\nThe benefits of programming with assertions by Philip Guo (Stanford University), 2008\nAssertions: a personal perspective by C.A.R. Hoare in: Annals of the History of Computing, IEEE, Volume: 25, Issue: 2 (2003), Page(s): 14 - 25\nMy Compiler Does Not Understand Me by Poul-Henning Kamp in: ACM Queue 10(5), May 2012\n", "subtitles": ["Details", "Usage", "Disabling assertions", "Comparison with error handling", "History", "See also", "References", "External links"], "title": "Assertion (software development)"},
{"content": "In computer science, conditional statements, conditional expressions and conditional constructs are features of a programming language, which perform different computations or actions depending on whether a programmer-specified boolean condition evaluates to true or false. Apart from the case of branch predication, this is always achieved by selectively altering the control flow based on some condition.In imperative programming languages, the term conditional statement is usually used, whereas in functional programming, the terms conditional expression or conditional construct are preferred, because these terms all have distinct meanings.A conditional is sometimes colloquially referred to as an if-check, especially when perceived as a simple one and when its specific form is irrelevant or unknown.Although dynamic dispatch is not usually classified as a conditional construct, it is another way to select between alternatives at runtime.If\u2013then(\u2013else)[edit]The if\u2013then construct (sometimes called if\u2013then\u2013else) is common across many programming languages. Although the syntax varies from language to language, the basic structure (in pseudocode form) looks like this:In the example code above, the part represented by (boolean condition) constitutes a conditional expression, having intrinsic value (e.g., it may be substituted by either of the values True or False) but having no intrinsic meaning. In contrast, the combination of this expression, the If and Then surrounding it, and the consequent that follows afterward constitute a conditional statement, having intrinsic meaning (e.g., expressing a coherent logical rule) but no intrinsic value.When an interpreter finds an If, it expects a boolean condition \u2013 for example, x > 0, which means the variable x contains a number that is greater than zero \u2013 and evaluates that condition. If the condition is true, the statements following the then are executed. Otherwise, the execution continues in the following branch \u2013 either in the else block (which is usually optional), or if there is no else branch, then after the end If.After either branch has been executed, control returns to the point after the end If.In early programming languages, especially some dialects of BASIC in the 1980s home computers, an if\u2013then statement could only contain GOTO statements. This led to a hard-to-read style of programming known as spaghetti programming, with programs in this style called spaghetti code. As a result, structured programming, which allows (virtually) arbitrary statements to be put in statement blocks inside an if statement, gained in popularity, until it became the norm even in most BASIC programming circles. Such mechanisms and principles were based on the older but more advanced ALGOL family of languages, and ALGOL-like languages such as Pascal and Modula-2 influenced modern BASIC variants for many years. While it is possible while using only GOTO statements in if\u2013then statements to write programs that are not spaghetti code and are just as well structured and readable as programs written in a structured programming language, structured programming makes this easier and enforces it. Structured if\u2013then\u2013else statements like the example above are one of the key elements of structured programming, and they are present in most popular high-level programming languages such as C, Java, JavaScript and Visual Basic .A subtlety is that the optional else clause found in many languages means that the context-free grammar is ambiguous, since nested conditionals can be parsed in multiple ways. Specifically,\nif a then if b then s else s2\ncan be parsed as\nif a then (if b then s) else s2\nor\nif a then (if b then s else s2)\ndepending on whether the else is associated with the first if or second if. This is known as the dangling else problem, and is resolved in various ways, depending on the language.Else if[edit]By using else if, it is possible to combine several conditions. Only the statements following the first condition that is found to be true will be executed. All other statements will be skipped. The statements ofelseif, in Ada, is simply syntactic sugar for else followed by if. In Ada, the difference is that only one end if is needed, if one uses elseif instead of else followed by if. This is similar in Perl, which provides the keyword elsif to avoid the large number of braces that would be required by multiple if and else statements and also in Python, which uses the special keyword elif because structure is denoted by indentation rather than braces, so a repeated use of else and if would require increased indentation after every condition. Similarly, the earlier UNIX shells (later gathered up to the POSIX shell syntax[1]) use elif too, but giving the choice of delimiting with spaces, line breaks, or both.However, in many languages more directly descended from Algol, such as Algol68, Simula, Pascal, BCPL and C, this special syntax for the else if construct is not present, nor is it present in the many syntactical derivatives of C, such as Java, ECMA-script, PHP, and so on. This works because in these languages, any single statement (in this case if cond...) can follow a conditional without being enclosed in a block.This design choice has a slight cost in that code else if branch is, effectively, adding an extra nesting level, complicating the job for some compilers (or its implementors), which has to analyse and implement arbitrarily long else if chains recursively.If all terms in the sequence of conditionals are testing the value of a single expression (e.g., if x=0 ... else if x=1 ... else if x=2...), then an alternative is the switch statement, also called case-statement or select-statement. Conversely, in languages that do not have a switch statement, these can be produced by a sequence of else if statements.If\u2013then\u2013else expressions[edit]Many languages support if expressions, which are similar to if statements, but return a value as a result. Thus, they are true expressions (which evaluate to a value), not statements (which changes the program state or perform some kind of action).Algol family[edit]ALGOL 60 and some other members of the ALGOL family allow if\u2013then\u2013else as an expression:\n  myvariable := if x > 10 then 1 else 2\nLisp dialects[edit]In dialects of Lisp \u2013 Scheme, Racket and Common Lisp \u2013 the first of which was inspired to a great extent by ALGOL:Haskell[edit]In Haskell 98, there is only an if expression, no if statement, and the else part is compulsory, as every expression must have some value.[2] Logic that would be expressed with conditionals in other languages is usually expressed with pattern matching in recursive functions.Because Haskell is lazy, it is possible to write control structures, such as if, as ordinary expressions; the lazy evaluation means that an if function can evaluate only the condition and proper branch (where a strict language would evaluate all three). It can be written like this:[3]C-like languages[edit]C and C-like languages have a special ternary operator (?:) for conditional expressions with a function that may be described by a template like this:\ncondition ? evaluated-when-true : evaluated-when-false\nThis means that it can be inlined into expressions, unlike if-statements, in C-like languages:which can be compared to the Algol-family if\u2013then\u2013else expressions (and similar in Ruby and Scala, among others).To accomplish the same using an if-statement, this would take more than one line of code (under typical layout conventions):Some argue that the explicit if/then statement is easier to read and that it may compile to more efficient code than the ternary operator,[4] while others argue that concise expressions are easier to read than statements spread over several lines.In Small Basic[edit]First, when the user runs the program, a cursor appears waiting for the reader to type a number. If that number is greater than 10, the text My variable is named 'foo'. is displayed on the screen. If the number is smaller than 10, then the message My variable is named 'bar'. is printed on the screen.In Visual Basic[edit]In Visual Basic and some other languages, a function called IIf is provided, which can be used as a conditional expression. However, it does not behave like a true conditional expression, because both the true and false branches are always evaluated; it is just that the result of one of them is thrown away, while the result of the other is returned by the IIf function.Arithmetic if[edit]Up to Fortran 77, the language Fortran has an arithmetic if statement which is halfway between a computed IF and a case statement, based on the trichotomy x < 0, x = 0, x > 0. This was the earliest conditional statement in Fortran:[5]Where e is any numeric expression (not necessarily an integer); this is equivalent toBecause this arithmetic IF is equivalent to multiple GOTO statements that could jump to anywhere, it is considered to be an unstructured control statement, and should not be used if more structured statements can be used. In practice it has been observed that most arithmetic IF statements referenced the following statement with one or two of the labels.This was the only conditional control statement in the original implementation of Fortran on the IBM 704 computer. On that computer the test-and-branch op-code had three addresses for those three states. Other computers would have flag registers such as positive, zero, negative, even, overflow, carry, associated with the last arithmetic operations and would use instructions such as 'Branch if accumulator negative' then 'Branch if accumulator zero' or similar. Note that the expression is evaluated once only, and in cases such as integer arithmetic where overflow may occur, the overflow or carry flags would be considered also.Object-oriented implementation in Smalltalk[edit]In contrast to other languages, in Smalltalk the conditional statement is not a language construct but defined in the class Boolean as an abstract method that takes two parameters, both closures. Boolean has two subclasses, True and False, which both define the method, True executing the first closure only, False executing the second closure only.[6]JavaScript[edit]Two examples in JavaScript:Lambda Calculus[edit]In Lambda Calculus, the concept of an if-then-else conditional can be expressed using the expressions:\ntrue = \u03bbx. \u03bby. x\nfalse = \u03bbx. \u03bby. y\nifThenElse = (\u03bbc. \u03bbx. \u03bby. (c x y))\n\ntrue takes up to two arguments and once both are provided(see currying), it returns the first argument given.\nfalse takes up to two arguments and once both are provided(see currying), it returns the second argument given.\nifThenElse takes up to three arguments and once all are provided, it passes both second and third argument to the first argument(which is a function that given two arguments, and produces a result). We expect ifThenElse to only take true or false as an argument, both of which project the given two arguments to their preferred single argument, which is then returned.\nnote: if ifThenElse is passed two functions as the left and right conditionals; it is necessary to also pass an empty tuple () to the result of ifThenElse in order to actually call the chosen function, otherwise ifThenElse will just return the function object without getting called.In a system where numbers can be used without definition(like Lisp, Traditional paper math, so on), the above can be expressed as a single closure below:\n((\u03bbtrue. \u03bbfalse. \u03bbifThenElse.\n    (ifThenElse true 2 3)\n)(\u03bbx. \u03bby. x)(\u03bbx. \u03bby. y)(\u03bbc. \u03bbl. \u03bbr. c l r))\nHere, true, false, and ifThenElse are bound to their respective definitions which are passed to their scope at the end of their block.A working JavaScript analogy(using only functions of single variable for rigor) to this is:\nvar computationResult = ((_true => _false => _ifThenElse => \n    _ifThenElse(_true)(2)(3) \n)(x => y => x)(x => y => y)(c => x => y => c(x)(y)));\nThe code above with multivariable functions looks like this:\nvar computationResult = ((_true, _false, _ifThenElse) =>\n    _ifThenElse(_true, 2, 3)\n)((x, y) => x, (x, y) => y, (c, x, y) => c(x, y));\nanother version of the earlier example without a system where numbers are assumed is below.First example shows the first branch being taken, while second example shows the second branch being taken.\n((\u03bbtrue. \u03bbfalse. \u03bbifThenElse.\n    (ifThenElse true (\u03bbFirstBranch. FirstBranch) (\u03bbSecondBranch. SecondBranch))\n)(\u03bbx. \u03bby. x)(\u03bbx. \u03bby. y)(\u03bbc. \u03bbl. \u03bbr. c l r))\n\n((\u03bbtrue. \u03bbfalse. \u03bbifThenElse.\n    (ifThenElse false (\u03bbFirstBranch. FirstBranch) (\u03bbSecondBranch. SecondBranch))\n)(\u03bbx. \u03bby. x)(\u03bbx. \u03bby. y)(\u03bbc. \u03bbl. \u03bbr. c l r))\nSmalltalk uses a similar idea for its true and false representations, with True and False being singleton objects that respond to messages ifTrue/ifFalse differently.Haskell used to use this exact model for its Boolean type, but at the time of writing, most Haskell programs use syntactic sugar if a then b else c construct which unlike ifThenElse does not compose unlesseither wrapped in another function or re-implemented as shown in The Haskell section of this page.Case and switch statements[edit]Switch statements (in some languages, case statements or multiway branches) compare a given value with specified constants and take action according to the first constant to match. There is usually a provision for a default action ('else','otherwise') to be taken if no match succeeds. Switch statements can allow compiler optimizations, such as lookup tables. In dynamic languages, the cases may not be limited to constant expressions, and might extend to pattern matching, as in the shell script example on the right, where the '*)' implements the default case as a regular expression matching any string.Pattern matching[edit]Pattern matching may be seen as a more sophisticated alternative to both if\u2013then\u2013else, and case statements. It is available in many programming languages with functional programming features, such as Wolfram Language, ML and many others. Here is a simple example written in the OCaml language:The power of pattern matching is the ability to concisely match not only actions but also values to patterns of data. Here is an example written in Haskell which illustrates both of these features:This code defines a function map, which applies the first argument (a function) to each of the elements of the second argument (a list), and returns the resulting list. The two lines are the two definitions of the function for the two kinds of arguments possible in this case \u2013 one where the list is empty (just return an empty list) and the other case where the list is not empty.Pattern matching is not strictly speaking always a choice construct, because it is possible in Haskell to write only one alternative, which is guaranteed to always be matched \u2013 in this situation, it is not being used as a choice construct, but simply as a way to bind names to values. However, it is frequently used as a choice construct in the languages in which it is available.Hash-based conditionals[edit]In programming languages that have associative arrays or comparable data structures, such as Python, Perl, PHP or Objective-C, it is idiomatic to use them to implement conditional assignment.[7]In languages that have anonymous functions or that allow a programmer to assign a named function to a variable reference, conditional flow can be implemented by using a hash as a dispatch table.Predication[edit]An alternative to conditional branch instructions is predication. Predication is an architectural feature that enables instructions to be conditionally executed instead of modifying the control flow.Choice system cross reference[edit]This table refers to the most recent language specification of each language. For languages that do not have a specification, the latest officially released implementation is referred to.\n^ This refers to pattern matching as a distinct conditional construct in the programming language \u2013 as opposed to mere string pattern matching support, such as regular expression support.\n1 2 3 4 5 The often-encountered else if in the C family of languages, and in COBOL and Haskell, is not a language feature but a set of nested and independent if then else statements combined with a particular source code layout. However, this also means that a distinct else\u2013if construct is not really needed in these languages.\n1 2 In Haskell and F#, a separate constant choice construct is unneeded, because the same task can be done with pattern matching.\n^ In a Ruby case construct, regular expression matching is among the conditional flow-control alternatives available. For an example, see this Stack Overflow question.\n1 2 SQL has two similar constructs that fulfill both roles, both introduced in SQL-92. A searched CASE expression CASE WHEN cond1 THEN expr1 WHEN cond2 THEN expr2 [...] ELSE exprDflt END works like if ... else if ... else, whereas a simple CASE expression: CASE expr WHEN val1 THEN expr1 [...] ELSE exprDflt END works like a switch statement. For details and examples see Case (SQL).\n^ Arithmetic if is obsolescent in Fortran 90.\nSee also[edit]\nBranch (computer science)\nConditional compilation\nDynamic dispatch for another way to make execution choices\nMcCarthy Formalism for history and historical references\nNamed condition\nTest (Unix)\nYoda conditions\nConditional move\nReferences[edit]External links[edit]\nIF NOT (ActionScript 3.0) video\n", "subtitles": ["If\u2013then(\u2013else)", "Lambda Calculus", "Case and switch statements", "Pattern matching", "Hash-based conditionals", "Predication", "Choice system cross reference", "See also", "References", "External links"], "title": "Conditional (computer programming)"},
{"content": "LOOP is a programming language designed by Uwe Scho\u0308ning, along with GOTO and WHILE. The only operations supported in the language are assignment, addition and looping.The key property of the LOOP language is that the functions it can compute are exactly the primitive recursive functions.[1]Features[edit]Each primitive recursive function is LOOP-computable and vice versa.[2]In contrast to GOTO programs and WHILE programs, LOOP programs always terminate.[3] Therefore, the set of functions computable by LOOP-programs is a proper subset of computable functions (and thus a subset of the computable by WHILE and GOTO program functions).[4]An example of a total computable function that is not LOOP computable is the Ackermann function.[5]Formal definition[edit]Syntax[edit]LOOP-programs consist of the symbols LOOP, DO, END, :=, +, - and ; as well as any number of variables and constants. LOOP-programs have the following syntax in modified Backus\u2013Naur form:\n\n  \n    \n      \n        \n          \n            \n              \n                P\n              \n              \n                :=\n              \n              \n                \n                  x\n                  \n                    i\n                  \n                \n                :=\n                \n                  x\n                  \n                    j\n                  \n                \n                +\n                c\n              \n            \n            \n              \n              \n                \n                  |\n                \n              \n              \n                \n                  x\n                  \n                    i\n                  \n                \n                :=\n                \n                  x\n                  \n                    j\n                  \n                \n                \u2212\n                c\n              \n            \n            \n              \n              \n                \n                  |\n                \n              \n              \n                P\n                ;\n                P\n              \n            \n            \n              \n              \n                \n                  |\n                \n              \n              \n                \n                  \n                    L\n                    O\n                    O\n                    P\n                  \n                \n                \n                \n                  x\n                  \n                    i\n                  \n                \n                \n                \n                  \n                    D\n                    O\n                  \n                \n                \n                P\n                \n                \n                  \n                    E\n                    N\n                    D\n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{array}{lrl}P&:=&x_{i}:=x_{j}+c\\\\&|&x_{i}:=x_{j}-c\\\\&|&P;P\\\\&|&{\\mathtt {LOOP}}\\,x_{i}\\,{\\mathtt {DO}}\\,P\\,{\\mathtt {END}}\\end{array}}}\n  \n\nHere, \n  \n    \n      \n        V\n        a\n        r\n        :=\n        {\n        \n          x\n          \n            0\n          \n        \n        ,\n        \n          x\n          \n            1\n          \n        \n        ,\n        .\n        .\n        .\n        }\n      \n    \n    {\\displaystyle Var:=\\{x_{0},x_{1},...\\}}\n  \n are variable names and \n  \n    \n      \n        c\n        \u2208\n        \n          N\n        \n      \n    \n    {\\displaystyle c\\in \\mathbb {N} }\n  \n are constants.Semantics[edit]If P is a LOOP program, P is equivalent to a function \n  \n    \n      \n        f\n        :\n        \n          \n            N\n          \n          \n            k\n          \n        \n        \u2192\n        \n          N\n        \n      \n    \n    {\\displaystyle f:\\mathbb {N} ^{k}\\rightarrow \\mathbb {N} }\n  \n. The variables \n  \n    \n      \n        \n          x\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle x_{1}}\n  \n through \n  \n    \n      \n        \n          x\n          \n            k\n          \n        \n      \n    \n    {\\displaystyle x_{k}}\n  \n in a LOOP program correspond to the arguments of the function \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n  \n, and are initialized before program execution with the appropriate values. All other variables are given the initial value zero. The variable \n  \n    \n      \n        \n          x\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle x_{0}}\n  \n corresponds to the value that \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n  \n takes when given the argument values from \n  \n    \n      \n        \n          x\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle x_{1}}\n  \n through \n  \n    \n      \n        \n          x\n          \n            k\n          \n        \n      \n    \n    {\\displaystyle x_{k}}\n  \n.A statement of the form\nx0 := x1 + c\nmeans the value of the constant \n  \n    \n      \n        c\n      \n    \n    {\\displaystyle c}\n  \n is added to the value of the variable \n  \n    \n      \n        \n          x\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle x_{1}}\n  \n, and the result is set as the value of the variable \n  \n    \n      \n        \n          x\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle x_{0}}\n  \n. \n  \n    \n      \n        c\n      \n    \n    {\\displaystyle c}\n  \n can have the value zero, which allows the value of one variable to be assigned to another variable:\nx0 := x1 + 0\nA statement of the form\nx0 := x1 - c\nmeans the value of the constant \n  \n    \n      \n        c\n      \n    \n    {\\displaystyle c}\n  \nis subtracted from the value of the variable \n  \n    \n      \n        \n          x\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle x_{1}}\n  \n, and the result is set as the value of the variable \n  \n    \n      \n        \n          x\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle x_{0}}\n  \n. Negative numbers aren't allowed, and are replaced by zeros.Variables are allowed to be simultaneously on the left and right side of an assignment. A statement of the form:\nx1: = x1 + c\nfor example, adds the value of the constant \n  \n    \n      \n        c\n      \n    \n    {\\displaystyle c}\n  \n to the variable \n  \n    \n      \n        \n          x\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle x_{1}}\n  \n.A statement of the form\nP1; P2\nrepresents the sequential execution of sub-programs \n  \n    \n      \n        \n          P\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle P_{1}}\n  \n and \n  \n    \n      \n        \n          P\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle P_{2}}\n  \n, in that order.A statement of the form\nLOOP x DO P END\nmeans the repeated execution of the partial program \n  \n    \n      \n        P\n      \n    \n    {\\displaystyle P}\n  \n a total of \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n times, where the value that \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n has at the beginning of the execution of the statement is used. Even if \n  \n    \n      \n        P\n      \n    \n    {\\displaystyle P}\n  \n changes the value of \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n, it won't affect how many times \n  \n    \n      \n        P\n      \n    \n    {\\displaystyle P}\n  \n is executed in the loop. If \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n has the value zero, then \n  \n    \n      \n        P\n      \n    \n    {\\displaystyle P}\n  \n is not executed inside the LOOP statement. This allows for branches in LOOP programs, where the conditional execution of a partial program depends on whether a variable has value zero or one.Example Programs[edit]Addition[edit]In the following program, the variable \n  \n    \n      \n        \n          x\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle x_{0}}\n  \n is set to the sum of the variables \n  \n    \n      \n        \n          x\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle x_{1}}\n  \n and \n  \n    \n      \n        \n          x\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle x_{2}}\n  \n.\nx0 := x1 + 0;\nLOOP x2 DO\n   x0 := x0 + 1\nEND\n\n\n\n  \n    \n      \n        \n          x\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle x_{0}}\n  \n is first assigned the value of \n  \n    \n      \n        \n          x\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle x_{1}}\n  \n. Then, \n  \n    \n      \n        \n          x\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle x_{0}}\n  \n is incremented a total of \n  \n    \n      \n        \n          x\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle x_{2}}\n  \n times by the LOOP statement. This program can be used as a subroutine in other LOOP programs. The LOOP syntax can be extended with the following statement, equivalent to calling the above as a subroutine:\nx0 := x1 + x2\nMultiplication[edit]The following LOOP program sets the value of the variable \n  \n    \n      \n        \n          x\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle x_{0}}\n  \n to the product of the variables \n  \n    \n      \n        \n          x\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle x_{1}}\n  \n and \n  \n    \n      \n        \n          x\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle x_{2}}\n  \n.\nLOOP x1 DO\n  x0 := x0 + x2\nEND\nThis multiplication program uses the syntax introduced by the addition subroutine from the previous example. The multiplication is performed here by adding the value of \n  \n    \n      \n        \n          x\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle x_{2}}\n  \n a total of \n  \n    \n      \n        \n          x\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle x_{1}}\n  \n times, storing results in \n  \n    \n      \n        \n          x\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle x_{0}}\n  \n.See also[edit]\n\u03bc-recursive function\nprimitive recursive function\nNotes and references[edit]External links[edit]\nLoop, Goto & While\n", "subtitles": ["Features", "Formal definition", "Example Programs", "See also", "Notes and references", "External links"], "title": "LOOP (programming language)"},
{"content": "The VEX prefix (from vector extensions) and VEX coding scheme are comprising an extension to the x86 and x86-64 instruction set architecture for microprocessors from Intel, AMD and others.Features[edit]The VEX coding scheme allows the definition of new instructions and the extension or modification of previously existing instruction codes. This serves the following purposes:\nThe opcode map is extended to make space for future instructions.\nIt allows instruction codes to have up to five operands, where the original scheme allows only two operands (in rare cases three operands).\nIt allows the size of SIMD vector registers to be extended from the 128-bits XMM registers to 256-bits registers named YMM. There is room for further extensions of the register size.\nIt allows existing two-operand instructions to be modified into non-destructive three-operand forms where the destination register is different from both source registers. For example, c = a + b instead of a = a + b (where register a is changed by the instruction).\nThe VEX prefix replaces the most commonly used instruction prefix bytes and escape codes. In many cases, the number of prefix bytes and escape bytes that are replaced is the same as the number of bytes in the VEX prefix, so that the total length of the VEX-encoded instruction is the same as the length of the legacy instruction code. In other cases, the VEX-encoded version is longer or shorter than the legacy code. In 32-bit mode VEX encoded instructions can only access the first 8 YMM/XMM registers; the encodings for the other registers would be interpreted as the legacy LDS and LES instructions that are not supported in 64-bit mode.The two-byte VEX prefix contains the following components:\nThe bit, R\u0305, similar to the REX.R prefix bit used in the x86-64 instruction set extension.\nFour bits named v\u0305, specifying a second source register operand.\nA bit named L specifying 256-bit vector length.\nTwo bits named p to replace operand size prefixes and operand type prefixes (66, F2, F3).\nThe three-byte VEX prefix additionally contains:\nThe three bits, X\u0305; B\u0305; and W, also similar to the corresponding bits in the REX prefix.\nFive bits named m. Two of the m bits are used for replacing existing escape codes and for specifying the length of the instruction. The remaining three m bits are reserved for future use, such as specifying vector lengths >256 bits, specifying different instruction lengths, or extending the opcode space, however as of 2013, Intel decided to introduce a new encoding scheme, the EVEX prefix, rather than expand the remaining m bits.\nTechnical description[edit]The VEX coding scheme uses a code prefix consisting of two or three bytes, which is added to existing or new instruction codes.[1]In x86 architecture, instructions with a memory operand may use the ModR/M byte which specifies the addressing mode. This byte has three bit fields:\nmod, bits [7:6] - combined with the r/m field, encodes either 8 registers or 24 addressing modes. Also encodes opcode information for some instructions\nreg/opcode, bits [5:3] - specifies either a register or three more bits of opcode information, as specified in the primary opcode byte\nr/m, bits [2:0] - can specify a register as an operand, or combine with the mod field to encode an addressing mode.\nThe base-plus-index and scale-plus-index forms of 32-bit addressing (encoded with r/m=100 and mod <>11) require another addressing byte, the SIB byte. It has the following fields:\nscale factor, encoded with bits [7:6]\nindex register, bits [5:3]\nbase register, bits [2:0].\nTo use 64-bit addressing and additional registers present in the x86-64 architecture, the REX prefix has been introduced which provides additional space for encoding addressing modes. Bit-field W expands the operand size to 64 bits, R expands reg, B expands r/m or reg (depending on the opcode format used), and X and B expand index and base in the SIB byte. However REX prefix is encoded quite inefficiently, wasting half of its 8 bits.The VEX prefix provides a compact representation of the REX prefix, as well as various other prefixes, to expand the addressing mode, register enumeration and operand size and width:\nR\u0305, X\u0305 and B\u0305 bits are inversion of the REX prefix's R, X and B bits; these provide a fourth (high) bit for register index fields (ModRM reg, SIB index, and ModRM r/m; SIB base; or opcode reg fields, respectively) allowing access to 16 instead of 8 registers. The W bit is equivalent to the REX prefix's W bit, and specifies a 64-bit operand; for non-integer instructions, it is a general opcode extension bit.\nv\u0305 is the inversion of an additional source register index.\nm replaces leading opcode prefix bytes. The values 1, 2 and 3 are equivalent to opcode prefixes 0F, 0F 38 and 0F 3A; all other values are reserved. The 2-byte VEX prefix always corresponds to the 0F prefix.\nL indicates the vector length; 0 for 128-bit SSE (XMM) registers, and 1 for 256-bit AVX (YMM) registers.\np encodes additional prefix bytes. The values 0, 1, 2, and 3 correspond to implied prefixes of none, 66, F3, and F2. These encode the operand type for SSE instructions: packed single, packed double, scalar single and scalar double, respectively.\nThe VEX prefix's initial-byte values, C4h and C5h, are the same as the opcodes of the LDS and LES instructions. These instructions are not supported in 64-bit mode. To resolve the ambiguity while in 32-bit mode, VEX's specification exploits the fact that a legal LDS or LES's ModRM byte can not be of the form 11xxxxxx (which would specify a register operand). Various bit-fields in the VEX prefix's second byte are inverted to ensure that the byte is always of this form in 32-bit mode.Instructions that need more than three operands have an extra suffix byte specifying one or two additional register operands. Instructions coded with the VEX prefix can have up to five operands. At most one of the operands can be a memory operand; and at most one of the operands can be an immediate constant of 4 or 8 bits. The remaining operands are registers.The AVX instruction set is the first instruction set extension to use the VEX coding scheme. The AVX instructions have up to four operands. The AVX instruction set allows the VEX prefix to be applied only to instructions using the SIMD XMM registers. However, the VEX coding scheme has space for applying the VEX prefix to other instructions as well in future instruction sets.Legacy SIMD instructions with a VEX prefix added are equivalent to the same instructions without VEX prefix with the following differences:\nThe VEX-encoded instruction can have one more operand, making it non-destructive.\nA 128-bit XMM instruction without VEX prefix leaves the upper half of the full 256-bit YMM register unchanged, while the VEX-encoded version sets the upper half to zero.\nInstructions that use the whole 256-bit YMM register should not be mixed with non-VEX instructions that leave the upper half of the register unchanged, for reasons of efficiency.History[edit]\nIn August 2007, AMD proposed the SSE5 instruction set extension which includes a new coding scheme for instructions with three operands, using an extra byte named DREX intended for the Bulldozer processor core, due to begin production in 2011.[2][3]\nIn March 2008, Intel proposed the AVX instruction set, using the new VEX coding scheme.[4]\nIn August 2008, commentators deplored the expected incompatibility between AMD and Intel instruction sets, and proposed that AMD revise their plans and replace the DREX scheme with the more flexible and extensible VEX scheme.[5]\nIn May 2009, AMD announced a revision of the proposed SSE5 instruction set to make it compatible with the AVX instruction set and the VEX coding scheme. The revised SSE5 is called XOP.[6]\nJanuary 2011. The AVX instruction set is supported in Intel's Sandy Bridge microprocessor architecture.\n2011. The AVX, XOP and FMA4 instruction sets, all using the VEX scheme, are supported in the AMD Bulldozer processor.[7]\n2013. The FMA3 instruction set is supported in Intel Haswell processors.\nSee also[edit]\nEVEX prefix\nAVX-512\nReferences[edit]", "subtitles": ["Features", "Technical description", "History", "See also", "References"], "title": "VEX prefix"},
{"content": "In computer science, a pointer is a programming language object, whose value refers to (or points to) another value stored elsewhere in the computer memory using its memory address. A pointer references a location in memory, and obtaining the value stored at that location is known as dereferencing the pointer. As an analogy, a page number in a book's index could be considered a pointer to the corresponding page; dereferencing such a pointer would be done by flipping to the page with the given page number and reading the text found on the indexed page.Pointers to data significantly improve performance for repetitive operations such as traversing strings, lookup tables, control tables and tree structures. In particular, it is often much cheaper in time and space to copy and dereference pointers than it is to copy and access the data to which the pointers point.Pointers are also used to hold the addresses of entry points for called subroutines in procedural programming and for run-time linking to dynamic link libraries (DLLs). In object-oriented programming, pointers to functions are used for binding methods, often using what are called virtual method tables.A pointer is a simple, more concrete implementation of the more abstract reference data type. Several languages support some type of pointer, although some have more restrictions on their use than others. While pointer has been used to refer to references in general, it more properly applies to data structures whose interface explicitly allows the pointer to be manipulated (arithmetically via pointer arithmetic) as a memory address, as opposed to a magic cookie or capability where this is not possible.[citation needed] Because pointers allow both protected and unprotected access to memory addresses, there are risks associated with using them particularly in the latter case. Primitive pointers are often stored in a format similar to an integer; however, attempting to dereference or look up a pointer whose value was never a valid memory address would cause a program to crash. To alleviate this potential problem, as a matter of type safety, pointers are considered a separate type parameterized by the type of data they point to, even if the underlying representation is an integer. Other measures may also be taken (such as validation & bounds checking), to verify the contents of the pointer variable contain a value that is both a valid memory address and within the numerical range that the processor is capable of addressing.History[edit]Harold Lawson is credited with the 1964 invention of the pointer.[2] In 2000, Lawson was presented the Computer Pioneer Award by the IEEE \u201c[f]or inventing the pointer variable and introducing this concept into PL/I, thus providing for the first time, the capability to flexibly treat linked lists in a general-purpose high level language\u201d.[3] According to the Oxford English Dictionary, the word pointer first appeared in print as a stack pointer in a technical memorandum by the System Development Corporation.Formal description[edit]In computer science, a pointer is a kind of reference.A data primitive (or just primitive) is any datum that can be read from or written to computer memory using one memory access (for instance, both a byte and a word are primitives).A data aggregate (or just aggregate) is a group of primitives that are logically contiguous in memory and that are viewed collectively as one datum (for instance, an aggregate could be 3 logically contiguous bytes, the values of which represent the 3 coordinates of a point in space). When an aggregate is entirely composed of the same type of primitive, the aggregate may be called an array; in a sense, a multi-byte word primitive is an array of bytes, and some programs use words in this way.In the context of these definitions, a byte is the smallest primitive; each memory address specifies a different byte. The memory address of the initial byte of a datum is considered the memory address (or base memory address) of the entire datum.A memory pointer (or just pointer) is a primitive, the value of which is intended to be used as a memory address; it is said that a pointer points to a memory address. It is also said that a pointer points to a datum [in memory] when the pointer's value is the datum's memory address.More generally, a pointer is a kind of reference, and it is said that a pointer references a datum stored somewhere in memory; to obtain that datum is to dereference the pointer. The feature that separates pointers from other kinds of reference is that a pointer's value is meant to be interpreted as a memory address, which is a rather low-level concept.References serve as a level of indirection: A pointer's value determines which memory address (that is, which datum) is to be used in a calculation. Because indirection is a fundamental aspect of algorithms, pointers are often expressed as a fundamental data type in programming languages; in statically (or strongly) typed programming languages, the type of a pointer determines the type of the datum to which the pointer points.Use in data structures[edit]When setting up data structures like lists, queues and trees, it is necessary to have pointers to help manage how the structure is implemented and controlled. Typical examples of pointers are start pointers, end pointers, and stack pointers. These pointers can either be absolute (the actual physical address or a virtual address in virtual memory) or relative (an offset from an absolute start address (base) that typically uses fewer bits than a full address, but will usually require one additional arithmetic operation to resolve).Relative addresses are a form of manual memory segmentation, and share many of its advantages and disadvantages. A two-byte offset, containing a 16-bit, unsigned integer, can be used to provide relative addressing for up to 64 kilobytes of a data structure. This can easily be extended to 128K, 256K or 512K if the address pointed to is forced to be aligned on a half-word, word or double-word boundary (but, requiring an additional shift left bitwise operation\u2014by 1, 2 or 3 bits\u2014in order to adjust the offset by a factor of 2, 4 or 8, before its addition to the base address). Generally, though, such schemes are a lot of trouble, and for convenience to the programmer absolute addresses (and underlying that, a flat address space) is preferred.A one byte offset, such as the hexadecimal ASCII value of a character (e.g. X'29') can be used to point to an alternative integer value (or index) in an array (e.g. X'01'). In this way, characters can be very efficiently translated from 'raw data' to a usable sequential index and then to an absolute address without a lookup table.Use in control tables[edit]Control tables that are used to control program flow usually make extensive use of pointers. The pointers, usually embedded in a table entry, may, for instance, be used to hold the entry points to subroutines to be executed, based on certain conditions defined in the same table entry. The pointers can however be simply indexes to other separate, but associated, tables comprising an array of the actual addresses or the addresses themselves (depending upon the programming language constructs available). They can also be used to point to earlier table entries (as in loop processing) or forward to skip some table entries (as in a switch or early exit from a loop). For this latter purpose, the pointer may simply be the table entry number itself and can be transformed into an actual address by simple arithmetic.Architectural roots[edit]Pointers are a very thin abstraction on top of the addressing capabilities provided by most modern architectures. In the simplest scheme, an address, or a numeric index, is assigned to each unit of memory in the system, where the unit is typically either a byte or a word \u2013 depending on whether the architecture is byte-addressable or word-addressable \u2013 effectively transforming all of memory into a very large array. The system would then also provide an operation to retrieve the value stored in the memory unit at a given address (usually utilizing the machine's general purpose registers).In the usual case, a pointer is large enough to hold more addresses than there are units of memory in the system. This introduces the possibility that a program may attempt to access an address which corresponds to no unit of memory, either because not enough memory is installed (i.e. beyond the range of available memory) or the architecture does not support such addresses. The first case may, in certain platforms such as the Intel x86 architecture, be called a segmentation fault (segfault). The second case is possible in the current implementation of AMD64, where pointers are 64 bit long and addresses only extend to 48 bits. Pointers must conform to certain rules (canonical addresses), so if a non-canonical pointer is dereferenced, the processor raises a general protection fault.On the other hand, some systems have more units of memory than there are addresses. In this case, a more complex scheme such as memory segmentation or paging is employed to use different parts of the memory at different times. The last incarnations of the x86 architecture support up to 36 bits of physical memory addresses, which were mapped to the 32-bit linear address space through the PAE paging mechanism. Thus, only 1/16 of the possible total memory may be accessed at a time. Another example in the same computer family was the 16-bit protected mode of the 80286 processor, which, though supporting only 16 MB of physical memory, could access up to 1 GB of virtual memory, but the combination of 16-bit address and segment registers made accessing more than 64 KB in one data structure cumbersome.In order to provide a consistent interface, some architectures provide memory-mapped I/O, which allows some addresses to refer to units of memory while others refer to device registers of other devices in the computer. There are analogous concepts such as file offsets, array indices, and remote object references that serve some of the same purposes as addresses for other types of objects.Uses[edit]Pointers are directly supported without restrictions in languages such as PL/I, C, C++, Pascal, FreeBASIC, and implicitly in most assembly languages. They are primarily used for constructing references, which in turn are fundamental to constructing nearly all data structures, as well as in passing data between different parts of a program.In functional programming languages that rely heavily on lists, pointers and references are managed abstractly by the language using internal constructs like cons.When dealing with arrays, the critical lookup operation typically involves a stage called address calculation which involves constructing a pointer to the desired data element in the array. In other data structures, such as linked lists, pointers are used as references to explicitly tie one piece of the structure to another.Pointers are used to pass parameters by reference. This is useful if the programmer wants a function's modifications to a parameter to be visible to the function's caller. This is also useful for returning multiple values from a function.Pointers can also be used to allocate and deallocate dynamic variables and arrays in memory. Since a variable will often become redundant after it has served its purpose, it is a waste of memory to keep it, and therefore it is good practice to deallocate it (using the original pointer reference) when it is no longer needed. Failure to do so may result in a memory leak (where available free memory gradually, or in severe cases rapidly, diminishes because of an accumulation of numerous redundant memory blocks).C pointers[edit]The basic syntax to define a pointer is:[4]This declares ptr as the identifier of an object of the following type:\npointer that points to an object of type int\nThis is usually stated more succinctly as ptr is a pointer to int.Because the C language does not specify an implicit initialization for objects of automatic storage duration,[5] care should often be taken to ensure that the address to which ptr points is valid; this is why it is sometimes suggested that a pointer be explicitly initialized to the null pointer value, which is traditionally specified in C with the standardized macro NULL:[6]Dereferencing a null pointer in C produces undefined behavior,[7] which could be catastrophic. However, most implementations[citation needed] simply halt execution of the program in question, usually with a segmentation fault.However, initializing pointers unnecessarily could hinder program analysis, thereby hiding bugs.In any case, once a pointer has been declared, the next logical step is for it to point at something:This assigns the value of the address of a to ptr. For example, if a is stored at memory location of 0x8130 then the value of ptr will be 0x8130 after the assignment. To dereference the pointer, an asterisk is used again:This means take the contents of ptr (which is 0x8130), locate that address in memory and set its value to 8. If a is later accessed again, its new value will be 8.This example may be clearer if memory is examined directly. Assume that a is located at address 0x8130 in memory and ptr at 0x8134; also assume this is a 32-bit machine such that an int is 32-bits wide. The following is what would be in memory after the following code snippet is executed:\n\n\n\nAddress\nContents\n\n\n0x8130\n0x00000005\n\n\n0x8134\n0x00000000\n\n\n\n(The NULL pointer shown here is 0x00000000.) By assigning the address of a to ptr:yields the following memory values:\n\n\n\nAddress\nContents\n\n\n0x8130\n0x00000005\n\n\n0x8134\n0x00008130\n\n\n\nThen by dereferencing ptr by coding:the computer will take the contents of ptr (which is 0x8130), 'locate' that address, and assign 8 to that location yielding the following memory:\n\n\n\nAddress\nContents\n\n\n0x8130\n0x00000008\n\n\n0x8134\n0x00008130\n\n\n\nClearly, accessing a will yield the value of 8 because the previous instruction modified the contents of a by way of the pointer ptr.C arrays[edit]In C, array indexing is formally defined in terms of pointer arithmetic; that is, the language specification requires that array[i] be equivalent to *(array + i).[8] Thus in C, arrays can be thought of as pointers to consecutive areas of memory (with no gaps),[8] and the syntax for accessing arrays is identical for that which can be used to dereference pointers. For example, an array array can be declared and used in the following manner:This allocates a block of five integers and names the block array, which acts as a pointer to the block. Another common use of pointers is to point to dynamically allocated memory from malloc which returns a consecutive block of memory of no less than the requested size that can be used as an array.While most operators on arrays and pointers are equivalent, the result of the sizeof operator differs. In this example, sizeof(array) will evaluate to 5*sizeof(int) (the size of the array), while sizeof(ptr) will evaluate to sizeof(int*), the size of the pointer itself.Default values of an array can be declared like:If array is located in memory starting at address 0x1000 on a 32-bit little-endian machine then memory will contain the following (values are in hexadecimal, like the addresses):\n\n\n\n\n0\n1\n2\n3\n\n\n1000\n2\n0\n0\n0\n\n\n1004\n4\n0\n0\n0\n\n\n1008\n3\n0\n0\n0\n\n\n100C\n1\n0\n0\n0\n\n\n1010\n5\n0\n0\n0\n\n\n\nRepresented here are five integers: 2, 4, 3, 1, and 5. These five integers occupy 32 bits (4 bytes) each with the least-significant byte stored first (this is a little-endian CPU architecture) and are stored consecutively starting at address 0x1000.The syntax for C with pointers is:\narray means 0x1000;\narray + 1 means 0x1004: the + 1 means to add the size of 1 int, which is 4 bytes;\n*array means to dereference the contents of array. Considering the contents as a memory address (0x1000), look up the value at that location (0x0002);\narray[i] means element number i, 0-based, of array which is translated into *(array + i).\nThe last example is how to access the contents of array. Breaking it down:\narray + i is the memory location of the (i)th element of array, starting at i=0;\n*(array + i) takes that memory address and dereferences it to access the value.\nC linked list[edit]Below is an example definition of a linked list in C.This pointer-recursive definition is essentially the same as the reference-recursive definition from the Haskell programming language:Nil is the empty list, and Cons a (Link a) is a cons cell of type a with another link also of type a.The definition with references, however, is type-checked and does not use potentially confusing signal values. For this reason, data structures in C are usually dealt with via wrapper functions, which are carefully checked for correctness.Pass-by-address using pointers[edit]Pointers can be used to pass variables by their address, allowing their value to be changed. For example, consider the following C code:Dynamic memory allocation[edit]In some programs, the required memory depends on what the user may enter. In such cases the programmer needs to allocate memory dynamically. This is done by allocating memory at the heap rather than on the stack, where variables usually are stored. (Variables can also be stored in the CPU registers, but that's another matter) Dynamic memory allocation can only be made through pointers, and names (like with common variables) can't be given.Pointers are used to store and manage the addresses of dynamically allocated blocks of memory. Such blocks are used to store data objects or arrays of objects. Most structured and object-oriented languages provide an area of memory, called the heap or free store, from which objects are dynamically allocated.The example C code below illustrates how structure objects are dynamically allocated and referenced. The standard C library provides the function malloc() for allocating memory blocks from the heap. It takes the size of an object to allocate as a parameter and returns a pointer to a newly allocated block of memory suitable for storing the object, or it returns a null pointer if the allocation failed.The code below illustrates how memory objects are dynamically deallocated, i.e., returned to the heap or free store. The standard C library provides the function free() for deallocating a previously allocated memory block and returning it back to the heap.Memory-mapped hardware[edit]On some computing architectures, pointers can be used to directly manipulate memory or memory-mapped devices.Assigning addresses to pointers is an invaluable tool when programming microcontrollers. Below is a simple example declaring a pointer of type int and initialising it to a hexadecimal address in this example the constant 0x7FFF:In the mid 80s, using the BIOS to access the video capabilities of PCs was slow. Applications that were display-intensive typically used to access CGA video memory directly by casting the hexadecimal constant 0xB8000 to a pointer to an array of 80 unsigned 16-bit int values. Each value consisted of an ASCII code in the low byte, and a colour in the high byte. Thus, to put the letter 'A' at row 5, column 2 in bright white on blue, one would write code like the following:Typed pointers and casting[edit]In many languages, pointers have the additional restriction that the object they point to has a specific type. For example, a pointer may be declared to point to an integer; the language will then attempt to prevent the programmer from pointing it to objects which are not integers, such as floating-point numbers, eliminating some errors.For example, in Cmoney would be an integer pointer and bags would be a char pointer. The following would yield a compiler warning of assignment from incompatible pointer type under GCCbecause money and bags were declared with different types. To suppress the compiler warning, it must be made explicit that you do indeed wish to make the assignment by typecasting itwhich says to cast the integer pointer of money to a char pointer and assign to bags.A 2005 draft of the C standard requires that casting a pointer derived from one type to one of another type should maintain the alignment correctness for both types (6.3.2.3 Pointers, par. 7):[9]In languages that allow pointer arithmetic, arithmetic on pointers takes into account the size of the type. For example, adding an integer number to a pointer produces another pointer that points to an address that is higher by that number times the size of the type. This allows us to easily compute the address of elements of an array of a given type, as was shown in the C arrays example above. When a pointer of one type is cast to another type of a different size, the programmer should expect that pointer arithmetic will be calculated differently. In C, for example, if the money array starts at 0x2000 and sizeof(int) is 4 bytes whereas sizeof(char) is 1 byte, then money + 1 will point to 0x2004, but bags + '1' would point to 0x2001. Other risks of casting include loss of data when wide data is written to narrow locations (e.g. bags[0] = 65537;), unexpected results when bit-shifting values, and comparison problems, especially with signed vs unsigned values.Although it is impossible in general to determine at compile-time which casts are safe, some languages store run-time type information which can be used to confirm that these dangerous casts are valid at runtime. Other languages merely accept a conservative approximation of safe casts, or none at all.Making pointers safer[edit]As a pointer allows a program to attempt to access an object that may not be defined, pointers can be the origin of a variety of programming errors. However, the usefulness of pointers is so great that it can be difficult to perform programming tasks without them. Consequently, many languages have created constructs designed to provide some of the useful features of pointers without some of their pitfalls, also sometimes referred to as pointer hazards. In this context, pointers that directly address memory (as used in this article) are referred to as raw pointers, by contrast with smart pointers or other variants.One major problem with pointers is that as long as they can be directly manipulated as a number, they can be made to point to unused addresses or to data which is being used for other purposes. Many languages, including most functional programming languages and recent imperative languages like Java, replace pointers with a more opaque type of reference, typically referred to as simply a reference, which can only be used to refer to objects and not manipulated as numbers, preventing this type of error. Array indexing is handled as a special case.A pointer which does not have any address assigned to it is called a wild pointer. Any attempt to use such uninitialized pointers can cause unexpected behavior, either because the initial value is not a valid address, or because using it may damage other parts of the program. The result is often a segmentation fault, storage violation or wild branch (if used as a function pointer or branch address).In systems with explicit memory allocation, it is possible to create a dangling pointer by deallocating the memory region it points into. This type of pointer is dangerous and subtle because a deallocated memory region may contain the same data as it did before it was deallocated but may be then reallocated and overwritten by unrelated code, unknown to the earlier code. Languages with garbage collection prevent this type of error because deallocation is performed automatically when there are no more references in scope.Some languages, like C++, support smart pointers, which use a simple form of reference counting to help track allocation of dynamic memory in addition to acting as a reference. In the absence of reference cycles, where an object refers to itself indirectly through a sequence of smart pointers, these eliminate the possibility of dangling pointers and memory leaks. Delphi strings support reference counting natively.The Rust programming language introduces a borrow checker, pointer lifetimes, and an optimisation based around optional types for null pointers to eliminate pointer bugs, without resorting to a garbage collector.Null pointer[edit]A null pointer has a value reserved for indicating that the pointer does not refer to a valid object. Null pointers are routinely used to represent conditions such as the end of a list of unknown length or the failure to perform some action; this use of null pointers can be compared to nullable types and to the Nothing value in an option type.Autorelative pointer[edit]An autorelative pointer is a pointer whose value is interpreted as an offset from the address of the pointer itself; thus, if a data structure has an autorelative pointer member that points to some portion of the data structure itself, then the data structure may be relocated in memory without having to update the value of the auto relative pointer.[10]The cited patent also uses the term self-relative pointer to mean the same thing. However, the meaning of that term has been used in other ways:\nto mean an offset from the address of a structure rather than from the address of the pointer itself;[citation needed]\nto mean a pointer containing its own address, which can be useful for reconstructing in any arbitrary region of memory a collection of data structures that point to each other.[11]\nBased pointer[edit]A based pointer is a pointer whose value is an offset from the value of another pointer. This can be used to store and load blocks of data, assigning the address of the beginning of the block to the base pointer.[12]Multiple indirection[edit]In some languages, a pointer can reference another pointer, requiring multiple dereference operations to get to the original value. While each level of indirection may add a performance cost, it is sometimes necessary in order to provide correct behavior for complex data structures. For example, in C it is typical to define a linked list in terms of an element that contains a pointer to the next element of the list:This implementation uses a pointer to the first element in the list as a surrogate for the entire list. If a new value is added to the beginning of the list, head has to be changed to point to the new element. Since C arguments are always passed by value, using double indirection allows the insertion to be implemented correctly, and has the desirable side-effect of eliminating special case code to deal with insertions at the front of the list:In this case, if the value of item is less than that of head, the caller's head is properly updated to the address of the new item.A basic example is in the argv argument to the main function in C (and C++), which is given in the prototype as char **argv\u2014this is because the variable argv itself is a pointer to an array of strings (an array of arrays), so *argv is a pointer to the 0th string (by convention the name of the program), and **argv is the 0th character of the 0th string.Function pointer[edit]In some languages, a pointer can reference executable code, i.e., it can point to a function, method, or procedure. A function pointer will store the address of a function to be invoked. While this facility can be used to call functions dynamically, it is often a favorite technique of virus and other malicious software writers.Dangling pointer[edit]A dangling pointer is a pointer that does not point to a valid object and consequently may make a program crash or behave oddly. In the Pascal or C programming languages, pointers that are not specifically initialized may point to unpredictable addresses in memory.The following example code shows a dangling pointer:Here, p2 may point to anywhere in memory, so performing the assignment *p2 = 'b'; can corrupt an unknown area of memory or trigger a segmentation fault.Back pointer[edit]In doubly linked lists or tree structures, a back pointer held on an element 'points back' to the item referring to the current element. These are useful for navigation and manipulation, at the expense of greater memory use.Pointer declaration syntax overview[edit]These pointer declarations cover most variants of pointer declarations. Of course it is possible to have triple pointers, but the main principles behind a triple pointer already exist in a double pointer.The () and [] have a higher priority than *. [13]Wild branch[edit]Where a pointer is used as the address of the entry point to a program or start of a function which doesn't return anything and is also either uninitialized or corrupted, if a call or jump is nevertheless made to this address, a wild branch is said to have occurred. The consequences are usually unpredictable and the error may present itself in several different ways depending upon whether or not the pointer is a valid address and whether or not there is (coincidentally) a valid instruction (opcode) at that address. The detection of a wild branch can present one of the most difficult and frustrating debugging exercises since much of the evidence may already have been destroyed beforehand or by execution of one or more inappropriate instructions at the branch location. If available, an instruction set simulator can usually not only detect a wild branch before it takes effect, but also provide a complete or partial trace of its history.Simulation using an array index[edit]It is possible to simulate pointer behavior using an index to an (normally one-dimensional) array.Primarily for languages which do not support pointers explicitly but do support arrays, the array can be thought of and processed as if it were the entire memory range (within the scope of the particular array) and any index to it can be thought of as equivalent to a general purpose register in assembly language (that points to the individual bytes but whose actual value is relative to the start of the array, not its absolute address in memory). Assuming the array is, say, a contiguous 16 megabyte character data structure, individual bytes (or a string of contiguous bytes within the array) can be directly addressed and manipulated using the name of the array with a 31 bit unsigned integer as the simulated pointer (this is quite similar to the C arrays example shown above). Pointer arithmetic can be simulated by adding or subtracting from the index, with minimal additional overhead compared to genuine pointer arithmetic.It is even theoretically possible, using the above technique, together with a suitable instruction set simulator to simulate any machine code or the intermediate (byte code) of any processor/language in another language that does not support pointers at all (for example Java / JavaScript). To achieve this, the binary code can initially be loaded into contiguous bytes of the array for the simulator to read, interpret and action entirely within the memory contained of the same array. If necessary, to completely avoid buffer overflow problems, bounds checking can usually be actioned for the compiler (or if not, hand coded in the simulator).Support in various programming languages[edit]Ada[edit]Ada is a strongly typed language where all pointers are typed and only safe type conversions are permitted. All pointers are by default initialized to null, and any attempt to access data through a null pointer causes an exception to be raised. Pointers in Ada are called access types. Ada 83 did not permit arithmetic on access types (although many compiler vendors provided for it as a non-standard feature), but Ada 95 supports \u201csafe\u201d arithmetic on access types via the package System.Storage_Elements.BASIC[edit]Several old versions of BASIC for the Windows platform had support for STRPTR() to return the address of a string, and for VARPTR() to return the address of a variable. Visual Basic 5 also had support for OBJPTR() to return the address of an object interface, and for an ADDRESSOF operator to return the address of a function. The types of all of these are integers, but their values are equivalent to those held by pointer types.Newer dialects of BASIC, such as FreeBASIC or BlitzMax, have exhaustive pointer implementations, however. In FreeBASIC, arithmetic on ANY pointers (equivalent to C's void*) are treated as though the ANY pointer was a byte width. ANY pointers cannot be dereferenced, as in C. Also, casting between ANY and any other type's pointers will not generate any warnings.C and C++[edit]In C and C++ pointers are variables that store addresses and can be null. Each pointer has a type it points to, but one can freely cast between pointer types (but not between a function pointer and an object pointer). A special pointer type called the \u201cvoid pointer\u201d allows pointing to any (non-function) object, but is limited by the fact that it cannot be dereferenced directly (it shall be cast). The address itself can often be directly manipulated by casting a pointer to and from an integral type of sufficient size, though the results are implementation-defined and may indeed cause undefined behavior; while earlier C standards did not have an integral type that was guaranteed to be large enough, C99 specifies the uintptr_t typedef name defined in <stdint.h>, but an implementation need not provide it.C++ fully supports C pointers and C typecasting. It also supports a new group of typecasting operators to help catch some unintended dangerous casts at compile-time. Since C++11, the C++ standard library also provides smart pointers (unique_ptr, shared_ptr and weak_ptr) which can be used in some situations as a safer alternative to primitive C pointers. C++ also supports another form of reference, quite different from a pointer, called simply a reference or reference type.Pointer arithmetic, that is, the ability to modify a pointer's target address with arithmetic operations (as well as magnitude comparisons), is restricted by the language standard to remain within the bounds of a single array object (or just after it), and will otherwise invoke undefined behavior. Adding or subtracting from a pointer moves it by a multiple of the size of its datatype. For example, adding 1 to a pointer to 4-byte integer values will increment the pointer's pointed-to byte-address by 4. This has the effect of incrementing the pointer to point at the next element in a contiguous array of integers\u2014which is often the intended result. Pointer arithmetic cannot be performed on void pointers because the void type has no size, and thus the pointed address can not be added to, although gcc and other compilers will perform byte arithmetic on void* as a non-standard extension, treating it as if it were char *.Pointer arithmetic provides the programmer with a single way of dealing with different types: adding and subtracting the number of elements required instead of the actual offset in bytes. (Pointer arithmetic with char * pointers uses byte offsets, because sizeof(char) is 1 by definition.) In particular, the C definition explicitly declares that the syntax a[n], which is the n-th element of the array a, is equivalent to *(a + n), which is the content of the element pointed by a + n. This implies that n[a] is equivalent to a[n], and one can write, e.g., a[3] or 3[a] equally well to access the fourth element of an array a.While powerful, pointer arithmetic can be a source of computer bugs. It tends to confuse novice programmers, forcing them into different contexts: an expression can be an ordinary arithmetic one or a pointer arithmetic one, and sometimes it is easy to mistake one for the other. In response to this, many modern high-level computer languages (for example Java) do not permit direct access to memory using addresses. Also, the safe C dialect Cyclone addresses many of the issues with pointers. See C programming language for more discussion.The void pointer, or void*, is supported in ANSI C and C++ as a generic pointer type. A pointer to void can store the address of any object (not function), and, in C, is implicitly converted to any other object pointer type on assignment, but it must be explicitly cast if dereferenced. K&R C used char* for the \u201ctype-agnostic pointer\u201d purpose (before ANSI C).C++ does not allow the implicit conversion of void* to other pointer types, even in assignments. This was a design decision to avoid careless and even unintended casts, though most compilers only output warnings, not errors, when encountering other casts.In C++, there is no void& (reference to void) to complement void* (pointer to void), because references behave like aliases to the variables they point to, and there can never be a variable whose type is void.C#[edit]In the C# programming language, pointers are supported only under certain conditions: any block of code including pointers must be marked with the unsafe keyword. Such blocks usually require higher security permissions to be allowed to run. The syntax is essentially the same as in C++, and the address pointed can be either managed or unmanaged memory. However, pointers to managed memory (any pointer to a managed object) must be declared using the fixed keyword, which prevents the garbage collector from moving the pointed object as part of memory management while the pointer is in scope, thus keeping the pointer address valid.An exception to this is from using the IntPtr structure, which is a safe managed equivalent to int*, and does not require unsafe code. This type is often returned when using methods from the System.Runtime.InteropServices, for example:The .NET framework includes many classes and methods in the System and System.Runtime.InteropServices namespaces (such as the Marshal class) which convert .NET types (for example, System.String) to and from many unmanaged types and pointers (for example, LPWSTR or void*) to allow communication with unmanaged code. Most such methods have the same security permission requirements as unmanaged code, since they can affect arbitrary places in memory.COBOL[edit]The COBOL programming language supports pointers to variables. Primitive or group (record) data objects declared within the LINKAGE SECTION of a program are inherently pointer-based, where the only memory allocated within the program is space for the address of the data item (typically a single memory word). In program source code, these data items are used just like any other WORKING-STORAGE variable, but their contents are implicitly accessed indirectly through their LINKAGE pointers.Memory space for each pointed-to data object is typically allocated dynamically using external CALL statements or via embedded extended language constructs such as EXEC CICS or EXEC SQL statements.Extended versions of COBOL also provide pointer variables declared with USAGE IS POINTER clauses. The values of such pointer variables are established and modified using SET and SET ADDRESS statements.Some extended versions of COBOL also provide PROCEDURE-POINTER variables, which are capable of storing the addresses of executable code.PL/I[edit]The PL/I language provides full support for pointers to all data types (including pointers to structures), recursion, multitasking, string handling, and extensive built-in functions. PL/I was quite a leap forward compared to the programming languages of its time.[citation needed]D[edit]The D programming language is a derivative of C and C++ which fully supports C pointers and C typecasting.Eiffel[edit]The Eiffel object-oriented language employs value and reference semantics without pointer arithmetic. Nevertheless, pointer classes are provided. They offer pointer arithmetic, typecasting, explicit memory management, interfacing with non-Eiffel software, and other features.Fortran[edit]Fortran-90 introduced a strongly typed pointer capability. Fortran pointers contain more than just a simple memory address. They also encapsulate the lower and upper bounds of array dimensions, strides (for example, to support arbitrary array sections), and other metadata. An association operator, => is used to associate a POINTER to a variable which has a TARGET attribute. The Fortran-90 ALLOCATE statement may also be used to associate a pointer to a block of memory. For example, the following code might be used to define and create a linked list structure:Fortran-2003 adds support for procedure pointers. Also, as part of the C Interoperability feature, Fortran-2003 supports intrinsic functions for converting C-style pointers into Fortran pointers and back.Go[edit]Go has pointers. Its declaration syntax is equivalent to that of C, but written the other way around, ending with the type. Unlike C, Go has garbage collection, and disallows pointer arithmetic. Reference types, like in C++, do not exist. Some built-in types, like maps and channels, are boxed (i.e. internally they are pointers to mutable structures), and are initialized using the make function. In an approach to unified syntax between pointers and non-pointers, the arrow (->) operator has been dropped: the dot operator on a pointer refers to the field or method of the dereferenced object. This, however, only works with 1 level of indirection.Java[edit]Unlike C, C++, or Pascal, there is no explicit representation of pointers in Java. Instead, more complex data structures like objects and arrays are implemented using references. The language does not provide any explicit pointer manipulation operators. It is still possible for code to attempt to dereference a null reference (null pointer), however, which results in a run-time exception being thrown. The space occupied by unreferenced memory objects is recovered automatically by garbage collection at run-time.[14]Modula-2[edit]Pointers are implemented very much as in Pascal, as are VAR parameters in procedure calls. Modula-2 is even more strongly typed than Pascal, with fewer ways to escape the type system. Some of the variants of Modula-2 (such as Modula-3) include garbage collection.Oberon[edit]Much as with Modula-2, pointers are available. There are still fewer ways to evade the type system and so Oberon and its variants are still safer with respect to pointers than Modula-2 or its variants. As with Modula-3, garbage collection is a part of the language specification.Pascal[edit]Unlike many languages that feature pointers, standard ISO Pascal only allows pointers to reference dynamically created variables that are anonymous and does not allow them to reference standard static or local variables.[15] It does not have pointer arithmetic. Pointers also must have an associated type and a pointer to one type is not compatible with a pointer to another type (e.g. a pointer to a char is not compatible with a pointer to an integer). This helps eliminate the type security issues inherent with other pointer implementations, particularly those used for PL/I or C. It also removes some risks caused by dangling pointers, but the ability to dynamically let go of referenced space by using the dispose standard procedure (which has the same effect as the free library function found in C) means that the risk of dangling pointers has not been entirely eliminated.[16]However, in some commercial and open source Pascal (or derivatives) compiler implementations \u2014like Free Pascal,[17] Turbo Pascal or the Object Pascal in Embarcadero Delphi\u2014 a pointer is allowed to reference standard static or local variables and can be cast from one pointer type to another. Moreover, pointer arithmetic is unrestricted: adding or subtracting from a pointer moves it by that number of bytes in either direction, but using the Inc or Dec standard procedures with it moves the pointer by the size of the data type it is declared to point to. An untyped pointer is also provided under the name Pointer, which is compatible with other pointer types.Perl[edit]The Perl programming language supports pointers, although rarely used, in the form of the pack and unpack functions. These are intended only for simple interactions with compiled OS libraries. In all other cases, Perl uses references, which are typed and do not allow any form of pointer arithmetic. They are used to construct complex data structures.[18]See also[edit]References[edit]External links[edit]\ncdecl.org A tool to convert pointer declarations to plain English\nOver IQ.com A beginner level guide describing pointers in a plain English.\nPointers and Memory Introduction to pointers \u2013 Stanford Computer Science Education Library\nPointers in C programming A visual model for the beginners in C programming\n0pointer.de A terse list of minimum length source codes that dereference a null pointer in several different programming languages\nThe C book \u2013 containing pointer examples in ANSI C\nJoint Technical Committee ISO/IEC JTC 1, Subcommittee SC 22, Working Group WG 14 (2007-09-08). International Standard ISO/IEC 9899 (PDF). Committee Draft. CS1 maint: Multiple names: authors list (link) .\n", "subtitles": ["History", "Formal description", "Use in data structures", "Use in control tables", "Architectural roots", "Uses", "Typed pointers and casting", "Making pointers safer", "Null pointer", "Autorelative pointer", "Based pointer", "Multiple indirection", "Function pointer", "Dangling pointer", "Back pointer", "Pointer declaration syntax overview", "Wild branch", "Simulation using an array index", "Support in various programming languages", "See also", "References", "External links"], "title": "Pointer (computer programming)"},
{"content": "A language construct is a syntactically allowable part of a program that may be formed from one or more lexical tokens in accordance with the rules of a programming language. In simpler terms, it is the syntax/way a programming language is written.The term language construct is often used as a synonym for control structure, and should not be confused with a function.[clarification needed]Examples of language constructs[edit]In PHP print is a language construct. [1]is the same as:In Java a class is written in this format:In C++ a class is written in this format:", "subtitles": [], "title": "Language construct"},
{"content": "In programming languages, a closure (also lexical closure or function closure) is a technique for implementing lexically scoped name binding in a language with first-class functions. Operationally, a closure is a record storing a function[a] together with an environment.[1] The environment is a mapping associating each free variable of the function (variables that are used locally, but defined in an enclosing scope) with the value or reference to which the name was bound when the closure was created.[b] A closure\u2014unlike a plain function\u2014allows the function to access those captured variables through the closure's copies of their values or references, even when the function is invoked outside their scope.Example. The following program fragment defines a higher-order function (function returning a function) add with a parameter x and a nested function addX. The nested function addX has access to x, because x is in the lexical scope of addX. The function add returns a closure; this closure contains (1) a reference to the function addX, and (2) a copy of the environment around addX in which x has the value given in that specific invocation of add.\nfunction add(x)\n   function addX(y)\n       return y + x\n   return addX\n\nvariable add1 = add(1)\nvariable add5 = add(5)\n\nassert add1(3) = 4\nassert add5(3) = 8\nNote that, as add returns a value of function type, the variables add1 and add5 are also of function type. Invoking add1(3) will return 4, because it assigns 3 to parameter y in the call to addX, using the saved environment for addX where x is 1. Invoking add5(3) will return 8, because while it also assigns 3 to parameter y in the call to addX, it is now using another saved environment for addX where x is 5. So while add1 and add5 both use the same function addX, the associated environments differ, and invoking the closures will bind the name x to two different values in the two invocations, thus evaluating the function to two different results.History and etymology[edit]The concept of closures was developed in the 1960s for the mechanical evaluation of expressions in the \u03bb-calculus and was first[2] fully implemented in 1970 as a language feature in the PAL programming language to support lexically scoped first-class functions.[2]:Turner's section 2, note 8 contains his claim about M-expressionsPeter J. Landin defined the term closure in 1964 as having an environment part and a control part as used by his SECD machine for evaluating expressions.[3] Joel Moses credits Landin with introducing the term closure to refer to a lambda expression whose open bindings (free variables) have been closed by (or bound in) the lexical environment, resulting in a closed expression, or closure.[4][5] This usage was subsequently adopted by Sussman and Steele when they defined Scheme in 1975,[6] a lexically scoped variant of LISP, and became widespread.Anonymous functions[edit]The term closure is often mistakenly used to mean anonymous function. This is probably because many programmers learn about both concepts at the same time, in the form of small helper functions that are anonymous closures. An anonymous function is a function literal without a name, while a closure is an instance of a function, a value, whose non-local variables have been bound either to values or to storage locations (depending on the language; see the lexical environment section below).For example, in the following Python code:the values of a and b are closures, in both cases produced by returning a nested function with a free variable from the enclosing function, so that the free variable binds to the value of parameter x of the enclosing function. The closures in a and b are functionally identical. The only difference in implementation is that in the first case we used a nested function with a name, g, while in the second case we used an anonymous nested function (using the Python keyword lambda for creating an anonymous function). The original name, if any, used in defining them is irrelevant.A closure is a value like any other value. It doesn't need to be assigned to a variable and can instead be used directly, as shown in the last two lines of the example. This usage may be deemed an anonymous closure.Note especially that the nested function definitions are not themselves closures: they have a free variable which is not yet bound. Only once the enclosing function is evaluated with a value for the parameter is the free variable of the nested function bound, creating a closure, which is then returned from the enclosing function.Lastly, a closure is only distinct from a function with free variables when outside of the scope of the non-local variables, otherwise the defining environment and the execution environment coincide and there is nothing to distinguish these (static and dynamic binding can't be distinguished because the names resolve to the same values). For example, in the below program, functions with a free variable x (bound to the non-local variable x with global scope) are executed in the same environment where x is defined, so it is immaterial whether these are actually closures:This is most often achieved by a function return, since the function must be defined within the scope of the non-local variables, in which case typically its own scope will be smaller.This can also be achieved by variable shadowing (which reduces the scope of the non-local variable), though this is less common in practice, as it is less useful and shadowing is discouraged. In this example f can be seen to be a closure because x in the body of f is bound to the x in the global namespace, not the x local to g:Applications[edit]The use of closures is associated with languages where functions are first-class objects, in which functions can be returned as results from higher-order functions, or passed as arguments to other function calls; if functions with free variables are first-class, then returning one creates a closure. This includes functional programming languages such as Lisp and ML, as well as many modern garbage-collected imperative languages, such as Python. Closures are also frequently used with callbacks, particularly for event handlers, such as in JavaScript, where they are used for interactions with a dynamic web page. Traditional imperative languages such as Algol, C and Pascal either do not support nested functions (C) or do not support calling nested functions after the enclosing function has exited (GNU C, Pascal), thus avoiding the need to use closures.Closures are used to implement continuation-passing style, and in this manner, hide state. Constructs such as objects and control structures can thus be implemented with closures. In some languages, a closure may occur when a function is defined within another function, and the inner function refers to local variables of the outer function. At run-time, when the outer function executes, a closure is formed, consisting of the inner function\u2019s code and references (the upvalues) to any variables of the outer function required by the closure.First-class functions[edit]Closures typically appear in languages in which functions are first-class values\u2014in other words, such languages enable functions to be passed as arguments, returned from function calls, bound to variable names, etc., just like simpler types such as strings and integers. For example, consider the following Scheme function:In this example, the lambda expression (lambda (book) (>= (book-sales book) threshold)) appears within the function best-selling-books. When the lambda expression is evaluated, Scheme creates a closure consisting of the code for the lambda expression and a reference to the threshold variable, which is a free variable inside the lambda expression.The closure is then passed to the filter function, which calls it repeatedly to determine which books are to be added to the result list and which are to be discarded. Because the closure itself has a reference to threshold, it can use that variable each time filter calls it. The function filter itself might be defined in a completely separate file.Here is the same example rewritten in JavaScript, another popular language with support for closures:The function keyword is used here instead of lambda, and an Array.filter method[7] instead of a global filter function, but otherwise the structure and the effect of the code are the same.A function may create a closure and return it, as in the following example:Because the closure in this case outlives the execution of the function that creates it, the variables f and dx live on after the function derivative returns, even though execution has left their scope and they are no longer visible. In languages without closures, the lifetime of an automatic local variable coincides with the execution of the stack frame where that variable is declared. In languages with closures, variables must continue to exist as long as any existing closures have references to them. This is most commonly implemented using some form of garbage collection.State representation[edit]A closure can be used to associate a function with a set of private variables, which persist over several invocations of the function. The scope of the variable encompasses only the closed-over function, so it cannot be accessed from other program code.In stateful languages, closures can thus be used to implement paradigms for state representation and information hiding, since the closure's upvalues (its closed-over variables) are of indefinite extent, so a value established in one invocation remains available in the next. Closures used in this way no longer have referential transparency, and are thus no longer pure functions; nevertheless, they are commonly used in impure functional languages such as Scheme.Other uses[edit]Closures have many uses:\nBecause closures delay evaluation\u2014i.e., they do not do anything until they are called\u2014they can be used to define control structures. For example, all of Smalltalk's standard control structures, including branches (if/then/else) and loops (while and for), are defined using objects whose methods accept closures. Users can easily define their own control structures also.\nIn languages which implement assignment, multiple functions can be produced that close over the same environment, enabling them to communicate privately by altering that environment. In Scheme:\n\nClosures can be used to implement object systems.[8]\nNote: Some speakers call any data structure that binds a lexical environment a closure, but the term usually refers specifically to functions.Implementation and theory[edit]Closures are typically implemented with a special data structure that contains a pointer to the function code, plus a representation of the function's lexical environment (i.e., the set of available variables) at the time when the closure was created. The referencing environment binds the non-local names to the corresponding variables in the lexical environment at the time the closure is created, additionally extending their lifetime to at least as long as the lifetime of the closure itself. When the closure is entered at a later time, possibly with a different lexical environment, the function is executed with its non-local variables referring to the ones captured by the closure, not the current environment.A language implementation cannot easily support full closures if its run-time memory model allocates all automatic variables on a linear stack. In such languages, a function's automatic local variables are deallocated when the function returns. However, a closure requires that the free variables it references survive the enclosing function's execution. Therefore, those variables must be allocated so that they persist until no longer needed, typically via heap allocation, rather than on the stack, and their lifetime must be managed so they survive until all closures referencing them are no longer in use.This explains why, typically, languages that natively support closures also use garbage collection. The alternatives are manual memory management of non-local variables (explicitly allocating on the heap and freeing when done), or, if using stack allocation, for the language to accept that certain use cases will lead to undefined behaviour, due to dangling pointers to freed automatic variables, as in lambda expressions in C++11[9] or nested functions in GNU C.[10] The funarg problem (or functional argument problem) describes the difficulty of implementing functions as first class objects in a stack-based programming language such as C or C++. Similarly in D version 1, it is assumed that the programmer knows what to do with delegates and automatic local variables, as their references will be invalid after return from its definition scope (automatic local variables are on the stack) \u2013 this still permits many useful functional patterns, but for complex cases needs explicit heap allocation for variables. D version 2 solved this by detecting which variables must be stored on the heap, and performs automatic allocation. Because D uses garbage collection, in both versions, there is no need to track usage of variables as they are passed.In strict functional languages with immutable data (e.g. Erlang), it is very easy to implement automatic memory management (garbage collection), as there are no possible cycles in variables' references. For example, in Erlang, all arguments and variables are allocated on the heap, but references to them are additionally stored on the stack. After a function returns, references are still valid. Heap cleaning is done by incremental garbage collector.In ML, local variables are lexically scoped, and hence define a stack-like model, but since they are bound to values and not to objects, an implementation is free to copy these values into the closure's data structure in a way that is invisible to the programmer.Scheme, which has an ALGOL-like lexical scope system with dynamic variables and garbage collection, lacks a stack programming model and does not suffer from the limitations of stack-based languages. Closures are expressed naturally in Scheme. The lambda form encloses the code, and the free variables of its environment persist within the program as long as they can possibly be accessed, and so they can be used as freely as any other Scheme expression.[citation needed]Closures are closely related to Actors in the Actor model of concurrent computation where the values in the function's lexical environment are called acquaintances. An important issue for closures in concurrent programming languages is whether the variables in a closure can be updated and, if so, how these updates can be synchronized. Actors provide one solution.[11]Closures are closely related to function objects; the transformation from the former to the latter is known as defunctionalization or lambda lifting; see also closure conversion.[citation needed]Differences in semantics[edit]Lexical environment[edit]As different languages do not always have a common definition of the lexical environment, their definitions of closure may vary also. The commonly held minimalist definition of the lexical environment defines it as a set of all bindings of variables in the scope, and that is also what closures in any language have to capture. However the meaning of a variable binding also differs. In imperative languages, variables bind to relative locations in memory that can store values. Although the relative location of a binding does not change at runtime, the value in the bound location can. In such languages, since closure captures the binding, any operation on the variable, whether done from the closure or not, are performed on the same relative memory location. This is often called capturing the variable by reference. Here is an example illustrating the concept in ECMAScript, which is one such language:Note how function foo and the closures referred to by variables f and g all use the same relative memory location signified by local variable x.On the other hand, many functional languages, such as ML, bind variables directly to values. In this case, since there is no way to change the value of the variable once it is bound, there is no need to share the state between closures\u2014they just use the same values. This is often called capturing the variable by value. Java's local and anonymous classes also fall into this category\u2014they require captured local variables to be final, which also means there is no need to share state.Some languages enable you to choose between capturing the value of a variable or its location. For example, in C++11, captured variables are either declared with [&], which means captured by reference, or with [=], which means captured by value.Yet another subset, lazy functional languages such as Haskell, bind variables to results of future computations rather than values. Consider this example in Haskell:The binding of r captured by the closure defined within function foo is to the computation (x / y)\u2014which in this case results in division by zero. However, since it is the computation that is captured, and not the value, the error only manifests itself when the closure is invoked, and actually attempts to use the captured binding.Closure leaving[edit]Yet more differences manifest themselves in the behavior of other lexically scoped constructs, such as return, break and continue statements. Such constructs can, in general, be considered in terms of invoking an escape continuation established by an enclosing control statement (in case of break and continue, such interpretation requires looping constructs to be considered in terms of recursive function calls). In some languages, such as ECMAScript, return refers to the continuation established by the closure lexically innermost with respect to the statement\u2014thus, a return within a closure transfers control to the code that called it. However, in Smalltalk, the superficially similar operator ^ invokes the escape continuation established for the method invocation, ignoring the escape continuations of any intervening nested closures. The escape continuation of a particular closure can only be invoked in Smalltalk implicitly by reaching the end of the closure's code. The following examples in ECMAScript and Smalltalk highlight the difference:The above code snippets will behave differently because the Smalltalk ^ operator and the JavaScript return operator are not analogous. In the ECMAScript example, return x will leave the inner closure to begin a new iteration of the forEach loop, whereas in the Smalltalk example, ^x will abort the loop and return from the method foo.Common Lisp provides a construct that can express either of the above actions: Lisp (return-from foo x) behaves as Smalltalk ^x, while Lisp (return-from nil x) behaves as JavaScript return x. Hence, Smalltalk makes it possible for a captured escape continuation to outlive the extent in which it can be successfully invoked. Consider:When the closure returned by the method foo is invoked, it attempts to return a value from the invocation of foo that created the closure. Since that call has already returned and the Smalltalk method invocation model does not follow the spaghetti stack discipline to facilitate multiple returns, this operation results in an error.Some languages, such as Ruby, enable the programmer to choose the way return is captured. An example in Ruby:Both Proc.new and lambda in this example are ways to create a closure, but semantics of the closures thus created are different with respect to the return statement.In Scheme, definition and scope of the return control statement is explicit (and only arbitrarily named 'return' for the sake of the example). The following is a direct translation of the Ruby sample.Closure-like constructs[edit]Features of some languages simulate some features of closures. Language features include some object-oriented techniques, for example in Java, C++, Objective-C, C#, D.Callbacks (C)[edit]Some C libraries support callbacks. This is sometimes implemented by providing two values when registering the callback with the library: a function pointer and a separate void* pointer to arbitrary data of the user's choice. When the library executes the callback function, it passes along the data pointer. This enables the callback to maintain state and to refer to information captured at the time it was registered with the library. The idiom is similar to closures in functionality, but not in syntax. The void* pointer is not type safe so this C idiom differs from type-safe closures in C#, Haskell or ML.Nested function and function pointer(C)With a gcc extension, a nested function can be used and a function pointer can emulate closures, providing the containing function does not exit. The example below is invalid:Local classes and lambda functions (Java)[edit]Java enables classes to be defined inside methods. These are called local classes. When such classes are not named, they are known as anonymous classes (or anonymous inner classes). A local class (either named or anonymous) may refer to names in lexically enclosing classes, or read-only variables (marked as final) in the lexically enclosing method.The capturing of final variables enables you to capture variables by value. Even if the variable you want to capture is non-final, you can always copy it to a temporary final variable just before the class.Capturing of variables by reference can be emulated by using a final reference to a mutable container, for example, a single-element array. The local class will not be able to change the value of the container reference itself, but it will be able to change the contents of the container.With the advent of Java 8's lambda expressions,[12] the closure causes the above code to be executed as:Local classes are one of the types of inner class that are declared within the body of a method. Java also supports inner classes that are declared as non-static members of an enclosing class.[13] They are normally referred to just as inner classes.[14] These are defined in the body of the enclosing class and have full access to instance variables of the enclosing class. Due to their binding to these instance variables, an inner class may only be instantiated with an explicit binding to an instance of the enclosing class using a special syntax.[15]Upon execution, this will print the integers from 0 to 9. Beware to not confuse this type of class with the nested class, which is declared in the same way with an accompanied usage of the static modifier; those have not the desired effect but are instead just classes with no special binding defined in an enclosing class.As of Java 8, Java supports functions as first class objects. Lambda expressions of this form are considered of type Function<T,U> with T being the domain and U the image type. The expression can be called with its .apply(T t) method, but not with a standard method call.Blocks (C, C++, Objective-C 2.0)[edit]Apple introduced Blocks, a form of closure, as a nonstandard extension into C, C++, Objective-C 2.0 and in Mac OS X 10.6 Snow Leopard and iOS 4.0. Apple made their implementation available for the GCC and clang compilers.Pointers to block and block literals are marked with ^. Normal local variables are captured by value when the block is created, and are read-only inside the block. Variables to be captured by reference are marked with __block. Blocks that need to persist outside of the scope they are created in may need to be copied.[16][17]Delegates (C#, D)[edit]C# anonymous methods and lambda expressions support closure:In D, closures are implemented by delegates, a function pointer paired with a context pointer (e.g. a class instance, or a stack frame on the heap in the case of closures).D version 1, has limited closure support. For example, the above code will not work correctly, because the variable a is on the stack, and after returning from test(), it is no longer valid to use it (most probably calling foo via dg(), will return a 'random' integer). This can be solved by explicitly allocating the variable 'a' on heap, or using structs or class to store all needed closed variables and construct a delegate from a method implementing the same code. Closures can be passed to other functions, as long as they are only used while the referenced values are still valid (for example calling another function with a closure as a callback parameter), and are useful for writing generic data processing code, so this limitation, in practice, is often not an issue.This limitation was fixed in D version 2 - the variable 'a' will be automatically allocated on the heap because it is used in the inner function, and a delegate of that function can escape the current scope (via assignment to dg or return). Any other local variables (or arguments) that are not referenced by delegates or that are only referenced by delegates that don't escape the current scope, remain on the stack, which is simpler and faster than heap allocation. The same is true for inner's class methods that references a function's variables.Function objects (C++)[edit]C++ enables defining function objects by overloading operator(). These objects behave somewhat like functions in a functional programming language. They may be created at runtime and may contain state, but they do not implicitly capture local variables as closures do. As of the 2011 revision, the C++ language also supports closures, which are a type of function object constructed automatically from a special language construct called lambda-expression. A C++ closure may capture its context either by storing copies of the accessed variables as members of the closure object or by reference. In the latter case, if the closure object escapes the scope of a referenced object, invoking its operator() causes undefined behavior since C++ closures do not extend the lifetime of their context.Inline agents (Eiffel)[edit]Eiffel includes inline agents defining closures. An inline agent is an object representing a routine, defined by giving the code of the routine in-line. For example, inthe argument to subscribe is an agent, representing a procedure with two arguments; the procedure finds the country at the corresponding coordinates and displays it. The whole agent is subscribed to the event type click_event for a certain button, so that whenever an instance of the event type occurs on that button \u2014 because a user has clicked the button \u2014 the procedure will be executed with the mouse coordinates being passed as arguments for x and y.The main limitation of Eiffel agents, which distinguishes them from closures in other languages, is that they cannot reference local variables from the enclosing scope. This design decision helps in avoiding ambiguity when talking about a local variable value in a closure - should it be the latest value of the variable or the value captured when the agent is created? Only Current (a reference to current object, analogous to this in Java), its features, and arguments of the agent itself can be accessed from within the agent body. The values of the outer local variables can be passed by providing additional closed operands to the agent.See also[edit]\nAnonymous function\nBlocks (C language extension)\nCommand pattern\nContinuation\nCurrying\nFunarg problem\nLambda calculus\nLazy evaluation\nPartial application\nSpaghetti stack\nSyntactic closure\nValue-level programming\nNotes[edit]References[edit]External links[edit]\nThe Original Lambda Papers: A classic series of papers by Guy Steele and Gerald Sussman discussing, among other things, the versatility of closures in the context of Scheme (where they appear as lambda expressions).\nNeal Gafter (2007-01-28). A Definition of Closures. \nGilad Bracha, Neal Gafter, James Gosling, Peter von der Ahe\u0301. Closures for the Java Programming Language (v0.5). CS1 maint: Multiple names: authors list (link)\nClosures: An article about closures in dynamically typed imperative languages, by Martin Fowler.\nCollection closure methods: An example of a technical domain where using closures is convenient, by Martin Fowler.\n", "subtitles": ["History and etymology", "Anonymous functions", "Applications", "Implementation and theory", "Differences in semantics", "Closure-like constructs", "See also", "Notes", "References", "External links"], "title": "Closure (computer programming)"},
{"content": "Racket (formerly PLT Scheme) is a general purpose, multi-paradigm programming language in the Lisp-Scheme family. One of its design goals is to serve as a platform for language creation, design, and implementation.[7][8] The language is used in a variety of contexts such as scripting, general-purpose programming, computer science education, and research.The platform provides an implementation of the Racket language (including a sophisticated run-time system,[9] various libraries, JIT compiler, and more) along with a development environment called DrRacket (formerly named DrScheme) written in Racket itself.[10] The IDE and an accompanying programming curriculum is used in the ProgramByDesign outreach program, an attempt to turn computing and programming into an indispensable part of the liberal arts curriculum.[11][12] The core language is known for its extensive macro system which enables the creation of embedded and domain-specific languages, language constructs such as classes or modules, and separate dialects of Racket with different semantics.[13][14][15][16]The platform distribution is free and open-source software distributed under the GNU Lesser General Public License (LGPL) license.[17] Extensions and packages written by the community are uploaded to Racket's centralized package catalog.History[edit]Development[edit]Matthias Felleisen founded PLT in the mid 1990s, first as a research group, soon after as a project dedicated to the production of pedagogic materials for novice programmers (lectures, exercises/projects, software). In January 1995, the group decided to develop a pedagogic programming environment based on Scheme. Matthew Flatt cobbled together MrEd\u2014the original virtual machine for Racket\u2014from libscheme,[18] wxWidgets, and a few other free systems.[19] In the years that followed, a team including Flatt, Robby Findler, Shriram Krishnamurthi, Cormac Flanagan, and many others produced DrScheme, a programming environment for novice Scheme programmers and a research environment for soft typing.[10] The main development language that DrScheme supported was called PLT Scheme.In parallel, the team started conducting workshops for high school teachers, training them in program design and functional programming. Field tests with these teachers and their students provided essential clues for the direction of the development.Over the following years, PLT added teaching languages, an algebraic stepper,[20] a transparent read-eval-print loop, a constructor-based printer, and many other innovations to DrScheme, producing an application-quality pedagogic program development environment. By 2001, the core team (Felleisen, Findler, Flatt, Krishnamurthi) had also written and published their first textbook, How to Design Programs, based on their teaching philosophy.Version history[edit]The first generation of PLT Scheme revisions introduced features for programming in the large with both modules and classes. Version 42 introduced units \u2014 a first-class module system \u2014 to complement classes for large scale development.[21] The class system gained features (e.g. Java-style interfaces) and also lost several features (e.g. multiple inheritance) throughout these versions.[13] The language evolved throughout a number of successive versions, and gaining milestone popularity in Version 53, leading to extensive work and the following Version 100, which would be equivalent to a 1.0 release in current popular version systems.The next major revision was named Version 200, which introduced a new default module system that cooperates with macros.[21] In particular, the module system ensures that run-time and compile-time computation are separated to support a tower of languages.[22] Unlike units, these modules are not first-class objects.Version 300 introduced Unicode support, foreign library support, and refinements to the class system.[21] Later on, the 300 series improved the performance of the language runtime with an addition of a JIT compiler and a switch to a default generational garbage collection.By the next major release, the project had switched to a more conventional sequence-based version numbering. Version 4.0 introduced the #lang shorthand to specify the language that a module is written in. In addition, the revision introduced immutable pairs and lists, support for fine-grained parallelism, and a statically-typed dialect.[23]Rename[edit]On June 7, 2010, PLT Scheme was renamed Racket.[24] The renaming coincided with the release of Version 5.0. Subsequently, the GUI backend was rewritten in Racket from C++ in Version 5.1 using native UI toolkits on all platforms.[19] Version 5.2 included a background syntax checking tool, a new plotting library, a database library, and a new extended REPL.[25] Version 5.3 included a new submodule feature for optionally loaded modules,[26] new optimization tools, a JSON library, and other features.[27] Version 5.3.1 introduced major improvements to DrRacket: the background syntax checker was turned on by default and a new documentation preview tool was added.[28]In version 6.0, Racket rolled out its second-generation package management system. As part of this development, the principal DrRacket and Racket repository was reorganized and split into a large set of small packages, making it possible to install a minimal racket and to install only those packages necessary.[29]Features[edit]Racket's core language includes macros, modules, lexical closures, tail calls, delimited continuations,[30] parameters (fluid variables), software contracts,[31] green and OS threads,[32][33][34] and more. The language also comes with primitives, such as eventspaces and custodians, which control resource management and enables the language to act like an operating system for loading and managing other programs.[9] Further extensions to the language are created with the powerful macro system, which together with the module system and custom parsers can control all aspects of a language.[35] Unlike programming languages that lack macro systems, most language constructs in Racket are written on top of the base language using macros. These include a mixin class system,[13] a component (or module) system as expressive as ML's,[14] and pattern matching.In addition, the language features the first contract system for a higher-order language.[36] Racket's contract system is inspired by the Design by Contract work for Eiffel and extends it to work for higher-order values such as first-class functions, objects, reference cells, and so on. For example, an object that is checked by a contract can be ensured to make contract checks when its methods are eventually invoked.Racket's compiler is a bytecode compiler that translates to an internal bytecode format that is run by the Racket virtual machine. On x86 and PowerPC platforms, the bytecode is further compiled using a JIT compiler at runtime.Since 2004, the language has also shipped with PLaneT, a package manager that is integrated into the module system so that third-party libraries can be transparently imported and used. Additionally, PLaneT has a built-in versioning policy to prevent dependency hell.[37]At the end of 2014, much of Racket's code was moved into a new packaging system separate from the main code base. This new packaging system is serviced by a client program called raco. The new package system provides fewer features than PLaneT; a blog post by Jay McCarthy on the Racket blog explains the rationale for the change and how to duplicate the older system.[38]Macros and extensibility[edit]The feature that distinguishes Racket from other languages in the Lisp family is its integrated language extensibility. Racket's extensibility features are built into the module system to allow context-sensitive and module-level control over syntax.[15] For example, the #%app syntactic form can be overridden to change the semantics of function application. Similarly, the #%module-begin form allows arbitrary static analysis of the entire module.[15] Since any module can be used as a language, via the #lang notation, this effectively means a programmer can control virtually any aspect of the language.The module-level extensibility features are combined with a Scheme-like hygienic macro system, which provides more features than Lisp's S-expression manipulation system,[39][40] Scheme 84's hygienic extend-syntax macros, or R5RS's syntax-rules. Indeed, it is fair to say that the macro system is a carefully tuned application programming interface (API) for compiler extensions. Using this compiler API, programmers can add features and entire domain-specific languages in a manner that makes them completely indistinguishable from built-in language constructs.The macro system in Racket has been used to construct entire language dialects. This includes Typed Racket\u2014a statically typed dialect of Racket that eases the migration from untyped to typed code,[41] and Lazy Racket\u2014a dialect with lazy evaluation.[42] Other dialects include FrTime (functional reactive programming), Scribble (documentation language),[43] Slideshow (presentation language),[44] and several languages for education.[45][46] Racket's core distribution provides libraries to aid the process of constructing new programming languages.[15]Such languages are not restricted to S-expression based syntax. In addition to conventional readtable-based syntax extensions, Racket's #lang makes it possible for a language programmer to define any arbitrary parser, for example, using the parser tools library.[47] See Racket logic programming for an example of such a language.Programming environment[edit]The language platform provides a self-hosted IDE[10] named DrRacket, a continuation-based web server,[48] a graphical user interface,[19] and other tools. Racket is also a viable scripting tool and can be used for scripting the Unix shell. It can parse command line arguments, execute external tools and includes libraries like all common scripting languages.DrRacket IDE[edit]DrRacket (formerly DrScheme) is widely used among introductory Computer Science courses that teach Scheme or Racket and is lauded for its simplicity and appeal to beginner programmers. The IDE was originally built for use with the TeachScheme! project (now ProgramByDesign), an outreach effort by Northeastern University and a number of affiliated universities for attracting high school students to computer science courses at the college level.The editor provides source highlighting for syntax and run-time errors, parenthesis matching, a debugger and an algebraic stepper. Its student-friendly features include support for multiple language levels (Beginning Student, Intermediate Student and so on). It also has integrated library support, and sophisticated analysis tools for advanced programmers. In addition, module-oriented programming is supported with the module browser, a contour view, integrated testing and coverage measurements, and refactoring support. It provides integrated, context-sensitive access to an extensive hyper-linked help system named Help Desk.DrRacket is available for Windows (95 and up), Mac OS X, Unix, and Linux with the X Window System and programs behave similarly on all these platforms.Code examples[edit]Here's a trivial hello world program:Running this program produces the output:\nHello, World!\nHere's a slightly less trivial program:This program, taken from the Racket website, draws a Sierpinski triangle, nested to depth 8.Using the #lang directive, a source file can be written in different dialects of Racket. Here is an example of the factorial program in Typed Racket, a statically typed dialect of Racket:Applications and practical use[edit]Apart from having a basis in programming language theory, Racket was designed to be used as a general-purpose language in production systems. Thus, the Racket distribution features an extensive library that covers systems and network programming, web development,[48] a uniform interface to the underlying operating system, a dynamic foreign function interface,[49] several flavours of regular expressions, lexer/parser generators,[47] logic programming, and a complete GUI framework.Racket has several features useful for a commercial language, among them an ability to generate standalone executables under Windows, Mac OS X and Unix, a profiler and debugger included in the integrated development environment (IDE), and a unit testing framework.Racket has been used for commercial projects and web applications. A notable example is the Hacker News website, which runs on Arc, which is developed in Racket. Naughty Dog has used it as a scripting language in several of their video games.[50]Racket is used to teach students algebra through game design in the Bootstrap program.[51]References[edit]Further reading[edit]\nFelleisen et al., 2013. Realm of Racket. No Starch Press.\nFelleisen et al., 2003. How to Design Programs. MIT Press.\nExternal links[edit]\nOfficial website\nRacket source code repository on GitHub\nPackage management system\nOfficial documentation[edit]\nQuick: An Introduction to Racket with Pictures\nThe Racket Guide\nThe Racket Reference\nHow to Program Racket \u2013 Racket style guide\nMiscellaneous[edit]\nF*dging up a Racket \u2013 shows how to write a Brainfuck dialect for Racket\n", "subtitles": ["History", "Features", "Programming environment", "Code examples", "Applications and practical use", "References", "Further reading", "External links"], "title": "Racket (programming language)"},
{"content": "Pascal is an imperative and procedural programming language, which Niklaus Wirth designed in 1968\u201369 and published in 1970, as a small, efficient language intended to encourage good programming practices using structured programming and data structuring. It is named in honor of the French mathematician, philosopher and physicist Blaise Pascal.Pascal was developed on the pattern of the ALGOL 60 language. Wirth had already developed several improvements to this language as part of the ALGOL X proposals, but these were not accepted and Pascal was developed separately and released in 1970. A derivative known as Object Pascal designed for object-oriented programming was developed in 1985; this was used by Apple Computer and Borland in the late 1980s and later developed into Delphi on the Microsoft Windows platform. Extensions to the Pascal concepts led to the Pascal-like languages Modula-2 and Oberon.History[edit]Earlier efforts[edit]Much of the history of computer language design during the 1960s traces its history to the ALGOL 60 language. ALGOL was developed during the 1950s with the explicit goal to be able to clearly describe algorithms. It included a number of features for structured programming that remain common in languages to this day.Shortly after its introduction, in 1962 Wirth began working on his dissertation with Helmut Weber on the Euler programming language. Euler was based on ALGOL's syntax and many concepts but was not a derivative. Its primary goal was to add dynamic lists and types, allowing it to be used in roles similar to Lisp. The language was published in 1965.By this time, a number of problems in ALGOL had been identified, notably the lack of a standardized string system. The group tasked with maintaining the language had begun the ALGOL X process to identify improvements, calling for submissions. Wirth and Tony Hoare submitted a conservative set of modifications to add strings and clean up some of the syntax. These were considered too minor to be worth using as the new standard ALGOL, so Wirth wrote a compiler for the language, which became known as ALGOL W.The ALGOL X efforts would go on to choose a dramatically more complex language, ALGOL 68. The complexity of this language led to considerable difficulty producing high-performance compilers, and it was not widely used in the industry. This left an opening for newer languages.Pascal[edit]Pascal was influenced by the ALGOL W efforts, with the explicit goals of producing a language that would be efficient both in the compiler and at run-time, allow for the development of well-structured programs, and to be useful for teaching students structured programming.[4] A generation of students used Pascal as an introductory language in undergraduate courses.One of the early successes for language was the introduction of UCSD Pascal, a version that ran on a custom operating system that could be ported to different platforms. A key platform was the Apple II, where it saw widespread use. This led to the use of Pascal becoming the primary high-level language used for development in the Apple Lisa, and later, the Macintosh. Parts of the original Macintosh operating system were hand-translated into Motorola 68000 assembly language from the Pascal sources.[5]The typesetting system TeX by Donald E. Knuth was written in WEB, the original literate programming system, based on DEC PDP-10 Pascal, while applications like Total Commander, Skype and Macromedia Captivate were written in Delphi (Object Pascal). Apollo Computer used Pascal as the systems programming language for its operating systems beginning in 1980.Variants of Pascal have also frequently been used for everything from research projects to PC games and embedded systems. Newer Pascal compilers exist which are widely used.[6]Object Pascal[edit]During work on the Lisa, Larry Tesler began corresponding with Wirth on the idea of adding object oriented extensions to the language. This led initially to Clascal, introduced in 1983. As the Lisa program faded and was replaced by the Mac, a further version known as Object Pascal was created. This was introduced on the Macintosh in 1985 as part of the MacApp application framework, and became Apple's primary development language into the early 1990s.The Object Pascal extensions were later added to Turbo Pascal, and over the years became the Delphi system for Microsoft Windows. Delphi is still used for developing Windows applications, but also has the ability to cross-compile the same code to Mac, iOS, Android and Linux. Another cross-platform version called Free Pascal, with the Lazarus IDE, is popular with Linux users since it also offers write once, compile anywhere development. CodeTyphon is a Lazarus distribution with more preinstalled packages and cross-compilers.Brief description[edit]Wirth's intention was to create an efficient language (regarding both compilation speed and generated code) based on structured programming, a recently popularized concept that he promoted in his book Algorithms + Data Structures = Programs. Pascal has its roots in the ALGOL 60 language, but also introduced concepts and mechanisms which (on top of ALGOL's scalars and arrays) enabled programmers to define their own complex (structured) datatypes, and also made it easier to build dynamic and recursive data structures such as lists, trees and graphs. Important features included for this were records, enumerations, subranges, dynamically allocated variables with associated pointers, and sets. To make this possible and meaningful, Pascal has a strong typing on all objects, which means that one type of data cannot be converted or interpreted as another without explicit conversions. Similar mechanisms are standard in many programming languages today. Other languages that influenced Pascal's development were Simula 67 and Wirth's own ALGOL W.Pascal, like many programming languages of today (but unlike most languages in the C family), allows nested procedure definitions to any level of depth, and also allows most kinds of definitions and declarations inside subroutines (procedures and functions). This enables a very simple and coherent syntax where a complete program is syntactically nearly identical to a single procedure or function (except for the heading, which has one of these three keywords).Implementations[edit]Early Pascal compilers[edit]The first Pascal compiler was designed in Zu\u0308rich for the CDC 6000 series mainframe computer family. Niklaus Wirth reports that a first attempt to implement it in Fortran in 1969 was unsuccessful due to Fortran's inadequacy to express complex data structures. The second attempt was implemented in a C-like language (Scallop by Max Engeli) and then translated by hand (by R. Schild) to Pascal itself for boot-strapping.[7] It was operational by mid-1970. Many Pascal compilers since have been similarly self-hosting, that is, the compiler is itself written in Pascal, and the compiler is usually capable of recompiling itself when new features are added to the language, or when the compiler is to be ported to a new environment. The GNU Pascal compiler is one notable exception, being written in C.The first successful port of the CDC Pascal compiler to another mainframe was completed by Welsh and Quinn at the Queen's University of Belfast (QUB) in 1972. The target was the ICL 1900 series. This compiler, in turn, was the parent of the Pascal compiler for the Information Computer Systems (ICS) Multum minicomputer. The Multum port was developed \u2013 with a view to using Pascal as a systems programming language \u2013 by Findlay, Cupples, Cavouras and Davis, working at the Department of Computing Science in Glasgow University. It is thought that Multum Pascal, which was completed in the summer of 1973, may have been the first 16-bit implementation.A completely new compiler was completed by Welsh et al. at QUB in 1977. It offered a source-language diagnostic feature (incorporating profiling, tracing and type-aware formatted postmortem dumps) that was implemented by Findlay and Watt at Glasgow University. This implementation was ported in 1980 to the ICL 2900 series by a team based at Southampton University and Glasgow University. The Standard Pascal Model Implementation was also based on this compiler, having been adapted, by Welsh and Hay at Manchester University in 1984, to check rigorously for conformity to the BSI 6192/ISO 7185 Standard and to generate code for a portable abstract machine.The first Pascal compiler written in North America was constructed at the University of Illinois under Donald B. Gillies for the PDP-11 and generated native machine code.The Pascal-P system[edit]To propagate the language rapidly, a compiler porting kit was created in Zurich that included a compiler that generated code for a virtual stack machine, i.e., code that lends itself to reasonably efficient interpretation, along with an interpreter for that code \u2013 the Pascal-P system. The P-system compilers were termed Pascal-P1, Pascal-P2, Pascal-P3, and Pascal-P4. Pascal-P1 was the first version, and Pascal-P4 was the last to come from Zurich. The version termed Pascal-P1 was coined after the fact for the many different sources for Pascal-P that existed. The compiler was redesigned to enhance portability, and issued as Pascal-P2. This code was later enhanced to become Pascal-P3, with an intermediate code backward compatible with Pascal-P2, and Pascal-P4, which was not backward compatible.The Pascal-P4 compiler/interpreter can still be run and compiled on systems compatible with original Pascal. However, it only accepts a subset of the Pascal language.Pascal-P5, created outside the Zurich group, accepts the full Pascal language and includes ISO 7185 compatibility.UCSD Pascal branched off Pascal-P2, where Kenneth Bowles utilized it to create the interpretive UCSD p-System. The UCSD p-System was one of three operating systems available at the launch of the original IBM Personal Computer.[8] UCSD Pascal used an intermediate code based on byte values, and thus was one of the earliest byte code compilers. Pascal-P1 through Pascal-P4 was not, but rather based on the CDC 6600 60 bit word length.A compiler based on the Pascal-P4 compiler, which created native binaries, was released for the IBM System/370 mainframe computer by the Australian Atomic Energy Commission; it was called the AAEC Pascal Compiler after the abbreviation of the name of the Commission.[9]In the early 1980s, Watcom Pascal was developed, also for the IBM System 370.Into the 1990s, Pascal was still running on VAX terminals at George Mason University to teach computer programming.Object Pascal and Turbo Pascal[edit]Apple Computer created its own Lisa Pascal for the Lisa Workshop in 1982, and ported the compiler to the Apple Macintosh and MPW in 1985. In 1985 Larry Tesler, in consultation with Niklaus Wirth, defined Object Pascal and these extensions were incorporated in both the Lisa Pascal and Mac Pascal compilers.In the 1980s, Anders Hejlsberg wrote the Blue Label Pascal compiler for the Nascom-2. A reimplementation of this compiler for the IBM PC was marketed under the names Compas Pascal and PolyPascal before it was acquired by Borland and renamed Turbo Pascal.Turbo Pascal became hugely popular, thanks to an aggressive pricing strategy, having one of the first full-screen integrated development environments, and very fast turnaround time (just seconds to compile, link, and run). It was written and highly optimized entirely in assembly language, making it smaller and faster than much of the competition.In 1986, Anders ported Turbo Pascal to the Macintosh and incorporated Apple's Object Pascal extensions into Turbo Pascal. These extensions were then added back into the PC version of Turbo Pascal for version 5.5. At the same time Microsoft also implemented the Object Pascal compiler.[10][11] Turbo Pascal 5.5 had a large influence on the Pascal community, which began concentrating mainly on the IBM PC in the late 1980s. Many PC hobbyists in search of a structured replacement for BASIC used this product. It also began to be adopted by professional developers. Around the same time a number of concepts were imported from C to let Pascal programmers use the C-based API of Microsoft Windows directly. These extensions included null-terminated strings, pointer arithmetic, function pointers, an address-of operator and unsafe typecasts.Turbo Pascal, and other derivatives with units or module concepts are modular languages. However, it does not provide a nested module concept or qualified import and export of specific symbols.Other variants[edit]Super Pascal is a variant that added non-numeric labels, a return statement and expressions as names of types.The universities of Wisconsin-Madison, Zu\u0308rich, Karlsruhe and Wuppertal developed the Pascal-SC[12][13] and Pascal-XSC[14][15][16] (Extensions for Scientific Computation) compilers, aimed at programming numerical computations. TMT Pascal the first Borland-compatible compiler for 32-bit DOS protected mode, OS/2 and Win32 operating systems. Also the TMT Pascal language was the first one which allowed function and operator overloading. Development for Pascal-SC started in 1978 supporting ISO 7185 Pascal level 0, but level 2 support was added at a later stage.[17] Pascal-SC originally targeted the Z80 processor, but was later rewritten for DOS (x86) and 68000. Pascal-XSC has at various times been ported to Unix (Linux, SunOS, HP-UX, AIX) and Microsoft/IBM (DOS with EMX, OS/2, Windows) operating systems. It operates by generating intermediate C source code which is then compiled to a native executable. Some of the Pascal-SC language extensions have been adopted by GNU Pascal.Pascal Sol was designed around 1983 by a French team to implement a Unix-like systems named Sol. It was standard Pascal level-1 (with parametrized array bounds) but the definition allowed alternative keywords and predefined identifiers in French and the language included a few extensions to ease system programming (e.g. an equivalent to lseek).[18] The Sol team later on moved to the ChorusOS project to design a distributed operating system.[19]IP Pascal was an implementation of the Pascal programming language using Micropolis DOS, but was moved rapidly to CP/M-80 running on the Z80. It was moved to the 80386 machine types in 1994, and exists today as Windows/XP and Linux implementations. In 2008, the system was brought up to a new level and the resulting language termed Pascaline (after Pascal's calculator). It includes objects, namespace controls, dynamic arrays, along with many other extensions, and generally features the same functionality and type protection as C#. It is the only such implementation that is also compatible with the original Pascal implementation, which is standardized as ISO 7185.Smart Mobile Studio[20] was created by Jon Aasenden and compiles his own dialect of Object Pascal to HTML5/JavascriptSmart Mobile Studio has an IDE which includes a visual component set, its language is unusual in that it incorporates extensions for the Javascript languageLanguage constructs[edit]Pascal, in its original form, is a purely procedural language and includes the traditional array of ALGOL-like control structures with reserved words such as if, then, else, while, for, and case ranging on a single statement or a begin-end statements block. Pascal also has data structuring constructs not included in the original ALGOL 60 types, like records, variants, pointers, enumerations, and sets and procedure/pointers. Such constructs were in part inherited or inspired from Simula 67, ALGOL 68, Niklaus Wirth's own ALGOL W and suggestions by C. A. R. Hoare.Pascal programs start with the program keyword with a list of external file descriptors as parameters[21] (not required in Turbo Pascal etc.); then follows the main block bracketed by the begin and end keywords. Semicolons separate statements, and the full stop (i.e., a period) ends the whole program (or unit). Letter case is ignored in Pascal source.Here is an example of the source code in use for a very simple Hello world program:Data types[edit]A type in Pascal, and in several other popular programming languages, defines a variable in such a way that it defines a range of values which the variable is capable of storing, and it also defines a set of operations that are permissible to be performed on variables of that type. The predefined types are:The range of values allowed for each (except boolean) is implementation defined. Functions are provided for some data conversions. For conversion of real to integer, the following functions are available: round (which rounds to integer using banker's rounding) and trunc (rounds towards zero).The programmer has the freedom to define other commonly used data types (e.g. byte, string, etc.) in terms of the predefined types using Pascal's type declaration facility, for example(Often-used types like byte and string are already defined in many implementations.)Subrange types[edit]Subranges of any ordinal data type (any simple type except real) can also be made:Set types[edit]In contrast with other programming languages from its time, Pascal supports a set type:A set is a fundamental concept for modern mathematics, and they may be used in many algorithms. Such a feature is useful and may be faster than an equivalent construct in a language that does not support sets. For example, for many Pascal compilers:executes faster than:Sets of non-contiguous values can be particularly useful, in terms of both performance and readability:For these examples, which involve sets over small domains, the improved performance is usually achieved by the compiler representing set variables as bit vectors. The set operators can then be implemented efficiently as bitwise machine code operations.Type declarations[edit]Types can be defined from other types using type declarations:Further, complex types can be constructed from simple types:File type[edit]As shown in the example above, Pascal files are sequences of components. Every file has a buffer variable which is denoted by f^. The procedures get (for reading) and put (for writing) move the buffer variable to the next element. Read is introduced such that read(f, x) is the same as x := f^; get(f);. Write is introduced such that write(f, x) is the same as f^ := x; put(f); The type text is predefined as file of char. While the buffer variable could be used for inspecting the next character to be used (check for a digit before reading an integer), this leads to serious problems with interactive programs in early implementations, but was solved later with the lazy I/O concept.In Jensen & Wirth Pascal, strings are represented as packed arrays of chars; they therefore have fixed length and are usually space-padded.Pointer types[edit]Pascal supports the use of pointers:Here the variable NodePtr is a pointer to the data type Node, a record. Pointers can be used before they are declared. This is a forward declaration, an exception to the rule that things must be declared before they are used.To create a new record and assign the value 10 and character A to the fields a and b in the record, and to initialise the pointer c to NIL, the commands would be:This could also be done using the with statement, as follows:Inside of the scope of the with statement, a and b refer to the subfields of the record pointer NodePtr and not to the record Node or the pointer type pNode.Linked lists, stacks and queues can be created by including a pointer type field (c) in the record (see also NIL).Unlike many languages that feature pointers, Pascal only allows pointers to reference dynamically created variables that are anonymous, and does not allow them to reference standard static or local variables. Pointers also must have an associated type, and a pointer to one type is not compatible with a pointer to another type (e.g. a pointer to a char is not compatible with a pointer to an integer). This helps eliminate the type security issues inherent with other pointer implementations, particularly those used for PL/I or C. It also removes some risks caused by dangling pointers, but the ability to dynamically deallocate referenced space by using the dispose function (which has the same effect as the free library function found in C) means that the risk of dangling pointers has not been entirely eliminated[22] as it has in languages such as Java and C#, which provide automatic garbage collection (but which do not entirely eliminate the related problem of memory leaks).Some of these restrictions can be lifted in newer dialects.Control structures[edit]Pascal is a structured programming language, meaning that the flow of control is structured into standard statements, usually without 'goto' commands.Procedures and functions[edit]Pascal structures programs into procedures and functions.Procedures and functions can nest to any depth, and the 'program' construct is the logical outermost block.Each procedure or function can have its own declarations of goto labels, constants, types, variables, and other procedures and functions, which must all be in that order. This ordering requirement was originally intended to allow efficient single-pass compilation. However, in some dialects (such as Embarcadero Delphi) the strict ordering requirement of declaration sections has been relaxed.Semicolons as statement separators[edit]Pascal adopted many language syntax features from the ALGOL language, including the use of a semicolon as a statement separator. This is in contrast to other languages, such as PL/I, C etc. which use the semicolon as a statement terminator. As illustrated in the above examples, no semicolon is needed before the end keyword of a record type declaration, a block, or a case statement; before the until keyword of a repeat statement; and before the else keyword of an if statement.The presence of an extra semicolon was not permitted in early versions of Pascal. However, the addition of ALGOL-like empty statements in the 1973 Revised Report and later changes to the language in ISO 7185:1983 now allow for optional semicolons in most of these cases. A semicolon is still not permitted immediately before the else keyword in an if statement, because the else follows a single statement, not a statement sequence. In the case of nested ifs, a semicolon cannot be used to avoid the dangling else problem (where the inner if does not have an else, but the outer if does) by putatively terminating the nested if with a semicolon \u2013 this instead terminates both if clauses. Instead, an explicit begin...end block must be used.[23]Programmers usually include these extra semicolons out of habit and to avoid changing the last line of a statement sequence when new code is appended.Resources[edit]Compilers and interpreters[edit]Several Pascal compilers and interpreters are available for general use:\nDelphi is Embarcadero's (formerly Borland/CodeGear) flagship rapid application development (RAD) product. It uses the Object Pascal language (termed 'Delphi' by Borland), descended from Pascal, to create applications for Windows, macOS, iOS, and Android. The .NET support that existed from D8 through D2005, D2006 and D2007 has been terminated, and replaced by a new language (Prism, which is rebranded Oxygene, see below) that is not fully backward compatible. In recent years Unicode support and generics were added (D2009, D2010, Delphi XE).\nFree Pascal is a multi-platform compiler written in Object Pascal (and is self-hosting). It is aimed at providing a convenient and powerful compiler, both able to compile legacy applications and to be the means of developing new ones. It is distributed under the GNU GPL, while packages and runtime library come under a modified GNU LGPL. Apart from compatibility modes for Turbo Pascal, Delphi and Mac Pascal, it also has its own procedural and object-oriented syntax modes with support for extended features such as operator overloading. It supports many platforms and operating systems. Current versions also feature an ISO mode.\nModern Pascal is a multi-platform interpreter and p-code compiler written in Free Pascal. It is aimed at providing alternative solutions for PHP and node.js, using either an ISO standard pascal dialect or a hybrid supporting JavaScript/C operators. From the CLI it is useful as a Free Pascal interpreter.\nTurbo51 is a free Pascal compiler for the 8051 family of microcontrollers, with Turbo Pascal 7 syntax.\nOxygene (formerly known as Chrome) is an Object Pascal compiler for the .NET and Mono platforms. It was created and is sold by RemObjects Software, and sold for a while by Embarcadero as the backend compiler of Prism.\nKylix was a descendant of Delphi, with support for the Linux operating system and an improved object library. It is no longer supported. Compiler and IDE are available now for non-commercial use.\nGNU Pascal Compiler (GPC) is the Pascal compiler of the GNU Compiler Collection (GCC). The compiler itself is written in C, the runtime library mostly in Pascal. Distributed under the GNU General Public License, it runs on many platforms and operating systems. It supports the ANSI/ISO standard languages and has partial Turbo Pascal dialect support. One of the more painful omissions is the absence of a 100% Turbo Pascal-compatible (short)string type. Support for Borland Delphi and other language variations is quite limited. There is some support for Mac-pascal however.\nDWScript aka DelphiWebScript, is an interpreter created by Matthias Ackermann and Hannes Hernler in 2000. Current version runs a dialect of Object Pascal largely compatible with Delphi, but also supports language constructs elements introduced in Prism. DWScript code can be embedded into Delphi applications similar to PascalScript, compiled into standalone application using SimpleMobileStudio or compiled into JavaScript code and placed on a web page.[24]\nDr. Pascal is an interpreter that runs Standard Pascal. Notable are the visible execution mode that shows a running program and its variables, and the extensive runtime error checking. Runs programs but does not emit a separate executable binary. Runs on DOS, Windows in DOS window, and old Macintosh.\nDr. Pascal's Extended Pascal Compiler tested on DOS, Windows 3.1, 95, 98, NT.\nVirtual Pascal was created by Vitaly Miryanov in 1995 as a native OS/2 compiler compatible with Borland Pascal syntax. Then, it had been commercially developed by fPrint, adding Win32 support, and in 2000 it became freeware. Today it can compile for Win32, OS/2 and Linux, and is mostly compatible with Borland Pascal and Delphi. Development was canceled on April 4, 2005.\nP4 compiler, the basis for many subsequent Pascal-implemented-in-Pascal compilers. It implements a subset of full Pascal.\nP5 compiler, is an ISO 7185 (full Pascal) adaption of P4.\nSmart Mobile Studio is a Pascal to HTML5/Javascript compiler\nTurbo Pascal was the dominant Pascal compiler for PCs during the 1980s and early 1990s, popular both because of its powerful extensions and extremely short compilation times. Turbo Pascal was compactly written and could compile, run, and debug all from memory without accessing disk. Slow floppy disk drives were common for programmers at the time, further magnifying Turbo Pascal's speed advantage. Currently, older versions of Turbo Pascal (up to 5.5) are available for free download from Borland's site.\nIP Pascal Implements the language Pascaline (named after Pascal's calculator), which is a highly extended Pascal compatible with original Pascal according to ISO 7185. It features modules with namespace control, including parallel tasking modules with semaphores, objects, dynamic arrays of any dimensions that are allocated at runtime, overloads, overrides, and many other extensions. IP Pascal has a built-in portability library that is custom tailored to the Pascal language. For example, a standard text output application from 1970's original Pascal can be recompiled to work in a window and even have graphical constructs added.\nPascal-XT was created by Siemens for their mainframe operating systems BS2000 and SINIX.\nPocketStudio is a Pascal subset compiler and RAD tool for Palm OS and MC68xxx processors with some own extensions to assist interfacing with the Palm OS API. It resembles Delphi and Lazarus with a visual form designer, an object inspector and a source code editor.\nMIDletPascal \u2013 A Pascal compiler and IDE that generates small and fast Java bytecode specifically designed to create software for mobiles\nVector Pascal Vector Pascal is a language for SIMD instruction sets such as the MMX and the AMD 3d Now, supporting all Intel and AMD processors, and Sony's PlayStation 2 Emotion Engine.\nMorfik Pascal allows the development of Web applications entirely written in Object Pascal (both server and browser side).\nWDSibyl \u2013 Visual Development Environment and Pascal compiler for Win32 and OS/2\nPP Compiler, a compiler for Palm OS that runs directly on the handheld computer.\nCDC 6000 Pascal compiler is the source code for the first (CDC 6000) Pascal compiler.\nPascal-S[25]\nAmigaPascal is a free Pascal compiler for the Amiga computer.\nA very extensive list can be found on Pascaland. The site is in French, but it is basically a list with URLs to compilers; there is little barrier for non-Francophones. The site, Pascal Central, a Mac centric Pascal info and advocacy site with a rich collection of article archives, plus links to many compilers and tutorials, may also be of interest.IDEs[edit]\nDev-Pascal is a Pascal IDE that was designed in Borland Delphi and which supports Free Pascal and GNU Pascal as backends.\nLazarus is a free Delphi-like visual cross-platform IDE for rapid application development (RAD). Based on Free Pascal, Lazarus is available for numerous platforms including Linux, FreeBSD, macOS and Microsoft Windows.\nLibraries[edit]\nWOL Library for creating GUI applications with the Free Pascal Compiler.\nStandards[edit]ISO/IEC 7185:1990 Pascal[edit]In 1983, the language was standardized, in the international standard IEC/ISO 7185,[26] and several local country specific standards, including the American ANSI/IEEE770X3.97-1983, and ISO 7185:1983. These two standards differed only in that the ISO standard included a level 1 extension for conformant arrays (an array where the boundaries of the array are not known until run time), where ANSI did not allow for this extension to the original (Wirth version) language. In 1989, ISO 7185 was revised (ISO 7185:1990) to correct various errors and ambiguities found in the original document.The ISO 7185 was stated to be a clarification of Wirth's 1974 language as detailed by the User Manual and Report [Jensen and Wirth], but was also notable for adding Conformant Array Parameters as a level 1 to the standard, level 0 being Pascal without conformant arrays. This addition was made at the request of C. A. R. Hoare, and with the approval of Niklaus Wirth. The precipitating cause was that Hoare wanted to create a Pascal version of the (NAG) Numerical Algorithms Library, which had originally been written in FORTRAN, and found that it was not possible to do so without an extension that would allow array parameters of varying size. Similar considerations motivated the inclusion in ISO 7185 of the facility to specify the parameter types of procedural and functional parameters.Note that Niklaus Wirth himself referred to the 1974 language as the Standard, for example, to differentiate it from the machine specific features of the CDC 6000 compiler. This language was documented in The Pascal Report,[27] the second part of the Pascal users manual and report.On the large machines (mainframes and minicomputers) Pascal originated on, the standards were generally followed. On the IBM PC, they were not. On IBM PCs, the Borland standards Turbo Pascal and Delphi have the greatest number of users. Thus, it is typically important to understand whether a particular implementation corresponds to the original Pascal language, or a Borland dialect of it.The IBM PC versions of the language began to differ with the advent of UCSD Pascal, an interpreted implementation that featured several extensions to the language, along with several omissions and changes. Many UCSD language features survive today, including in Borland's dialect.ISO/IEC 10206:1990 Extended Pascal[edit]In 1990, an extended Pascal standard was created as ISO/IEC 10206,[28] which is identical in technical content[29] to IEEE/ANSI 770X3.160-1989[30]Variations[edit]Niklaus Wirth's Zurich version of Pascal was issued outside ETH in two basic forms, the CDC 6000 compiler source, and a porting kit called Pascal-P system. The Pascal-P compiler left out several features of the full language. For example, procedures and functions used as parameters, undiscriminated variant records, packing, dispose, interprocedural gotos and other features of the full compiler were omitted.UCSD Pascal, under Professor Kenneth Bowles, was based on the Pascal-P2 kit, and consequently shared several of the Pascal-P language restrictions. UCSD Pascal was later adopted as Apple Pascal, and continued through several versions there. Although UCSD Pascal actually expanded the subset Pascal in the Pascal-P kit by adding back standard Pascal constructs, it was still not a complete standard installation of Pascal.In the early 1990s, Alan Burns and Geoff Davies developed Pascal-FC, an extension to Pl/0 (from the Niklaus' book 'Algorithms+Data Structures=Programs'). Several constructs were added to use Pascal-FC as a teaching tool for Concurrent Programming (such as semaphores, monitors, channels, remote-invocation and resources). To be able to demonstrate concurrency, the compiler output (a kind of P-code) could then be executed on a virtual machine. This virtual machine not only simulated a normal \u2013 fair \u2013 environment, but could also simulate extreme conditions (unfair mode).Borland-like Pascal compilers[edit]Borland's Turbo Pascal, written by Anders Hejlsberg, was written in assembly language independent of UCSD or the Zurich compilers. However, it adopted much of the same subset and extensions as the UCSD compiler. This is probably because the UCSD system was the most common Pascal system suitable for developing applications on the resource-limited microprocessor systems available at that time.The shrink-wrapped Turbo Pascal version 3 and later incarnations, including Borland's Object Pascal and Delphi and non-Borland near-compatibles became popular with programmers including shareware authors, and so the SWAG library of Pascal code features a large amount of code written with such versions as Delphi in mind.Software products (compilers, and Interactive/Rapid Development Environments with compilers, in this category:\nTurbo Pascal - TURBO.EXE up to version 7, and Turbo Pascal for Windows (TPW) and Turbo Pascal for Macintosh.\nBorland Pascal 7 (essentially Turbo Pascal 7 for Windows).\nObject Pascal - an extension of the Pascal language that was developed at Apple Computer by a team led by Larry Tesler in consultation with Niklaus Wirth, the inventor of Pascal; its features were added to Borland's Turbo Pascal for Macintosh and in 1989 for Turbo Pascal 5.5 for DOS.\nDelphi - Object Pascal is essentially its underlying language.\nFree Pascal (or fpc) - Free Pascal adopted the de facto standard dialect of Pascal programmers, Borland Pascal and, later, Delphi.\nPascalABC.NET - is a new generation Pascal programming language including compiler and integrated development environment (IDE)\nBorland Kylix is a compiler and integrated development environment (IDE) formerly sold by Borland, but later discontinued. It is a Linux version of the Borland Delphi software development environment and C++Builder.\nLazarus - similar to Kylix in function, is a free cross-platform visual integrated development environment (IDE) for rapid application development (RAD) using the Free Pascal compiler, which supports dialects of Object Pascal, to varying degrees.\nVirtual Pascal - VP2/1 is a fully Borland Pascal and Borland Delphi compatible 32-bit Pascal compiler for OS/2 and Win 32 (with a Linux version on the way).[31]\nSybil is an open source Delphi-like IDE and compiler; implementations include WDSibyl for Microsoft Windows and OS/2, a commercial Borland Pascal compatible environment released by a company called Speedsoft that was later developed into a Delphi like RAD environment called Sybil and then open sourced under the GPL when that company closed down; Open Sybil is an ongoing project, an Open source Pascal RAD (Rapid Application Development) Tool for OS/2 and eCS that was originally based on Speedsoft's WDsybl SPCC (Sibyl Portable Component Classes) and SVDE (Sibyl Visual Development Tool) sources but now the core is SOM, WPS and OpenDoc.[32]\nList of related standards[edit]\nISO 8651-2:1988 Information processing systems \u2013 Computer graphics \u2013 Graphical Kernel System (GKS) language bindings \u2013 Part 2: Pascal\nReception[edit]Pascal generated a wide variety of responses in the computing community, both critical and complimentary.Early criticism[edit]While very popular in the 1980s and early 1990s, implementations of Pascal that closely followed Wirth's initial definition of the language were widely criticized for being unsuitable for use outside teaching. Brian Kernighan, who popularized the C language, outlined his most notable criticisms of Pascal as early as 1981, in his paper Why Pascal Is Not My Favorite Programming Language.[33] The most serious problem described in his article was that array sizes and string lengths were part of the type, so it was not possible to write a function that would accept variable length arrays or even strings as parameters. This made it unfeasible to write, for example, a sorting library. The author also criticized the unpredictable order of evaluation of boolean expressions, poor library support, and lack of static variables, and raised a number of smaller issues. Also, he stated that the language did not provide any simple constructs to escape (knowingly and forcibly ignore) restrictions and limitations. More general complaints from other sources[22][34] noted that the scope of declarations was not clearly defined in the original language definition, which sometimes had serious consequences when using forward declarations to define pointer types, or when record declarations led to mutual recursion, or when an identifier may or may not have been used in an enumeration list. Another difficulty was that, like ALGOL 60, the language did not allow procedures or functions passed as parameters to predefine the expected type of their parameters.Most of Kernighan's criticisms were directly addressed in the paper The Pascal Programming Language,[35] specifically, under Myth 6: Pascal is Not For Serious Programmers.[36]Despite initial criticisms, Pascal continued to evolve, and most of Kernighan's points do not apply to versions of the language which were enhanced to be suitable for commercial product development, such as Borland's Turbo Pascal. As Kernighan predicted in his article, most of the extensions to fix these issues were incompatible from compiler to compiler. Since the early 1990s, however, most of the varieties seem condensed into two categories, ISO and Borland-like.Extended Pascal addresses many of these early criticisms. It supports variable-length strings, variable initialization, separate compilation, short-circuit boolean operators, and default (OTHERWISE) clauses for case statements.[37]See also[edit]\nConcurrent Pascal\nComparison of Pascal and Borland Delphi\nComparison of Pascal and C\nModula-2\nOberon (programming language)\nObject Pascal\nReal Programmers Don't Use Pascal\nReferences[edit]Further reading[edit]\nNiklaus Wirth: The Programming Language Pascal. 35\u201363, Acta Informatica, Volume 1, 1971.\nC. A. R. Hoare: Notes on data structuring. In O-J Dahl, E W Dijkstra and C A R Hoare, editors, Structured Programming, pages 83\u2013174. Academic Press, 1972.\nC. A. R. Hoare, Niklaus Wirth: An Axiomatic Definition of the Programming Language Pascal. 335\u2013355, Acta Informatica, Volume 2, 1973.\nKathleen Jensen and Niklaus Wirth: PASCAL \u2013 User Manual and Report. Springer-Verlag, 1974, 1985, 1991, ISBN 0-387-97649-3 and ISBN 3-540-97649-3.\nNiklaus Wirth: Algorithms + Data Structures = Programs. Prentice-Hall, 1975, ISBN 0-13-022418-9.\nNiklaus Wirth: An assessment of the programming language PASCAL. 23\u201330 ACM SIGPLAN Notices Volume 10, Issue 6, June 1975.\nN. Wirth, and A. I. Wasserman, ed: Programming Language Design. IEEE Computer Society Press, 1980\nD. W. Barron (Ed.): Pascal \u2013 The Language and its Implementation. John Wiley 1981, ISBN 0-471-27835-1\nPeter Grogono: Programming in Pascal, Revised Edition, Addison-Wesley, 1980\nRichard S. Forsyth: Pascal in Work and Play, Chapman and Hall, 1982\nN. Wirth, M. Broy, ed, and E. Denert, ed: Pascal and its Successors in Software Pioneers: Contributions to Software Engineering. Springer-Verlag, 2002, ISBN 3-540-43081-4\nN. Wirth: Recollections about the Development of Pascal. ACM SIGPLAN Notices, Volume 28, No 3, March 1993.\nExternal links[edit]\nThe Pascal Programming Language\nStandard Pascal \u2013 resources and history of original, standard Pascal\nFree Pascal SciTech portal \u2013 with applications of Lazarus and Free Pascal for Science, medicine and technology\nPascal-P \u2013 the Pascal-P compiler and versions\nPascal-P5 \u2013 Pascal-P5 web page\nPascal-P5 source code \u2013 SourceForge project for P5\nOnline Vintage Pascal8000 Compiler \u2013 for small experiments\n", "subtitles": ["History", "Brief description", "Implementations", "Language constructs", "Resources", "Standards", "Reception", "See also", "References", "Further reading", "External links"], "title": "Pascal (programming language)"},
{"content": "In computer programming, a variable or scalar is a storage location (identified by a memory address) paired with an associated symbolic name (an identifier), which contains some known or unknown quantity of information referred to as a value. The variable name is the usual way to reference the stored value, in addition to referring to the variable itself, depending on the context. This separation of name and content allows the name to be used independently of the exact information it represents. The identifier in computer source code can be bound to a value during run time, and the value of the variable may thus change during the course of program execution.[1][2]Variables in programming may not directly correspond to the concept of variables in mathematics. The latter is abstract, having no reference to a physical object such as storage location. The value of a computing variable is not necessarily part of an equation or formula as in mathematics. Variables in computer programming are frequently given long names to make them relatively descriptive of their use, whereas variables in mathematics often have terse, one- or two-character names for brevity in transcription and manipulation.A variable's storage location may be referred by several different identifiers, a situation known as aliasing. Assigning a value to the variable using one of the identifiers will change the value that can be accessed through the other identifiers.Compilers have to replace variables' symbolic names with the actual locations of the data. While a variable's name, type, and location often remain fixed, the data stored in the location may be changed during program execution.Actions on a variable[edit]In imperative programming languages, values can generally be accessed or changed at any time. In pure functional and logic languages, variables are bound to expressions and keep a single value during their entire lifetime due to the requirements of referential transparency. In imperative languages, the same behavior is exhibited by (named) constants (symbolic constants), which are typically contrasted with (normal) variables.Depending on the type system of a programming language, variables may only be able to store a specified datatype (e.g. integer or string). Alternatively, a datatype may be associated only with the current value, allowing a single variable to store anything supported by the programming language.Identifiers referencing a variable[edit]An identifier referencing a variable can be used to access the variable in order to read out the value, or alter the value, or edit other attributes of the variable, such as access permission, locks, semaphores, etc.For instance, a variable might be referenced by the identifier total_count and the variable can contain the number 1956. If the same variable is referenced by the identifier x as well, and if using this identifier x, the value of the variable is altered to 2009, then reading the value using the identifier total_count will yield a result of 2009 and not 1956.If a variable is only referenced by a single identifier that can simply be called the name of the variable. Otherwise, we can speak of one of the names of the variable. For instance, in the previous example, the total_count is a name of the variable in question, and x is another name of the same variable.Scope and extent[edit]The scope of a variable describes where in a program's text the variable may be used, while the extent (or lifetime) describes when in a program's execution a variable has a (meaningful) value. The scope of a variable is actually a property of the name of the variable, and the extent is a property of the variable itself. These should not be confused with context (also called environment), which is a property of the program, and varies by point in the source code or execution \u2013 see scope: overview. Further, object lifetime may coincide with variable lifetime, but in many cases is not tied to variable lifetime.A variable name's scope affects its extent.Scope is an important part of the name resolution of a variable. Most languages define a specific scope for each variable (as well as any other named entity), which may differ within a given program. The scope of a variable is the portion of the program code for which the variable's name has meaning and for which the variable is said to be visible. Entrance into that scope typically begins a variable's lifetime (as it comes into context) and exit from that scope typically ends its lifetime (as it goes out of context). For instance, a variable with lexical scope is meaningful only within a certain function/subroutine, or more finely within a block of expressions/statements (accordingly with function scope or block scope); this is static resolution, performable at parse-time or compile-time. Alternatively, a variable with dynamic scope is resolved at run-time, based on a global binding stack that depends on the specific control flow. Variables only accessible within a certain functions are termed local variables. A global variable, or one with indefinite scope, may be referred to anywhere in the program.Extent, on the other hand, is a runtime (dynamic) aspect of a variable. Each binding of a variable to a value can have its own extent at runtime. The extent of the binding is the portion of the program's execution time during which the variable continues to refer to the same value or memory location. A running program may enter and leave a given extent many times, as in the case of a closure.Unless the programming language features garbage collection, a variable whose extent permanently outlasts its scope can result in a memory leak, whereby the memory allocated for the variable can never be freed since the variable which would be used to reference it for deallocation purposes is no longer accessible. However, it can be permissible for a variable binding to extend beyond its scope, as occurs in Lisp closures and C static local variables; when execution passes back into the variable's scope, the variable may once again be used. A variable whose scope begins before its extent does is said to be uninitialized and often has an undefined, arbitrary value if accessed (see wild pointer), since it has yet to be explicitly given a particular value. A variable whose extent ends before its scope may become a dangling pointer and deemed uninitialized once more since its value has been destroyed. Variables described by the previous two cases may be said to be out of extent or unbound. In many languages, it is an error to try to use the value of a variable when it is out of extent. In other languages, doing so may yield unpredictable results. Such a variable may, however, be assigned a new value, which gives it a new extent.For space efficiency, a memory space needed for a variable may be allocated only when the variable is first used and freed when it is no longer needed. A variable is only needed when it is in scope, thus beginning each variable's lifetime when it enters scope may give space to unused variables. To avoid wasting such space, compilers often warn programmers if a variable is declared but not used.It is considered good programming practice to make the scope of variables as narrow as feasible so that different parts of a program do not accidentally interact with each other by modifying each other's variables. Doing so also prevents action at a distance. Common techniques for doing so are to have different sections of a program use different name spaces, or to make individual variables private through either dynamic variable scoping or lexical variable scoping.Many programming languages employ a reserved value (often named null or nil) to indicate an invalid or uninitialized variable.Typing[edit]In statically typed languages such as Java or ML, a variable also has a type, meaning that only certain kinds of values can be stored in it. For example, a variable of type integer is prohibited from storing text values.In dynamically typed languages such as Python, it is values, not variables, which carry type. In Common Lisp, both situations exist simultaneously: A variable is given a type (if undeclared, it is assumed to be T, the universal supertype) which exists at compile time. Values also have types, which can be checked and queried at runtime.Typing of variables also allows polymorphisms to be resolved at compile time. However, this is different from the polymorphism used in object-oriented function calls (referred to as virtual functions in C++) which resolves the call based on the value type as opposed to the supertypes the variable is allowed to have.Variables often store simple data, like integers and literal strings, but some programming languages allow a variable to store values of other datatypes as well. Such languages may also enable functions to be parametric polymorphic. These functions operate like variables to represent data of multiple types. For example, a function named length may determine the length of a list. Such a length function may be parametric polymorphic by including a type variable in its type signature, since the amount of elements in the list is independent of the elements' types.Parameters[edit]The formal parameters (or formal arguments) of functions are also referred to as variables. For instance, in this Python code segment,the variable named x is a parameter because it is given a value when the function is called. The integer 5 is the argument which gives x its value. In most languages, function parameters have local scope. This specific variable named x can only be referred to within the addtwo function (though of course other functions can also have variables called x).Memory allocation[edit]The specifics of variable allocation and the representation of their values vary widely, both among programming languages and among implementations of a given language. Many language implementations allocate space for local variables, whose extent lasts for a single function call on the call stack, and whose memory is automatically reclaimed when the function returns. More generally, in name binding, the name of a variable is bound to the address of some particular block (contiguous sequence) of bytes in memory, and operations on the variable manipulate that block. Referencing is more common for variables whose values have large or unknown sizes when the code is compiled. Such variables reference the location of the value instead of storing the value itself, which is allocated from a pool of memory called the heap.Bound variables have values. A value, however, is an abstraction, an idea; in implementation, a value is represented by some data object, which is stored somewhere in computer memory. The program, or the runtime environment, must set aside memory for each data object and, since memory is finite, ensure that this memory is yielded for reuse when the object is no longer needed to represent some variable's value.Objects allocated from the heap must be reclaimed\u2014especially when the objects are no longer needed. In a garbage-collected language (such as C#, Java, Python, Golang and Lisp), the runtime environment automatically reclaims objects when extant variables can no longer refer to them. In non-garbage-collected languages, such as C, the program (and the programmer) must explicitly allocate memory, and then later free it, to reclaim its memory. Failure to do so leads to memory leaks, in which the heap is depleted as the program runs, risks eventual failure from exhausting available memory.When a variable refers to a data structure created dynamically, some of its components may be only indirectly accessed through the variable. In such circumstances, garbage collectors (or analogous program features in languages that lack garbage collectors) must deal with a case where only a portion of the memory reachable from the variable needs to be reclaimed.Naming conventions[edit]Unlike their mathematical counterparts, programming variables and constants commonly take multiple-character names, e.g. COST or total. Single-character names are most commonly used only for auxiliary variables; for instance, i, j, k for array index variables.Some naming conventions are enforced at the language level as part of the language syntax and involve the format of valid identifiers. In almost all languages, variable names cannot start with a digit (0\u20139) and cannot contain whitespace characters. Whether, which, and when punctuation marks are permitted in variable names varies from language to language; many languages only permit the underscore (_) in variable names and forbid all other punctuation. In some programming languages, specific (often punctuation) characters (known as sigils) are prefixed or appended to variable identifiers to indicate the variable's type.Case-sensitivity of variable names also varies between languages and some languages require the use of a certain case in naming certain entities;[note 1] Most modern languages are case-sensitive; some older languages are not. Some languages reserve certain forms of variable names for their own internal use; in many languages, names beginning with two underscores (__) often fall under this category.However, beyond the basic restrictions imposed by a language, the naming of variables is largely a matter of style. At the machine code level, variable names are not used, so the exact names chosen do not matter to the computer. Thus names of variables identify them, for the rest they are just a tool for programmers to make programs easier to write and understand. Using poorly chosen variable names can make code more difficult to review than non-descriptive names, so names which are clear are often encouraged.[3][4]Programmers often create and adhere to code style guidelines which offer guidance on naming variables or impose a precise naming scheme. Shorter names are faster to type but are less descriptive; longer names often make programs easier to read and the purpose of variables easier to understand. However, extreme verbosity in variable names can also lead to less comprehensible code.See also[edit]\nControl variable (programming)\nNon-local variable\nTemporary variable\nVariable interpolation\nNotes[edit]References[edit]", "subtitles": ["Actions on a variable", "Identifiers referencing a variable", "Scope and extent", "Typing", "Parameters", "Memory allocation", "Naming conventions", "See also", "Notes", "References"], "title": "Variable (computer science)"},
{"content": "PL/I (Programming Language One, pronounced /pi\u02d0 \u025bl w\u028cn/) is a procedural, imperative computer programming language designed for scientific, engineering, business and system programming uses. It has been used by various academic, commercial and industrial organizations since it was introduced in the 1960s, and continues to be actively used.[2]PL/I's main domains are data processing, numerical computation, scientific computing, and system programming; it supports recursion, structured programming, linked data structure handling, fixed-point, floating-point, complex, character string handling, and bit string handling. The language syntax is English-like and suited for describing complex data formats, with a wide set of functions available to verify and manipulate them.Early history[edit]In the 1950s and early 1960s, business and scientific users programmed for different computer hardware using different programming languages. Business users were moving from Autocoders via COMTRAN to COBOL, while scientific users programmed in General Interpretive Programme (GIP), Fortran, ALGOL, GEORGE, and others. The IBM System/360[3] (announced in 1964 and delivered in 1966) was designed as a common machine architecture for both groups of users, superseding all existing IBM architectures. Similarly, IBM wanted a single programming language for all users. It hoped that Fortran could be extended to include the features needed by commercial programmers. In October 1963 a committee was formed[4] composed originally of three IBMers from New York and three members of SHARE, the IBM scientific users group, to propose these extensions to Fortran. Given the constraints of Fortran, they were unable to do this and embarked on the design of a new programming language based loosely on ALGOL labeled NPL. This acronym conflicted with that of the UK's National Physical Laboratory and was replaced briefly by MPPL (MultiPurpose Programming Language) and, in 1965, with PL/I (with a Roman numeral I). The first definition appeared in April 1964.[5][6]IBM took NPL as a starting point and completed the design to a level that the first compiler could be written: the NPL definition was incomplete in scope and in detail.[7] Control of the PL/I language[8] was vested initially in the New York Programming Center and later at the IBM UK Laboratory at Hursley. The SHARE and GUIDE user groups were involved in extending the language and had a role in IBM's process for controlling the language through their PL/I Projects. The experience of defining such a large language showed the need for a formal definition of PL/I. A project was set up in 1967 in IBM Laboratory Vienna to make an unambiguous and complete specification.[9] This led in turn to one of the first large scale Formal Methods for development, VDM.Fred Brooks is credited with ensuring PL/I had the CHARACTER data type.[10]The language was first specified in detail in the manual PL/I Language Specifications. C28-6571 written in New York from 1965 and superseded by PL/I Language Specifications. GY33-6003 written in Hursley from 1967. IBM continued to develop PL/I in the late sixties and early seventies, publishing it in the GY33-6003 manual. These manuals were used by the Multics group and other early implementers.The first compiler was delivered in 1966. The Standard for PL/I was approved in 1976.Goals and principles[edit]The goals for PL/I evolved during the early development of the language. Competitiveness with COBOL's record handling and report writing capabilities was needed. The scope of usefulness of the language grew to include system programming and event-driven programming. The additional goals for PL/I were:[3]\nPerformance of compiled code competitive with that of Fortran (but this was not achieved).\nBe extensible, for new hardware and new application areas\nImprove the productivity and time scales of the programming process, transferring effort from the programmer to the compiler\nBe machine-independent and operate effectively across the main hardware and operating system ranges\nTo meet these goals PL/I borrowed ideas from contemporary languages while adding substantial new capabilities and casting it with a distinctive concise and readable syntax. A number of principles and capabilities combined to give the language its character and were key in meeting the goals:\nBlock structure, with underlying semantics (including recursion), a\u0300 la Algol 60. Arguments are passed using call by reference, using dummy variables for values where needed (call by value).\nA wide range of computational data types, program control data types, and forms of data structure (strong typing).\nDynamic extents for arrays and strings with inheritance of extents by procedure parameters.\nConcise syntax for expressions, declarations, and statements with permitted abbreviations. Suitable for a character set of 60 glyphs and sub-settable to 48.\nAn extensive structure of defaults in statements, options, and declarations to hide some complexities and facilitate extending the language while minimizing keystrokes.\nPowerful iterative processing with good support for structured programming.\nThere were to be no reserved words (although the function names DATE and TIME, proved to be impossible to meet this goal). New attributes, statements and statement options could be added to PL/I without invalidating existing programs. Not even IF, THEN, ELSE, and DO were reserved.[11]\nOrthogonality: each capability to be independent of other capabilities and freely combined with other capabilities wherever meaningful. Each capability to be available in all contexts where meaningful, to exploit it as widely as possible and to avoid arbitrary restrictions. Orthogonality helps makes the language large.[clarification needed]\nException handling capabilities for controlling and intercepting exceptional conditions at run time.\nPrograms divided into separately compilable sections, with extensive compile-time facilities (a.k.a. macros), not part of the standard, for tailoring and combining sections of source code into complete programs. External names to bind separately compiled procedures into a single program.\nDebugging facilities integrated into the language.\nLanguage summary[edit]The language is designed to be all things to all programmers.[12] The summary is extracted from the ANSI PL/I Standard[13] and the ANSI PL/I General-Purpose Subset Standard.[14]A PL/I program consists of a set of procedures, each of which is written as a sequence of statements. The %INCLUDE construct is used to include text from other sources during program translation. All of the statement types are summarized here in groupings which give an overview of the language (the Standard uses this organization).(Features such as multi-tasking and the PL/I preprocessor are not in the Standard but are supported in the PL/I F compiler and some other implementations are discussed in the Language evolution section.)Names may be declared to represent data of the following types, either as single values, or as aggregates in the form of arrays, with a lower-bound and upper-bound per dimension, or structures (comprising nested structure, array and scalar variables):The arithmetic type comprises these attributes:The base, scale, precision and scale factor of the Picture-for-arithmetic type is encoded within the picture-specification. The mode is specified separately, with the picture specification applied to both the real and the imaginary parts.Values are computed by expressions written using a specific set of operations and builtin functions, most of which may be applied to aggregates as well as to single values, together with user-defined procedures which, likewise, may operate on and return aggregate as well as single values. The assignment statement assigns values to one or more variables.There are no reserved words in PL/I. A statement is terminated by a semi-colon. The maximum length of a statement is implementation defined. A comment may appear anywhere in a program where a space is permitted and is preceded by the characters forward slash, asterisk and is terminated by the characters asterisk, forward slash (i.e. /* This is a comment. */). Statements may have a label-prefix introducing an entry name (ENTRY and PROCEDURE statements) or label name, and a condition prefix enabling or disabling a computational condition \u2013  e.g. (NOSIZE)). Entry and label names may be single identifiers or identifiers followed by a subscript list of constants (as in L(12,2):A=0;).A sequence of statements becomes a group when preceded by a DO statement and followed by an END statement. Groups may include nested groups and begin blocks. The IF statement specifies a group or a single statement as the THEN part and the ELSE part (see the sample program). The group is the unit of iteration. The begin block (BEGIN; stmt-list END;) may contain declarations for names and internal procedures local to the block. A procedure starts with a PROCEDURE statement and is terminated syntactically by an END statement. The body of a procedure is a sequence of blocks, groups, and statements and contains declarations for names and procedures local to the procedure or EXTERNAL to the procedure.An ON-unit is a single statement or block of statements written to be executed when one or more of these conditions occur:a computational condition,or an Input/Output condition,or one of the conditions:\nAREA, CONDITION (identifier), ERROR, FINISH\nA declaration of an identifier may contain one or more of the following attributes (but they need to be mutually consistent):Current compilers from Kednos, Micro Focus, and particularly that from IBM implement many extensions over the standardized version of the language. The IBM extensions are summarised in the Implementation sub-section for the compiler later. Although there are some extensions common to these compilers the lack of a current standard means that compatibility is not guaranteed.Standardization[edit]Language standardization began in April 1966 in Europe with ECMA TC10. In 1969 ANSI established a Composite Language Development Committee, nicknamed Kludge, which fortunately was renamed X3J1 PL/I.[15] Standardization became a joint effort of ECMA TC/10 and ANSI X3J1. A subset of the GY33-6003[16] document was offered to the joint effort by IBM and became the base document for standardization. The major features omitted from the base document were multitasking and the attributes for program optimization (e.g. NORMAL and ABNORMAL).Proposals to change the base document were voted upon by both committees. In the event that the committees disagreed, the chairs, initially Michael Marcotty of General Motors and C.A.R. Hoare representing ICL had to resolve the disagreement. In addition to IBM, Honeywell, CDC, Data General, Digital Equipment, Prime Computer, Burroughs, RCA, and Univac served on X3J1 along with major users Eastman Kodak, MITRE, Union Carbide, Bell Laboratories, and various government and university representatives. Further development of the language occurred in the standards bodies, with continuing improvements in structured programming and internal consistency, and with the omission of the more obscure or contentious features.As language development neared an end, X3J1/TC10 realized that there were a number of problems with a document written in English text. Discussion of a single item might appear in multiple places which might or might not agree. It was difficult to determine if there were omissions as well as inconsistencies. Consequently, David Beech (IBM), Robert Freiburghouse (Honeywell), Milton Barber (CDC), M. Donald MacLaren (Argonne National Laboratory), Craig Franklin (Data General), Lois Frampton (Digital Equipment), and editor, D.J. Andrews of IBM undertook to rewrite the entire document, each producing one or more complete chapters. The standard is couched as a formal definition[13] using a PL/I Machine[17] to specify the semantics. It was the first, and possibly the only, programming language standard to be written as a semi-formal definition.A PL/I General-Purpose Subset (Subset-G) standard was issued by ANSI in 1981[14] and a revision published in 1987.[18] The General Purpose subset was widely adopted as the kernel for PL/I implementations.Implementations[edit]IBM PL/I F and D compilers[edit]PL/I was first implemented by IBM, at its Hursley Laboratories in the United Kingdom, as part of the development of System/360. The first production PL/I compiler was the PL/I F compiler for the OS/360 Operating System, built by John Nash's team at Hursley in the UK: the runtime library team was managed by I.M. (Nobby) Clarke. The PL/I F compiler was written entirely in System/360 assembly language.[19] Release 1 shipped in 1966. OS/360 is a real-memory environment and the compiler was designed for systems with as little as 64 kilobytes of real storage \u2013 F being 64 kB in S/360 parlance. To fit a large compiler into the 44 kilobytes of memory available on a 64-kilobyte machine, the compiler consists of a control phase and a large number of compiler phases (approaching 100). The phases are brought into memory from disk, one at a time, to handle particular language features and aspects of compilation. Each phase makes a single pass over the partially-compiled program, usually held in memory.[20]Aspects of the language were still being designed as PL/I F was implemented, so some were omitted until later releases. PL/I RECORD I/O was shipped with PL/I F Release 2. The list processing functions[21] \u2013  Based Variables, Pointers, Areas and Offsets and LOCATE-mode I/O \u2013  were first shipped in Release 4. In a major attempt to speed up PL/I code to compete with Fortran object code, PL/I F Release 5 does substantial program optimization of DO-loops facilitated by the REORDER option on procedures.A version of PL/I F was released on the TSS/360 timesharing operating system for the System/360 Model 67, adapted at the IBM Mohansic Lab. The IBM La Gaude Lab in France developed Language Conversion Programs[22] to convert Fortran, Cobol, and Algol programs to the PL/I F level of PL/I.The PL/I D compiler, using 16 kilobytes of memory, was developed by IBM Germany for the DOS/360 low end operating system. It implements a subset of the PL/I language requiring all strings and arrays to have fixed extents, thus simplifying the run-time environment. Reflecting the underlying operating system, it lacks dynamic storage allocation and the controlled storage class.[23] It was shipped within a year of PL/I F.Multics PL/I and derivatives[edit]Compilers were implemented by several groups in the early 1960s. The Multics project at MIT, one of the first to develop an operating system in a high-level language, used Early PL/I (EPL), a subset dialect of PL/I, as their implementation language in 1964. EPL was developed at Bell Labs and MIT by Douglas McIlroy, Robert Morris, and others. The influential Multics PL/I compiler[24] was the source of compiler technology used by a number of manufacturers and software groups.The Honeywell PL/I compiler (for Series 60) is an implementation of the full ANSI X3J1 standard.[25]IBM PL/I optimizing and checkout compilers[edit]The PL/I Optimizer and Checkout compilers produced in Hursley support a common level of PL/I language[26] and aimed to replace the PL/I F compiler. The checkout compiler is a rewrite of PL/I F in BSL, IBM's PL/I-like proprietary implementation language (later PL/S).[19] The performance objectives set for the compilers are shown in an IBM presentation to the BCS.[27] The compilers had to produce identical results \u2013  the Checkout Compiler is used to debug programs that would then be submitted to the Optimizer. Given that the compilers had entirely different designs and were handling the full PL/I language this goal was challenging: it was achieved.The PL/I optimizing compiler took over from the PL/I F compiler and was IBM's workhorse compiler from the 1970s to the 1990s. Like PL/I F, it is a multiple pass compiler with a 44 kilobyte design point, but it is an entirely new design. Unlike the F compiler, it has to perform compile time evaluation of constant expressions using the run-time library, reducing the maximum memory for a compiler phase to 28 kilobytes. A second-time around design, it succeeded in eliminating the annoyances of PL/I F such as cascading diagnostics. It was written in S/360 Macro Assembler by a team, led by Tony Burbridge, most of whom had worked on PL/I F. Macros were defined to automate common compiler services and to shield the compiler writers from the task of managing real-mode storage, allowing the compiler to be moved easily to other memory models. The gamut of program optimization techniques developed for the contemporary IBM Fortran H compiler were deployed: the Optimizer equaled Fortran execution speeds in the hands of good programmers. Announced with IBM S/370 in 1970, it shipped first for the DOS/360 operating system in August 1971, and shortly afterward for OS/360, and the first virtual memory IBM operating systems OS/VS1, MVS, and VM/CMS. (The developers were unaware that while they were shoehorning the code into 28 kb sections, IBM Poughkeepsie was finally ready to ship virtual memory support in OS/360). It supported the batch programming environments and, under TSO and CMS, it could be run interactively. This compiler went through many versions covering all mainframe operating systems including the operating systems of the Japanese PCMs.The compiler has been superseded by IBM PL/I for OS/2, AIX, Linux, z/OS below.The PL/I checkout compiler,[28][29] (colloquially The Checker) announced in August 1970 was designed to speed and improve the debugging of PL/I programs. The team was led by Brian Marks. The three-pass design cut the time to compile a program to 25% of that taken by the F Compiler. It can be run from an interactive terminal, converting PL/I programs into an internal format, H-text. This format is interpreted by the Checkout compiler at run-time, detecting virtually all types of errors. Pointers are represented in 16 bytes, containing the target address and a description of the referenced item, thus permitting bad pointer use to be diagnosed. In a conversational environment when an error is detected, control is passed to the user who can inspect any variables, introduce debugging statements and edit the source program. Over time the debugging capability of mainframe programming environments developed most of the functions offered by this compiler and it was withdrawn (in the 1990s?)Digital PL/I[edit]Perhaps the most commercially successful implementation aside from IBM's was Digital Equipment's 1988 release of the ANSI PL/I 1987 subset. The implementation is a strict superset of the ANSI X3.4-1981 PL/I General Purpose Subset and provides most of the features of the new ANSI X3.74-1987 PL/I General Purpose Subset.[30] The front end was designed by Robert Freiburghouse, and the code generator was implemented by Dave Cutler, who managed the design and implementation of VAX/VMS. It runs on VMS on VAX and ALPHA and on Tru64. UniPrise Systems, Inc., was responsible for the compiler;[31]Teaching subset compilers[edit]In the late 1960s and early 1970s, many US and Canadian Universities were establishing time-sharing services on campus and needed conversational compiler/interpreters for use in teaching science, mathematics, engineering, and computer science. Dartmouth were developing BASIC, but PL/I was a popular choice, as it was concise and easy to teach. As the IBM offerings were unsuitable,[32] a number of schools built their own subsets of PL/I and their own interactive support. Examples are:A compiler developed at Cornell University for teaching a dialect called PL/C, which had the unusual capability of never failing to compile any program through the use of extensive automatic correction of many syntax errors and by converting any remaining syntax errors to output statements. The language was almost all of PL/I as implemented by IBM.[33] PL/C was a very fast compiler.PLAGO, created at the Polytechnic Institute of Brooklyn, used a simplified subset of the PL/I language[34] and focused on good diagnostic error messages and fast compilation times.The Computer Systems Research Group of the University of Toronto produced the SP/k compilers which supported a sequence of subsets of PL/I called SP/1, SP/2, SP/3, ..., SP/8 for teaching programming. Programs that ran without errors under the SP/k compilers produced the same results under other contemporary PL/I compilers such as IBM's PL/I F compiler, IBM's checkout compiler or Cornell University's PL/C compiler.[35]Other examples are PL0 by P. Grouse at the University of New South Wales, PLUM by Marvin Zelkowitz at the University of Maryland.,[36] and PLUTO from the University of Toronto.IBM PL/I for OS/2, AIX, Linux, z/OS[edit]In a major revamp of PL/I, IBM Santa Teresa in California launched an entirely new compiler in 1992. The initial shipment was for OS/2 and included most ANSI-G features and many new PL/I features.[37] Subsequent releases covered additional platforms (MVS, VM, OS/390, AIX and Windows)[38] and continued to add functions to make PL/I fully competitive with other languages offered on the PC (particularly C and C++) in areas where it had been overtaken. The corresponding IBM Language Environment supports inter-operation of PL/I programs with Database and Transaction systems, and with programs written in C, C++, and COBOL, the compiler supports all the data types needed for intercommunication with these languages.The PL/I design principles were retained and withstood this major extension comprising several new data types, new statements and statement options, new exception conditions, and new organisations of program source. The resulting language is a compatible super-set of the PL/I Standard and of the earlier IBM compilers. Major topics added to PL/I were:\nNew attributes for better support of object-oriented programming \u2013 the DEFINE ALIAS, ORDINAL, and DEFINE STRUCTURE statement to introduce user-defined types, the HANDLE locator data type, the TYPE data type itself, the UNION data type, and built-in functions for manipulating the new types.\nAdditional data types and attributes corresponding to common PC data types (e.g. UNSIGNED, VARYINGZ).\nImprovements in readability of programs \u2013 often rendering implied usages explicit (e.g. BYVALUE attribute for parameters)\nAdditional structured programming constructs.\nInterrupt handling additions.\nCompile time preprocessor extended to offer almost all PL/I string handling features and to interface with the Application Development Environment\nThe latest series of PL/I compilers for z/OS, called Enterprise PL/I for z/OS, leverage code generation for the latest z/Architecture processors (z990, zEC12, Z13) via the use of ARCHLVL parm control passed during compilation, and was the second High level language supported by z/OS Language Environment to do so (XL C/C++ being the first, and Enterprise COBOL v5 the last.)Object orientation[edit]ORDINAL is a new computational data type. The ordinal facilities are like those in Pascal, e.g. DEFINE ORDINAL Colour (red, yellow, green, blue, violet); but in addition the name and internal values are accessible via built-in functions. Built-in functions provide access to an ordinal value's predecessor and successor.The DEFINE-statement (see below) allows additional TYPEs to be declared composed from PL/I's built-in attributes.The HANDLE(data structure) locator data type is similar to the POINTER data type, but strongly typed to bind only to a particular data structure. The => operator is used to select a data structure using a handle.The UNION attribute (equivalent to CELL in early PL/I specifications) permits several scalar variables, arrays, or structures to share the same storage in a unit that occupies the amount of storage needed for the largest alternative.Competitiveness on PC and with C[edit]These attributes were added:\nThe string attributes VARYINGZ (for zero-terminated character strings), HEXADEC, WIDECHAR, and GRAPHIC.\nThe optional arithmetic attributes UNSIGNED and SIGNED, BIGENDIAN and LITTLEENDIAN. UNSIGNED necessitated the UPTHRU and DOWNTHRU option on iterative groups enabling a counter-controlled loop to be executed without exceeding the limit value (also essential for ORDINALs and good for documenting loops.\nThe DATE(pattern) attribute for controlling date representations and additions to bring time and date to best current practice. New functions for manipulating dates include \u2013  DAYS and DAYSTODATE for converting between dates and number of days, and a general DATETIME function for changing date formats.\nNew string-handling functions were added \u2013  to centre text, to edit using a picture format, and to trim blanks or selected characters from the head or tail of text, VERIFYR to VERIFY from the right. and SEARCH and TALLY functions.Compound assignment operators a la C e.g. +=, &=, -=, ||= were added. A+=1 is equivalent to A=A+1.Additional parameter descriptors and attributes were added for omitted arguments and variable length argument lists.Program readability \u2013 making intentions explicit[edit]The VALUE attribute declares an identifier as a constant (derived from a specific literal value or restricted expression).Parameters can have the BYADDR (pass by address) or BYVALUE (pass by value) attributes.The ASSIGNABLE and NONASSIGNABLE attributes prevent unintended assignments.DO FOREVER; obviates the need for the contrived construct DO WHILE ( '1'B );.The DEFINE-statement introduces user-specified names (e.g. INTEGER) for combinations of built-in attributes (e.g. FIXED BINARY(31,0)). Thus DEFINE ALIAS INTEGER FIXED BINARY(31.0) creates the TYPE name INTEGER as an alias for the set of built-in attributes FIXED BINARY(31.0). DEFINE STRUCTURE applies to structures and their members; it provides a TYPE name for a set of structure attributes and corresponding substructure member declarations for use in a structure declaration (a generalisation of the LIKE attribute).Structured programming additions[edit]A LEAVE statement to exit a loop, and an ITERATE to continue with the next iteration of a loop.UPTHRU and DOWNTHRU options on iterative groups.The package construct consisting of a set of procedures and declarations for use as a unit. Variables declared outside of the procedures are local to the package, and can use STATIC, BASED or CONTROLLED storage. Procedure names used in the package also are local, but can be made external by means of the EXPORTS option of the PACKAGE-statement.Interrupt handling[edit]The RESIGNAL-statement executed in an ON-unit terminates execution of the ON-unit, and raises the condition again in the procedure that called the current one (thus passing control to the corresponding ON-unit for that procedure).The INVALIDOP condition handles invalid operation codes detected by the PC processor, as well as illegal arithmetic operations such as subtraction of two infinite values.The ANYCONDITION condition is provided to intercept conditions for which no specific ON-unit has been provided in the current procedure.The STORAGE condition is raised when an ALLOCATE statement is unable to obtain sufficient storage.Other mainframe and minicomputer compilers[edit]A number of vendors produced compilers to compete with IBM PL/I F or Optimizing compiler on mainframes and minicomputers in the 1970s. In the 1980s the target was usually the emerging ANSI-G subset.\nIn 1974 Burroughs Corporation announced PL/I for the B6700 and B7700.[39]\nUNIVAC released a UNIVAC PL/I,[40] and in the 1970s also used a variant of PL/I, PL/I PLUS, for system programming.\nFrom 1978 Data General provided PL/I on its Eclipse and Eclipse MV platforms running the AOS, AOS/VS & AOS/VS II operating systems.[41] A number of operating system utility programs were written in the language.\nCDC delivered an optimizing subset PL/I compiler for Cyber 70, 170 and 6000 series.[42]\nFujitsu delivered a PL/I compiler equivalent to the PL/I Optimizer.\nPrime Computer utilized PL/P in the later years of the 50 series to implement the portions of PRIMOS that were not still written in Fortran IV.\nStratus Technologies PL/I is an ANSI G implementation.[43]\nPL/G Subset for IBM Series/1 Mini Computer with Real Time extensions PL/I Language Reference GC34-0085-0\nPL/I compilers for Microsoft .NET[edit]\nIn 2011, Raincode designed a full legacy compiler for the Microsoft .NET platform, named Raincode PL/I.\nPL/I compilers for personal computers and Unix[edit]\nIn 1973 Gary Kildall implemented a subset PL/M for Intel. PL/M was used to write the CP/M operating system proper \u2013  and much application software running on CP/M and MP/M. Digital Research sold a PL/I compiler[44] for the Personal Computer written in PL/M. PL/M was used to write much other software at Intel for the 8080, 8085, and Z-80 processors during the 1970s.\nPL/M-86 was a version of the PL/M language for the 8086 and 8088 microprocessor. It was heavily used by Daisy Systems for electronic design automation software on the Logician family of special-purpose workstations.\nMicro Focus implemented Open PL/I for Windows[45] and UNIX/Linux systems,[46] which they acquired from Liant.\nIBM delivered PL/I for OS/2[37] (1994).\nIron Spring PL/I for OS/2 and later Linux was introduced in 2007.\nSpecial purpose and system PL/I compilers[edit]\nA subset of PL/P (called PL/S) was used to write new portions of Prime Computer PRIMOS at all rings of protection in its later revisions.\nIBM delivered PL/I for the Series/1 computer family in the 1970s.[47]\nPaul Abrahams of NYU's Courant Institute of Mathematical Sciences wrote CIMS PL/I in 1972 in PL/I, bootstrapping via PL/I F. It supported about 70% of PL/I compiling to the CDC 6600[48]\nPL/8 (or PL.8), so-called because it was about 80% of PL/I, was developed by IBM Research in the 1970s, and is used for several IBM internal systems development tasks (e.g. millicode for z/Architecture)\nHoneywell, Inc. developed PL-6 for use in creating the CP-6 operating system.\nPL/I dialect compilers[edit]\nPL/S, a dialect of PL/I, initially called BSL was developed in the late 1960s and became the IBM system programming language. Almost all IBM mainframe system software in the 1970s and 1980s was written in PL/S. It differed from PL/I in that there were no data type conversions, no run-time environment, structures were mapped differently, and assignment was a byte by byte copy. All strings and arrays had fixed extents, or used the REFER option. IBM uses an improved and renamed PL/S (PL/X) for internal work on current operating systems, OS/390 and now z/OS. It is also used for some z/VSE and z/VM components.\nXPL is a dialect of PL/I used to write other compilers using the XPL compiler techniques. XPL added a heap string datatype to its small subset of PL/I.\nHAL/S is a real-time aerospace programming language, best known for its use in the Space Shuttle program. It was designed by Intermetrics in the 1970s for NASA. HAL/S was implemented in XPL.\nIBM and various subcontractors also developed another PL/I variant in the early 1970s to support signal processing for the Navy called SPL/I.\nSabreTalk, a real-time dialect of PL/I used to program the Sabre airline reservation system.\nUsage[edit]PL/I implementations were developed for mainframes from the late 1960s, mini computers in the 1970s, and personal computers[44] in the 1980s and 1990s. Although its main use has been on mainframes, there are PL/I versions for DOS, Microsoft Windows, OS/2, AIX, OpenVMS, and Unix.It has been widely used in business data processing[49] and for system use for writing operating systems on certain platforms. Very complex and powerful systems have been built with PL/I:The SAS System was initially written in PL/I; the SAS data step is still modeled on PL/I syntax.The pioneering online airline reservation system Sabre was originally written for the IBM 7090 in assembler. The S/360 version was largely written using SabreTalk, a purpose built subset PL/I compiler for a dedicated control program.PL/I was used to write an executable formal definition[50] to interpret IBM's System Network ArchitecturePL/I did not fulfill its supporters' hopes that it would displace Fortran and COBOL and become the major player on mainframes. It remained a minority but significant player. There cannot be a definitive explanation for this, but some trends in the 1970s and 1980s militated against its success by progressively reducing the territory on which PL/I enjoyed a competitive advantage.First, the nature of the mainframe software environment changed. Application subsystems for database and transaction processing (CICS and IMS and Oracle on System 370) and application generators became the focus of mainframe users' application development. Significant parts of the language became irrelevant because of the need to use the corresponding native features of the subsystems (such as tasking and much of input/output). Fortran was not used in these application areas, confining PL/I to COBOL's territory; most users stayed with COBOL. But as the PC became the dominant environment for program development, Fortran, COBOL and PL/I all became minority languages overtaken by C++, Java and the like.Second, PL/I was overtaken in the system programming field. The IBM system programming community was not ready to use PL/I; instead, IBM developed and adopted a proprietary dialect of PL/I for system programming. \u2013 PL/S.[51] With the success of PL/S inside IBM, and of C outside IBM, the unique PL/I strengths for system programming became less valuable.Third, the development environments grew capabilities for interactive software development that, again, made the unique PL/I interactive and debugging strengths less valuable.Fourth, COBOL and Fortran added features such as structured programming, character string operations, and object orientation, that further reduced PL/I's relative advantages.On mainframes there were substantial business issues at stake too. IBM's hardware competitors had little to gain and much to lose from success of PL/I. Compiler development was expensive, and the IBM compiler groups had an in-built competitive advantage. Many IBM users wished to avoid being locked into proprietary solutions. With no early support for PL/I by other vendors it was best to avoid PL/I.Evolution of the PL/I language[edit]This article uses the PL/I standard as the reference point for language features. But a number of features of significance in the early implementations were not in the Standard; and some were offered by non-IBM compilers. And the de facto language continued to grow after the standard, ultimately driven by developments on the Personal Computer.Significant features omitted from the standard[edit]Multi tasking[edit]Multi tasking was implemented by PL/I F, the Optimizer and the newer AIX and Z/OS compilers. It comprised the data types EVENT and TASK, the TASK-option on the CALL-statement (Fork), the WAIT-statement (Join), the DELAY(delay-time), EVENT-options on the record I/O statements and the UNLOCK statement to unlock locked records on EXCLUSIVE files. Event data identify a particular event and indicate whether it is complete ('1'B) or incomplete ('0'B): task data items identify a particular task (or process) and indicate its priority relative to other tasks.Preprocessor[edit]The first IBM Compile time preprocessor was built by the IBM Boston Advanced Programming Center located in Cambridge, Mass, and shipped with the PL/I F compiler. The %INCLUDE statement was in the Standard, but the rest of the features were not. The DEC and Kednos[52] PL/I compilers implemented much the same set of features as IBM, with some additions of their own. IBM has continued to add preprocessor features to its compilers. The preprocessor treats the written source program as a sequence of tokens, copying them to an output source file or acting on them. When a % token is encountered the following compile time statement is executed: when an identifier token is encountered and the identifier has been DECLAREd, ACTIVATEd, and assigned a compile time value, the identifier is replaced by this value. Tokens are added to the output stream if they do not require action (e.g. +), as are the values of ACTIVATEd compile time expressions. Thus a compile time variable PI could be declared, activated, and assigned using %PI='3.14159265'. Subsequent occurrences of PI would be replaced by 3.14159265.The data type supported are FIXED DECIMAL integers and CHARACTER strings of varying length with no maximum length. The structure statements are:\n%[label-list:]DO iteration: statements; %[label-list:]END;\n%procedure-name: PROCEDURE (parameter list) RETURNS (type); statements...;\n%[label-list:]END;\n%[label-list:]IF...%THEN...%ELSE..\nand the simple statements, which also may have a [label-list:]\n%ACTIVATE(identifier-list) and %DEACTIVATE\nassignment statement\n%DECLARE identifier-attribute-list\n%GO TO label\n%INCLUDE\nnull statement\nThe feature allowed programmers to use identifiers for constants \u2013  e.g. product part numbers or mathematical constants \u2013  and was superseded in the standard by named constants for computational data. Conditional compiling and iterative generation of source code, possible with compile-time facilities, was not supported by the standard. Several manufacturers implemented these facilities.Structured programming additions[edit]Structured programming additions were made to PL/I during standardization but were not accepted into the standard. These features were the LEAVE-statement to exit from an iterative DO, the UNTIL-option and REPEAT-option added to DO, and a case statement of the general form: SELECT (expression) {WHEN (expression) group}... OTHERWISE group\nThese features were all included in DEC PL/I.[53]Debug facilities[edit]PL/I F had offered some debug facilities that were not put forward for the standard but were implemented by others \u2013  notably the CHECK(variable-list) condition prefix, CHECK on-condition and the SNAP option. The IBM Optimizing and Checkout compilers added additional features appropriate to the conversational mainframe programming environment (e.g. an ATTENTION condition).Significant features developed since the standard[edit]Several attempts had been made to design a structure member type that could have one of several datatypes (CELL in early IBM). With the growth of classes in programming theory, approaches to this became possible on a PL/I base \u2013  UNION, TYPE etc. have been added by several compilers.PL/I had been conceived in a single-byte character world. With support for Japanese and Chinese language becoming essential, and the developments on International Code Pages, the character string concept was expanded to accommodate wide non-ASCII/EBCDIC strings.Time and date handling were overhauled to deal with the millennium problem, with the introduction of the DATETIME function that returned the date and time in one of about 35 different formats. Several other date functions deal with conversions to and from days and seconds.Criticisms[edit]Implementation issues[edit]Though the language is easy to learn and use, implementing a PL/I compiler is difficult and time-consuming. A language as large as PL/I needed subsets that most vendors could produce and most users master. This was not resolved until ANSI G was published. The compile time facilities, unique to PL/I, took added implementation effort and additional compiler passes. A PL/I compiler was two to four times as large as comparable Fortran or COBOL compilers, and also that much slower\u2014supposedly offset by gains in programmer productivity. This was anticipated in IBM before the first compilers were written.[7]Some argue that PL/I is unusually hard to parse.[54] The PL/I keywords are not reserved so programmers can use them as variable or procedure names in programs. Because the original PL/I(F) compiler attempts auto-correction when it encounters a keyword used in an incorrect context, it often assumes it is a variable name. This leads to cascading diagnostics, a problem solved by later compilers.The effort needed to produce good object code was perhaps underestimated during the initial design of the language. Program optimization (needed to compete with the excellent program optimization carried out by available Fortran compilers) is unusually complex owing to side effects and pervasive problems with aliasing of variables. Unpredictable modification can occur asynchronously in exception handlers, which may be provided by ON statements in (unseen) callers. Together, these make it difficult to reliably predict when a program's variables might be modified at runtime. In typical use, however, user-written error handlers (the ON-unit) often do not make assignments to variables. In spite of the aforementioned difficulties, IBM produced its PL/I optimising compiler in 1971.PL/I contains many rarely-used features, such as multitasking support (an IBM extension to the language) which add cost and complexity to the compiler, and its co-processing facilities require a multi-programming environment with support for non-blocking multiple threads for processes by the operating system. Compiler writers were free to select whether to implement these features.An undeclared variable is, by default, declared by first occurrence\u2014thus misspelling might lead to unpredictable results. This implicit declaration is no different from FORTRAN programs. For PL/I(F), however, an attribute listing enables the programmer to detect any misspelled or undeclared variable.Programmer issues[edit]Many programmers were slow to move from COBOL or Fortran due to a perceived complexity of the language and immaturity of the PL/I F compiler. Programmers were sharply divided into scientific programmers (who used Fortran) and business programmers (who used COBOL), with significant tension and even dislike between the groups. PL/I syntax borrowed from both COBOL and Fortran syntax. So instead of noticing features that would make their job easier, Fortran programmers of the time noticed COBOL syntax and had the opinion that it was a business language, while COBOL programmers noticed Fortran syntax and looked upon it as a scientific language.Both COBOL and Fortran programmers viewed it as a bigger version of their own language, and both were somewhat intimidated by the language and disinclined to adopt it. Another factor was pseudo-similarities to COBOL, Fortran, and ALGOL. These were PL/I elements that looked similar to one of those languages, but worked differently in PL/I. Such frustrations left many experienced programmers with a jaundiced view of PL/I, and often an active dislike for the language. An early UNIX fortune file contained the following tongue-in-cheek description of the language:\nSpeaking as someone who has delved into the intricacies of PL/I, I am sure that only Real Men could have written such a machine-hogging, cycle-grabbing, all-encompassing monster. Allocate an array and free the middle third? Sure! Why not? Multiply a character string times a bit string and assign the result to a float decimal? Go ahead! Free a controlled variable procedure parameter and reallocate it before passing it back? Overlay three different types of variable on the same memory location? Anything you say! Write a recursive macro? Well, no, but Real Men use rescan. How could a language so obviously designed and written by Real Men not be intended for Real Man use?\nOn the positive side, full support for pointers to all data types (including pointers to structures), recursion, multitasking, string handling, and extensive built-in functions PL/I was indeed quite a leap forward compared to the programming languages of its time. However, these were not enough to convince a majority of programmers or shops to switch to PL/I.The PL/I F compiler's compile time preprocessor was unusual (outside the Lisp world[55]) in using its target language's syntax and semantics (e.g. as compared to the C preprocessor's # directives).Special topics in PL/I[edit]Storage classes[edit]PL/I provides several 'storage classes' to indicate how the lifetime of variables' storage is to be managed \u2013  STATIC, AUTOMATIC, CONTROLLED and BASED. The simplest to implement is STATIC, which indicates that memory is allocated and initialized at load-time, as is done in COBOL working-storage and early Fortran. This is the default for EXTERNAL variables. PL/I's default storage class for INTERNAL variables is AUTOMATIC, similar to that of other block-structured languages influenced by ALGOL, like the auto storage class in the C language, and default storage allocation in Pascal and local-storage in IBM COBOL. Storage for AUTOMATIC variables is allocated upon entry into the BEGIN-block, procedure, or ON-unit in which they are declared. The compiler and runtime system allocate memory for a stack frame to contain them and other housekeeping information. If a variable is declared with an INITIAL-attribute, code to set it to an initial value is executed at this time. Care is required to manage the use of initialization properly. Large amounts of code can be executed to initialize variables every time a scope is entered, especially if the variable is an array or structure. Storage for AUTOMATIC variables is freed at block exit: STATIC, CONTROLLED or BASED variables are used to retain variables' contents between invocations of a procedure or block. CONTROLLED storage is also managed using a stack, but the pushing and popping of allocations on the stack is managed by the programmer, using ALLOCATE and FREE statements. Storage for BASED variables is managed using ALLOCATE/FREE, but instead of a stack these allocations have independent lifetimes and are addressed through OFFSET or POINTER variables.The AREA attribute is used to declare programmer-defined heaps. Data can be allocated and freed within a specific area, and the area can be deleted, read, and written as a unit.Storage type sharing[edit]There are several ways of accessing allocated storage through different data declarations. Some of these are well defined and safe, some can be used safely with careful programming, and some are inherently unsafe and/or machine dependent.Passing a variable as an argument to a parameter by reference allows the argument's allocated storage to be referenced using the parameter. The DEFINED attribute (e.g. DCL A(10,10), B(2:9,2:9) DEFINED A) allows part or all of a variable's storage to be used with a different, but consistent, declaration. The language definition includes a CELL alttribute (later renamed UNION) to allow different definitions of data to share the same storage. This was not supported by many early IBM compilers. These usages are safe and machine independent.Record I/O and list processing produce situations where the programmer needs to fit a declaration to the storage of the next record or item, before knowing what type of data structure it has. Based variables and pointers are key to such programs. The data structures must be designed appropriately, typically using fields in a data structure to encode information about its type and size. The fields can be held in the preceding structure or, with some constraints, in the current one. Where the encoding is in the preceding structure, the program needs to allocate a based variable with a declaration that matches the current item (using expressions for extents where needed). Where the type and size information are to be kept in the current structure (self defining structures) the type-defining fields must be ahead of the type dependent items and in the same place in every version of the data structure. The REFER-option is used for self-defining extents (e.g. string lengths as in DCL 1 A BASED, 2 N BINARY, 2 B CHAR(LENGTH REFER A.N.), etc  \u2013  where LENGTH is used to allocate instances of the data structure. For self-defining structures, any typing and REFERed fields are placed ahead of the real data. If the records in a data set, or the items in a list of data structures, are organised this way they can be handled safely in a machine independent way.PL/I implementations do not (except for the PL/I Checkout compiler) keep track of the data structure used when storage is first allocated. Any BASED declaration can be used with a pointer into the storage to access the storage \u2013  inherently unsafe and machine dependent. However, this usage has become important for pointer arithmetic (typically adding a certain amount to a known address). This has been a contentious subject in computer science. In addition to the problem of wild references and buffer overruns, issues arise due to the alignment and length for data types used with particular machines and compilers. Many cases where pointer arithmetic might be needed involve finding a pointer to an element inside a larger data structure. The ADDR function computes such pointers, safely and machine independently.Pointer arithmetic may be accomplished by aliasing a binary variable with a pointer as in\nDCL P POINTER, N FIXED BINARY(31) BASED(ADDR(P)); N=N+255;\nIt relies on pointers being the same length as FIXED BINARY(31) integers and aligned on the same boundaries.With the prevalence of C and its free and easy attitude to pointer arithmetic, recent IBM PL/I compilers allow pointers to be used with the addition and subtraction operators to giving the simplest syntax (but compiler options can disallow these practices where safety and machine independence are paramount).ON-units and exception handling[edit]When PL/I was designed, programs only ran in batch mode, with no possible intervention from the programmer at a terminal. An exceptional condition such as division by zero would abort the program yielding only a hexadecimal core dump. PL/I exception handling, via ON-units, allowed the program to stay in control in the face of hardware or operating system exceptions and to recover debugging information before closing down more gracefully. As a program became properly debugged, most of the exception handling could be removed or disabled: this level of control became less important when conversational execution became commonplace.Computational exception handling is enabled and disabled by condition prefixes on statements, blocks(including ON-units) and procedures. \u2013 e.g. (SIZE, NOSUBSCRIPTRANGE): A(I)=B(I)*C; . Operating system exceptions for Input/Output and storage management are always enabled.The ON-unit is a single statement or BEGIN-block introduced by an ON-statement. Executing the ON statement enables the condition specified, e.g., ON ZERODIVIDE ON-unit. When the exception for this condition occurs and the condition is enabled, the ON-unit for the condition is executed. ON-units are inherited down the call chain. When a block, procedure or ON-unit is activated, the ON-units established by the invoking activation are inherited by the new activation. They may be over-ridden by another ON-statement and can be reestablished by the REVERT-statement. The exception can be simulated using the SIGNAL-statement \u2013 e.g. to help debug the exception handlers. The dynamic inheritance principle for ON-units allows a routine to handle the exceptions occurring within the subroutines it uses.If no ON-unit is in effect when a condition is raised a standard system action is taken (often this is to raise the ERROR condition). The system action can be reestablished using the SYSTEM option of the ON-statement. With some conditions it is possible to complete executing an ON-unit and return to the point of interrupt (e.g., the STRINGRANGE, UNDERFLOW, CONVERSION, OVERFLOW, AREA and FILE conditions) and resume normal execution. With other conditions such as (SUBSCRIPTRANGE), the ERROR condition is raised when this is attempted. An ON-unit may be terminated with a GO TO preventing a return to the point of interrupt, but permitting the program to continue execution elsewhere as determined by the programmer.An ON-unit needs to be designed to deal with exceptions that occur in the ON-unit itself. The ON ERROR SYSTEM; statement allows a nested error trap; if an error occurs within an ON-unit, control might pass to the operating system where a system dump might be produced, or, for some computational conditions, continue execution (as mentioned above).The PL/I RECORD I/O statements have relatively simple syntax as they do not offer options for the many situations from end-of-file to record transmission errors that can occur when a record is read or written. Instead, these complexities are handled in the ON-units for the various file conditions. The same approach was adopted for AREA sub-allocation and the AREA condition.The existence of exception handling ON-units can have an effect on optimization, because variables can be inspected or altered in ON-units. Values of variables that might otherwise be kept in registers between statements, may need to be returned to storage between statements. This is discussed in the section on Implementation Issues above.Sample programs[edit]Hello world program[edit]\nHello2: proc options(main);\n     put list ('Hello, world!');\nend Hello2;\nSearch for a string[edit]\n/* Read in a line, which contains a string,\n/* and then print every subsequent line that contains that string. */\n\nfind_strings: procedure options (main);\n   declare pattern character (100) varying;\n   declare line character (100) varying;\n   declare line_no fixed binary;\n\n   on endfile (sysin) stop;\n\n   get edit (pattern) (L);\n   line_no = 1;\n   do forever;\n      get edit (line) (L);\n      if index(line, pattern) > 0 then\n         put skip list (line_no, line);\n      line_no = line_no + 1;\n   end;\n\nend find_strings;\nSee also[edit]\nList of programming languages\nTimeline of programming languages\nNotes[edit]References[edit]Textbooks[edit]\nBarnes, R.A. (1979). PL/I for Programmers. North-Holland. \nHughes, J.K. (1986). PL/I Structured Programming (3rd ed.). Wiley. ISBN 0-8053-0051-1. \nGroner, G.F. (1971). PL/I Programming in Technological Applications. Books on Demand, Ann Arbor, MI. \nAnderson, M.E. (1973). PL/I for Programmers. Prentice-Hall. \nStoutemyer, D.R. (1971). PL/I Programming for Engineering & Science. Prentice-Hall. \nZiegler, R.R. & C. (1986). PL/I: Structured Programming and Problem Solving (1st ed.). West. ISBN 0-314-93915-6. \nSturm, E. (2009). The New PL/I ... for PC, Workstation and Mainframe. Vieweg-Teubner, Wiesbaden, Germany. ISBN 978-3-8348-0726-7. \nVowels, R.A. Introduction to PL/I, Algorithms, and Structured Programming (3rd ed.). ISBN 0-9596384-9-0. \nAbrahams, Paul (1979). The PL/I Programming Language (PDF). Courant Mathematics and Computing Laboratory, New York University. \nStandards[edit]\nANSI ANSI X3.53-1976 (R1998) Information Systems - Programming Language - PL/I\nANSI ANSI X3.74-1981 (R1998) Information Systems - Programming Language - PL/I General-Purpose Subset\nECMA 50 Programming Language PL/I, 1st edition, December 1976\nISO 6160:1979 Programming languages\u2014PL/I\nISO/IEC 6522:1992 Information technology\u2014Programming languages\u2014PL/I general purpose subset\nReference manuals[edit]\nBurroughs Corporation, B 6700 / B 7700 PL/I Language Reference, 5001530. Detroit, 1977.\nCDC. R. A. Vowels, PL/I for CDC Cyber. Optimizing compiler for the CDC Cyber 70 series.\nDigital Equipment Corporation, decsystem10 Conversational Programming Language User's Manual, DEC-10-LCPUA-A-D. Maynard, 1975.\nFujitsu Ltd, Facom OS IV PL/I Reference Manual, 70SP5402E-1,1974. 579 pages. PL/I F subset.\nHoneywell, Inc., Multics PL/I Language Specification, AG94-02. 1981.\nIBM, Operating System/360 PL/I: Language Specifications, C28-6571. 1965.\nIBM, OS PL/I Checkout and Optimizing Compilers: Language Reference Manual, GC33-0009. 1970.\nIBM, NPL Technical Report, December 1964.\nIBM, Enterprise PL/I for z/OS Version 4 Release 1 Language Reference Manual, SC14-7285-00. 2010.\nIBM, OS/2 PL/I Version 2: Programming: Language Reference, 3rd Ed., Form SC26-4308, San Jose. 1994.\nKednos PL/I for OpenVMS Systems. Reference Manual, AA-H952E-TM. Nov 2003.\nLiant Software Corporation (1994), Open PL/I Language Reference Manual, Rev. Ed., Framingham (Mass.).\nNixdorf Computer, Terminalsystem 8820 Systemtechnischer Teil PL/I-Subset,05001.17.8.93-01, 1976.\nIng. C. Olivetti, Mini PL/I Reference Manual, 1975, No. 3970530 V\nQ1 Corporation, The Q1/LMC Systems Software Manual, Farmingdale, 1978.\nExternal links[edit]\nIBM PL/I Compilers for z/OS, AIX, MVS, VM and VSE\nIron Spring Software, PL/I for Linux and OS/2\nKednos PL/I for OpenVMS and Tru64\nMicro Focus\u2019 Mainframe PL/I Migration Solution\nOS PL/I V2R3 grammar Version 0.1\nPliedit, PL/I editor for Eclipse\nPower vs. Adventure - PL/I and C, a side-by-side comparison of PL/I and C.\nThe Compilerator, Online PL/I compiler for small experiments and tinkering.\nSoftpanorama PL/1 page\nThe PL/I Language\nPL1GCC project in SourceForge\n", "subtitles": ["Early history", "Goals and principles", "Language summary", "Standardization", "Implementations", "Usage", "Evolution of the PL/I language", "Criticisms", "Special topics in PL/I", "Sample programs", "See also", "Notes", "References", "External links"], "title": "PL/I"},
{"content": "The Maya Embedded Language (MEL) is a scripting language used to simplify tasks in Autodesk's 3D Graphics Software Maya. Most tasks that can be achieved through Maya's GUI can be achieved with MEL, as well as certain tasks that are not available from the GUI. MEL offers a method of speeding up complicated or repetitive tasks, as well as allowing users to redistribute a specific set of commands to others that may find it useful.Design[edit]MEL is syntactically similar to Perl and Tcl. It provides some memory management and dynamic array-allocation, and offers direct access to functions specific to Maya. The majority of standard Maya commands are default MEL scripts, saved in the Maya Program Files directory.MEL is fairly limited compared to mainstream scripting languages. Mel is primarily a language meant to journal the Maya session, thus lacks many advanced features such as associative arrays. Python was added to Maya as an alternative to MEL in Maya 8.5. Still MEL offers some advantages to Python:\nDiscoverability of MEL is better since the built-in command echo server was created with MEL in mind.\nA lot of legacy code and inner working of Maya is in MEL form. This provides many insights into undocumented inner workings of Maya.\nMEL is often criticized for its lack of programming features, and object orientation. In practice, however, MEL is used as a metaprogramming language where MEL just instructs Maya's node architecture to solve the problem. Since nodes form the actual computational core of Maya using a more robust readily apparent programming method would be a very inefficient approach. In many ways MEL complements the Maya API, with somewhat clear boundaries between allocated tasks. This makes MEL harder to learn and fully understand.Uses[edit]The tools designed using MEL scripts generally come under the following categories:\nData I/O\nMotion capture data import\nProprietary game data export\nScene meta-data for production tracking\nGeometry creation/modification\nCustom primitives\nThird-party renderer specific data type (e.g., RenderMan sub-divisional surfaces)\nModeling tools not offered in the base package\nAnimation tools\nMuscle simulators\nRigging/setup controls\nCrowd AI behavior\nLighting /rendering tools\nAutomatic creation of common complex shader setups\nPre- and post-render effects\nMaking calls to third-party renderers\nDynamics\nCustom particle behavior\nSimulated object emergent flocking and swarming\nGenetic algorithms\nCloth simulation\nFile and folder scene setup and maintenance\nUser interface customization\nCustom character controls\nRemoval of invalid Maya commands\nCustom UIs\nExample[edit]This is an example of a script which copies a selected object through its path:Features[edit]Maya also offers an expression language that is a super-set of MEL, and results in nodes that are executed as part of Maya's dependency graph. Expressions are developed with Maya's expression editor, and allow scripts to trigger while Maya evaluates the scene file at each change in time, to simulate complex behaviors or perform other useful tasks.References[edit]External links[edit]\nOfficial website\nMEL command reference (Maya 2013)\nMEL command reference (Maya 2014)\nMEL command reference (Maya 2015)\nMEL and Expressions - Autodesk Maya 2015 Help\nMel Wiki\nUseful Scripts for Maya\nCGSociety - Maya Programming forum\nCGSociety - 'MEL scripts' thread\nCGSociety - 'Maya UI Building' thread\nMEL Scripting Tutorials for Maya - CreativeCrash\nvariables in mel - ACCAD instructor Alan Price\nMEL video tutorials - lynda.com\nDigital Tutors search - MEL\nmaya mel - polyextrude.com\nMEL - maya rigging wiki\nMel Scripting - RobTheBloke\n", "subtitles": ["Design", "Uses", "Example", "Features", "References", "External links"], "title": "Maya Embedded Language"},
{"content": "In computer science, conditional statements, conditional expressions and conditional constructs are features of a programming language, which perform different computations or actions depending on whether a programmer-specified boolean condition evaluates to true or false. Apart from the case of branch predication, this is always achieved by selectively altering the control flow based on some condition.In imperative programming languages, the term conditional statement is usually used, whereas in functional programming, the terms conditional expression or conditional construct are preferred, because these terms all have distinct meanings.A conditional is sometimes colloquially referred to as an if-check, especially when perceived as a simple one and when its specific form is irrelevant or unknown.Although dynamic dispatch is not usually classified as a conditional construct, it is another way to select between alternatives at runtime.If\u2013then(\u2013else)[edit]The if\u2013then construct (sometimes called if\u2013then\u2013else) is common across many programming languages. Although the syntax varies from language to language, the basic structure (in pseudocode form) looks like this:In the example code above, the part represented by (boolean condition) constitutes a conditional expression, having intrinsic value (e.g., it may be substituted by either of the values True or False) but having no intrinsic meaning. In contrast, the combination of this expression, the If and Then surrounding it, and the consequent that follows afterward constitute a conditional statement, having intrinsic meaning (e.g., expressing a coherent logical rule) but no intrinsic value.When an interpreter finds an If, it expects a boolean condition \u2013 for example, x > 0, which means the variable x contains a number that is greater than zero \u2013 and evaluates that condition. If the condition is true, the statements following the then are executed. Otherwise, the execution continues in the following branch \u2013 either in the else block (which is usually optional), or if there is no else branch, then after the end If.After either branch has been executed, control returns to the point after the end If.In early programming languages, especially some dialects of BASIC in the 1980s home computers, an if\u2013then statement could only contain GOTO statements. This led to a hard-to-read style of programming known as spaghetti programming, with programs in this style called spaghetti code. As a result, structured programming, which allows (virtually) arbitrary statements to be put in statement blocks inside an if statement, gained in popularity, until it became the norm even in most BASIC programming circles. Such mechanisms and principles were based on the older but more advanced ALGOL family of languages, and ALGOL-like languages such as Pascal and Modula-2 influenced modern BASIC variants for many years. While it is possible while using only GOTO statements in if\u2013then statements to write programs that are not spaghetti code and are just as well structured and readable as programs written in a structured programming language, structured programming makes this easier and enforces it. Structured if\u2013then\u2013else statements like the example above are one of the key elements of structured programming, and they are present in most popular high-level programming languages such as C, Java, JavaScript and Visual Basic .A subtlety is that the optional else clause found in many languages means that the context-free grammar is ambiguous, since nested conditionals can be parsed in multiple ways. Specifically,\nif a then if b then s else s2\ncan be parsed as\nif a then (if b then s) else s2\nor\nif a then (if b then s else s2)\ndepending on whether the else is associated with the first if or second if. This is known as the dangling else problem, and is resolved in various ways, depending on the language.Else if[edit]By using else if, it is possible to combine several conditions. Only the statements following the first condition that is found to be true will be executed. All other statements will be skipped. The statements ofelseif, in Ada, is simply syntactic sugar for else followed by if. In Ada, the difference is that only one end if is needed, if one uses elseif instead of else followed by if. This is similar in Perl, which provides the keyword elsif to avoid the large number of braces that would be required by multiple if and else statements and also in Python, which uses the special keyword elif because structure is denoted by indentation rather than braces, so a repeated use of else and if would require increased indentation after every condition. Similarly, the earlier UNIX shells (later gathered up to the POSIX shell syntax[1]) use elif too, but giving the choice of delimiting with spaces, line breaks, or both.However, in many languages more directly descended from Algol, such as Algol68, Simula, Pascal, BCPL and C, this special syntax for the else if construct is not present, nor is it present in the many syntactical derivatives of C, such as Java, ECMA-script, PHP, and so on. This works because in these languages, any single statement (in this case if cond...) can follow a conditional without being enclosed in a block.This design choice has a slight cost in that code else if branch is, effectively, adding an extra nesting level, complicating the job for some compilers (or its implementors), which has to analyse and implement arbitrarily long else if chains recursively.If all terms in the sequence of conditionals are testing the value of a single expression (e.g., if x=0 ... else if x=1 ... else if x=2...), then an alternative is the switch statement, also called case-statement or select-statement. Conversely, in languages that do not have a switch statement, these can be produced by a sequence of else if statements.If\u2013then\u2013else expressions[edit]Many languages support if expressions, which are similar to if statements, but return a value as a result. Thus, they are true expressions (which evaluate to a value), not statements (which changes the program state or perform some kind of action).Algol family[edit]ALGOL 60 and some other members of the ALGOL family allow if\u2013then\u2013else as an expression:\n  myvariable := if x > 10 then 1 else 2\nLisp dialects[edit]In dialects of Lisp \u2013 Scheme, Racket and Common Lisp \u2013 the first of which was inspired to a great extent by ALGOL:Haskell[edit]In Haskell 98, there is only an if expression, no if statement, and the else part is compulsory, as every expression must have some value.[2] Logic that would be expressed with conditionals in other languages is usually expressed with pattern matching in recursive functions.Because Haskell is lazy, it is possible to write control structures, such as if, as ordinary expressions; the lazy evaluation means that an if function can evaluate only the condition and proper branch (where a strict language would evaluate all three). It can be written like this:[3]C-like languages[edit]C and C-like languages have a special ternary operator (?:) for conditional expressions with a function that may be described by a template like this:\ncondition ? evaluated-when-true : evaluated-when-false\nThis means that it can be inlined into expressions, unlike if-statements, in C-like languages:which can be compared to the Algol-family if\u2013then\u2013else expressions (and similar in Ruby and Scala, among others).To accomplish the same using an if-statement, this would take more than one line of code (under typical layout conventions):Some argue that the explicit if/then statement is easier to read and that it may compile to more efficient code than the ternary operator,[4] while others argue that concise expressions are easier to read than statements spread over several lines.In Small Basic[edit]First, when the user runs the program, a cursor appears waiting for the reader to type a number. If that number is greater than 10, the text My variable is named 'foo'. is displayed on the screen. If the number is smaller than 10, then the message My variable is named 'bar'. is printed on the screen.In Visual Basic[edit]In Visual Basic and some other languages, a function called IIf is provided, which can be used as a conditional expression. However, it does not behave like a true conditional expression, because both the true and false branches are always evaluated; it is just that the result of one of them is thrown away, while the result of the other is returned by the IIf function.Arithmetic if[edit]Up to Fortran 77, the language Fortran has an arithmetic if statement which is halfway between a computed IF and a case statement, based on the trichotomy x < 0, x = 0, x > 0. This was the earliest conditional statement in Fortran:[5]Where e is any numeric expression (not necessarily an integer); this is equivalent toBecause this arithmetic IF is equivalent to multiple GOTO statements that could jump to anywhere, it is considered to be an unstructured control statement, and should not be used if more structured statements can be used. In practice it has been observed that most arithmetic IF statements referenced the following statement with one or two of the labels.This was the only conditional control statement in the original implementation of Fortran on the IBM 704 computer. On that computer the test-and-branch op-code had three addresses for those three states. Other computers would have flag registers such as positive, zero, negative, even, overflow, carry, associated with the last arithmetic operations and would use instructions such as 'Branch if accumulator negative' then 'Branch if accumulator zero' or similar. Note that the expression is evaluated once only, and in cases such as integer arithmetic where overflow may occur, the overflow or carry flags would be considered also.Object-oriented implementation in Smalltalk[edit]In contrast to other languages, in Smalltalk the conditional statement is not a language construct but defined in the class Boolean as an abstract method that takes two parameters, both closures. Boolean has two subclasses, True and False, which both define the method, True executing the first closure only, False executing the second closure only.[6]JavaScript[edit]Two examples in JavaScript:Lambda Calculus[edit]In Lambda Calculus, the concept of an if-then-else conditional can be expressed using the expressions:\ntrue = \u03bbx. \u03bby. x\nfalse = \u03bbx. \u03bby. y\nifThenElse = (\u03bbc. \u03bbx. \u03bby. (c x y))\n\ntrue takes up to two arguments and once both are provided(see currying), it returns the first argument given.\nfalse takes up to two arguments and once both are provided(see currying), it returns the second argument given.\nifThenElse takes up to three arguments and once all are provided, it passes both second and third argument to the first argument(which is a function that given two arguments, and produces a result). We expect ifThenElse to only take true or false as an argument, both of which project the given two arguments to their preferred single argument, which is then returned.\nnote: if ifThenElse is passed two functions as the left and right conditionals; it is necessary to also pass an empty tuple () to the result of ifThenElse in order to actually call the chosen function, otherwise ifThenElse will just return the function object without getting called.In a system where numbers can be used without definition(like Lisp, Traditional paper math, so on), the above can be expressed as a single closure below:\n((\u03bbtrue. \u03bbfalse. \u03bbifThenElse.\n    (ifThenElse true 2 3)\n)(\u03bbx. \u03bby. x)(\u03bbx. \u03bby. y)(\u03bbc. \u03bbl. \u03bbr. c l r))\nHere, true, false, and ifThenElse are bound to their respective definitions which are passed to their scope at the end of their block.A working JavaScript analogy(using only functions of single variable for rigor) to this is:\nvar computationResult = ((_true => _false => _ifThenElse => \n    _ifThenElse(_true)(2)(3) \n)(x => y => x)(x => y => y)(c => x => y => c(x)(y)));\nThe code above with multivariable functions looks like this:\nvar computationResult = ((_true, _false, _ifThenElse) =>\n    _ifThenElse(_true, 2, 3)\n)((x, y) => x, (x, y) => y, (c, x, y) => c(x, y));\nanother version of the earlier example without a system where numbers are assumed is below.First example shows the first branch being taken, while second example shows the second branch being taken.\n((\u03bbtrue. \u03bbfalse. \u03bbifThenElse.\n    (ifThenElse true (\u03bbFirstBranch. FirstBranch) (\u03bbSecondBranch. SecondBranch))\n)(\u03bbx. \u03bby. x)(\u03bbx. \u03bby. y)(\u03bbc. \u03bbl. \u03bbr. c l r))\n\n((\u03bbtrue. \u03bbfalse. \u03bbifThenElse.\n    (ifThenElse false (\u03bbFirstBranch. FirstBranch) (\u03bbSecondBranch. SecondBranch))\n)(\u03bbx. \u03bby. x)(\u03bbx. \u03bby. y)(\u03bbc. \u03bbl. \u03bbr. c l r))\nSmalltalk uses a similar idea for its true and false representations, with True and False being singleton objects that respond to messages ifTrue/ifFalse differently.Haskell used to use this exact model for its Boolean type, but at the time of writing, most Haskell programs use syntactic sugar if a then b else c construct which unlike ifThenElse does not compose unlesseither wrapped in another function or re-implemented as shown in The Haskell section of this page.Case and switch statements[edit]Switch statements (in some languages, case statements or multiway branches) compare a given value with specified constants and take action according to the first constant to match. There is usually a provision for a default action ('else','otherwise') to be taken if no match succeeds. Switch statements can allow compiler optimizations, such as lookup tables. In dynamic languages, the cases may not be limited to constant expressions, and might extend to pattern matching, as in the shell script example on the right, where the '*)' implements the default case as a regular expression matching any string.Pattern matching[edit]Pattern matching may be seen as a more sophisticated alternative to both if\u2013then\u2013else, and case statements. It is available in many programming languages with functional programming features, such as Wolfram Language, ML and many others. Here is a simple example written in the OCaml language:The power of pattern matching is the ability to concisely match not only actions but also values to patterns of data. Here is an example written in Haskell which illustrates both of these features:This code defines a function map, which applies the first argument (a function) to each of the elements of the second argument (a list), and returns the resulting list. The two lines are the two definitions of the function for the two kinds of arguments possible in this case \u2013 one where the list is empty (just return an empty list) and the other case where the list is not empty.Pattern matching is not strictly speaking always a choice construct, because it is possible in Haskell to write only one alternative, which is guaranteed to always be matched \u2013 in this situation, it is not being used as a choice construct, but simply as a way to bind names to values. However, it is frequently used as a choice construct in the languages in which it is available.Hash-based conditionals[edit]In programming languages that have associative arrays or comparable data structures, such as Python, Perl, PHP or Objective-C, it is idiomatic to use them to implement conditional assignment.[7]In languages that have anonymous functions or that allow a programmer to assign a named function to a variable reference, conditional flow can be implemented by using a hash as a dispatch table.Predication[edit]An alternative to conditional branch instructions is predication. Predication is an architectural feature that enables instructions to be conditionally executed instead of modifying the control flow.Choice system cross reference[edit]This table refers to the most recent language specification of each language. For languages that do not have a specification, the latest officially released implementation is referred to.\n^ This refers to pattern matching as a distinct conditional construct in the programming language \u2013 as opposed to mere string pattern matching support, such as regular expression support.\n1 2 3 4 5 The often-encountered else if in the C family of languages, and in COBOL and Haskell, is not a language feature but a set of nested and independent if then else statements combined with a particular source code layout. However, this also means that a distinct else\u2013if construct is not really needed in these languages.\n1 2 In Haskell and F#, a separate constant choice construct is unneeded, because the same task can be done with pattern matching.\n^ In a Ruby case construct, regular expression matching is among the conditional flow-control alternatives available. For an example, see this Stack Overflow question.\n1 2 SQL has two similar constructs that fulfill both roles, both introduced in SQL-92. A searched CASE expression CASE WHEN cond1 THEN expr1 WHEN cond2 THEN expr2 [...] ELSE exprDflt END works like if ... else if ... else, whereas a simple CASE expression: CASE expr WHEN val1 THEN expr1 [...] ELSE exprDflt END works like a switch statement. For details and examples see Case (SQL).\n^ Arithmetic if is obsolescent in Fortran 90.\nSee also[edit]\nBranch (computer science)\nConditional compilation\nDynamic dispatch for another way to make execution choices\nMcCarthy Formalism for history and historical references\nNamed condition\nTest (Unix)\nYoda conditions\nConditional move\nReferences[edit]External links[edit]\nIF NOT (ActionScript 3.0) video\n", "subtitles": ["If\u2013then(\u2013else)", "Lambda Calculus", "Case and switch statements", "Pattern matching", "Hash-based conditionals", "Predication", "Choice system cross reference", "See also", "References", "External links"], "title": "Conditional (computer programming)"},
{"content": "In computer science, an invariant is a condition that can be relied upon to be true during execution of a program, or during some portion of it. It is a logical assertion that is held to always be true during a certain phase of execution. For example, a loop invariant is a condition that is true at the beginning and end of every execution of a loop.Use[edit]Invariants are especially useful when reasoning about whether a computer program is correct. The theory of optimizing compilers, the methodology of design by contract, and formal methods for determining program correctness, all rely heavily on invariants.Programmers often use assertions in their code to make invariants explicit. Some object oriented programming languages have a special syntax for specifying class invariants.Example[edit]The MU puzzle is a good example of a logical problem where determining an invariant is useful. The puzzle asks one to start with the word MI and transform it into the word MU using in each step one of the following transformation rules:\nIf a string ends with an I, a U may be appended (xI \u2192 xIU)\nThe string after the M may be completely duplicated (Mx \u2192 Mxx)\nAny three consecutive I's (III) may be replaced with a single U (xIIIy \u2192 xUy)\nAny two consecutive U's may be removed (xUUy \u2192 xy)\nAn example derivation (superscripts indicating the applied rules) is\nMI \u21922 MII \u21922 MIIII \u21923 MUI \u21922 MUIUI \u21921 MUIUIU \u21922 MUIUIUUIUIU \u21924 MUIUIIUIU \u2192 ...\nIs it possible to convert MI into MU using these four transformation rules only?One could spend many hours applying these transformation rules to strings. However, it might be quicker to find a property that is invariant to all rules (i.e. that isn't changed by any of them), and demonstrates that getting to MU is impossible. Logically looking at the puzzle, the only way to get rid of any I's is to have three consecutive I's in the string. This makes the following invariant interesting to consider:\nThe number of I's in the string is not a multiple of 3.\nThis is an invariant to the problem if for each of the transformation rules the following holds: if the invariant held before applying the rule, it will also hold after applying it. If we look at the net effect of applying the rules on the number of I's and U's we can see this actually is the case for all rules:\n\n\n\nRule\n#I's\n#U's\nEffect on invariant\n\n\n1\n+0\n+1\nNumber of I's is unchanged. If the invariant held, it still does.\n\n\n2\n\u00d72\n\u00d72\nIf n is not a multiple of 3, then 2\u00d7n isn't either. The invariant still holds.\n\n\n3\n\u22123\n+1\nIf n is not a multiple of 3, n\u22123 isn't either. The invariant still holds.\n\n\n4\n+0\n\u22122\nNumber of I's is unchanged. If the invariant held, it still does.\n\n\n\nThe table above shows clearly that the invariant holds for each of the possible transformation rules, which basically means that whichever rule we pick, at whatever state, if the number of I's was not a multiple of three before applying the rule, it won't be afterwards either.Given that there is a single I in the starting string MI, and one is not a multiple of three, it's impossible to go from MI to MU as zero is a multiple of three.Automatic invariant detection in imperative programs[edit]Abstract interpretation tools can compute simple invariants of given imperative computer programs. The kind of properties that can be found depend on the abstract domains used. Typical example properties are single integer variable ranges like 0<=x<1024, relations between several variables like 0<=i-j<2*n-1, and modulus information like y%4==0. Academic research prototypes also consider simple properties of pointer structures.[1]Any more sophisticated invariants still have to be provided manually. In particular, when verifying an imperative program using the Hoare calculus,[2] a loop invariant has to be provided manually for each loop in the program, which is one of the reasons for that task being extremely tedious.In the above example, no tool will be able to detect from rules 1-4 that a derivation MI \u2192...\u2192 MU is impossible. However, once the abstraction from the string to the number of its Is has been made by hand, leading e.g. to the following C program, an abstract interpretation tool will be able to detect that ICount%3 can't be 0 and hence the while-loop will never terminate.See also[edit]\nAssertion (software development)\nClass invariant\nConst correctness\nDesign by contract\nHoare logic\nLoop invariant\nInvariant-based programming\nReferences[edit]\nJ.D. Fokker, H. Zantema, S.D. Swierstra (1991). Iteratie en invariatie, Programmeren en Correctheid. Academic Service. ISBN 90-6233-681-7.\nExternal links[edit]\nApplet: Visual Invariants in Sorting Algorithms by William Braynen in 1997\n", "subtitles": ["Use", "Example", "Automatic invariant detection in imperative programs", "See also", "References", "External links"], "title": "Invariant (computer science)"},
{"content": "In computer science, the syntax of a computer language is the set of rules that defines the combinations of symbols that are considered to be a correctly structured document or fragment in that language. This applies both to programming languages, where the document represents source code, and markup languages, where the document represents data. The syntax of a language defines its surface form.[1] Text-based computer languages are based on sequences of characters, while visual programming languages are based on the spatial layout and connections between symbols (which may be textual or graphical). Documents that are syntactically invalid are said to have a syntax error.Syntax \u2013 the form \u2013 is contrasted with semantics \u2013 the meaning. In processing computer languages, semantic processing generally comes after syntactic processing, but in some cases semantic processing is necessary for complete syntactic analysis, and these are done together or concurrently. In a compiler, the syntactic analysis comprises the frontend, while semantic analysis comprises the backend (and middle end, if this phase is distinguished).Levels of syntax[edit]Computer language syntax is generally distinguished into three levels:\nWords \u2013 the lexical level, determining how characters form tokens;\nPhrases \u2013 the grammar level, narrowly speaking, determining how tokens form phrases;\nContext \u2013 determining what objects or variables names refer to, if types are valid, etc.\nDistinguishing in this way yields modularity, allowing each level to be described and processed separately, and often independently. First a lexer turns the linear sequence of characters into a linear sequence of tokens; this is known as lexical analysis or lexing. Second the parser turns the linear sequence of tokens into a hierarchical syntax tree; this is known as parsing narrowly speaking. Thirdly the contextual analysis resolves names and checks types. This modularity is sometimes possible, but in many real-world languages an earlier step depends on a later step \u2013 for example, the lexer hack in C is because tokenization depends on context. Even in these cases, syntactical analysis is often seen as approximating this ideal model.The parsing stage itself can be divided into two parts: the parse tree or concrete syntax tree which is determined by the grammar, but is generally far too detailed for practical use, and the abstract syntax tree (AST), which simplifies this into a usable form. The AST and contextual analysis steps can be considered a form of semantic analysis, as they are adding meaning and interpretation to the syntax, or alternatively as informal, manual implementations of syntactical rules that would be difficult or awkward to describe or implement formally.The levels generally correspond to levels in the Chomsky hierarchy. Words are in a regular language, specified in the lexical grammar, which is a Type-3 grammar, generally given as regular expressions. Phrases are in a context-free language (CFL), generally a deterministic context-free language (DCFL), specified in a phrase structure grammar, which is a Type-2 grammar, generally given as production rules in Backus\u2013Naur form (BNF). Phrase grammars are often specified in much more constrained grammars than full context-free grammars, in order to make them easier to parse; while the LR parser can parse any DCFL in linear time, the simple LALR parser and even simpler LL parser are more efficient, but can only parse grammars whose production rules are constrained. In principle, contextual structure can be described by a context-sensitive grammar, and automatically analyzed by means such as attribute grammars, though in general this step is done manually, via name resolution rules and type checking, and implemented via a symbol table which stores names and types for each scope.Tools have been written that automatically generate a lexer from a lexical specification written in regular expressions and a parser from the phrase grammar written in BNF: this allows one to use declarative programming, rather than need to have procedural or functional programming. A notable example is the lex-yacc pair. These automatically produce a concrete syntax tree; the parser writer must then manually write code describing how this is converted to an abstract syntax tree. Contextual analysis is also generally implemented manually. Despite the existence of these automatic tools, parsing is often implemented manually, for various reasons \u2013 perhaps the phrase structure is not context-free, or an alternative implementation improves performance or error-reporting, or allows the grammar to be changed more easily. Parsers are often written in functional languages, such as Haskell, or in scripting languages, such as Python or Perl, or in C or C++.Examples of errors[edit]As an example, (add 1 1) is a syntactically valid Lisp program (assuming the 'add' function exists, else name resolution fails), adding 1 and 1. However, the following are invalid:\n(_ 1 1)    lexical error: '_' is not valid\n(add 1 1   parsing error: missing closing ')'\nNote that the lexer is unable to identify the first error \u2013 all it knows is that, after producing the token LEFT_PAREN, '(' the remainder of the program is invalid, since no word rule begins with '_'. The second error is detected at the parsing stage: The parser has identified the list production rule due to the '(' token (as the only match), and thus can give an error message; in general it may be ambiguous.Type errors and undeclared variable errors are sometimes considered to be syntax errors when they are detected at compile-time (which is usually the case when compiling strongly-typed languages), though it is common to classify these kinds of error as semantic errors instead.[2][3][4]As an example, the Python code\n'a' + 1\ncontains a type error because it adds a string literal to an integer literal. Type errors of this kind can be detected at compile-time: They can be detected during parsing (phrase analysis) if the compiler uses separate rules that allow integerLiteral + integerLiteral but not stringLiteral + integerLiteral, though it is more likely that the compiler will use a parsing rule that allows all expressions of the form LiteralOrIdentifier + LiteralOrIdentifier and then the error will be detected during contextual analysis (when type checking occurs). In some cases this validation is not done by the compiler, and these errors are only detected at runtime.In a dynamically typed language, where type can only be determined at runtime, many type errors can only be detected at runtime. For example, the Python code\na + b\nis syntactically valid at the phrase level, but the correctness of the types of a and b can only be determined at runtime, as variables do not have types in Python, only values do. Whereas there is disagreement about whether a type error detected by the compiler should be called a syntax error (rather than a static semantic error), type errors which can only be detected at program execution time are always regarded as semantic rather than syntax errors.Syntax definition[edit]The syntax of textual programming languages is usually defined using a combination of regular expressions (for lexical structure) and Backus\u2013Naur form (for grammatical structure) to inductively specify syntactic categories (nonterminals) and terminal symbols. Syntactic categories are defined by rules called productions, which specify the values that belong to a particular syntactic category.[1] Terminal symbols are the concrete characters or strings of characters (for example keywords such as define, if, let, or void) from which syntactically valid programs are constructed.A language can have different equivalent grammars, such as equivalent regular expressions (at the lexical levels), or different phrase rules which generate the same language. Using a broader category of grammars, such as LR grammars, can allow shorter or simpler grammars compared with more restricted categories, such as LL grammar, which may require longer grammars with more rules. Different but equivalent phrase grammars yield different parse trees, though the underlying language (set of valid documents) is the same.Example: Lisp S-expressions[edit]Below is a simple grammar, defined using the notation of regular expressions and Extended Backus\u2013Naur form. It describes the syntax of S-expressions, a data syntax of the programming language Lisp, which defines productions for the syntactic categories expression, atom, number, symbol, and list:This grammar specifies the following:\nan expression is either an atom or a list;\nan atom is either a number or a symbol;\na number is an unbroken sequence of one or more decimal digits, optionally preceded by a plus or minus sign;\na symbol is a letter followed by zero or more of any characters (excluding whitespace); and\na list is a matched pair of parentheses, with zero or more expressions inside it.\nHere the decimal digits, upper- and lower-case characters, and parentheses are terminal symbols.The following are examples of well-formed token sequences in this grammar: '12345', '()', '(a b c232 (1))'Complex grammars[edit]The grammar needed to specify a programming language can be classified by its position in the Chomsky hierarchy. The phrase grammar of most programming languages can be specified using a Type-2 grammar, i.e., they are context-free grammars,[5] though the overall syntax is context-sensitive (due to variable declarations and nested scopes), hence Type-1. However, there are exceptions, and for some languages the phrase grammar is Type-0 (Turing-complete).In some languages like Perl and Lisp the specification (or implementation) of the language allows constructs that execute during the parsing phase. Furthermore, these languages have constructs that allow the programmer to alter the behavior of the parser. This combination effectively blurs the distinction between parsing and execution, and makes syntax analysis an undecidable problem in these languages, meaning that the parsing phase may not finish. For example, in Perl it is possible to execute code during parsing using a BEGIN statement, and Perl function prototypes may alter the syntactic interpretation, and possibly even the syntactic validity of the remaining code.[6] Colloquially this is referred to as only Perl can parse Perl (because code must be executed during parsing, and can modify the grammar), or more strongly even Perl cannot parse Perl (because it is undecidable). Similarly, Lisp macros introduced by the defmacro syntax also execute during parsing, meaning that a Lisp compiler must have an entire Lisp run-time system present. In contrast, C macros are merely string replacements, and do not require code execution.[7][8]Syntax versus semantics[edit]The syntax of a language describes the form of a valid program, but does not provide any information about the meaning of the program or the results of executing that program. The meaning given to a combination of symbols is handled by semantics (either formal or hard-coded in a reference implementation). Not all syntactically correct programs are semantically correct. Many syntactically correct programs are nonetheless ill-formed, per the language's rules; and may (depending on the language specification and the soundness of the implementation) result in an error on translation or execution. In some cases, such programs may exhibit undefined behavior. Even when a program is well-defined within a language, it may still have a meaning that is not intended by the person who wrote it.Using natural language as an example, it may not be possible to assign a meaning to a grammatically correct sentence or the sentence may be false:\nColorless green ideas sleep furiously. is grammatically well formed but has no generally accepted meaning.\nJohn is a married bachelor. is grammatically well formed but expresses a meaning that cannot be true.\nThe following C language fragment is syntactically correct, but performs an operation that is not semantically defined (because p is a null pointer, the operations p->real and p->im have no meaning):As a simpler example,is syntactically valid, but not semantically defined, as it uses an uninitialized variable. Even though compilers for some programming languages (e.g., Java and C#) would detect uninitialized variable errors of this kind, they should be regarded as semantic errors rather than syntax errors.[4][9]See also[edit]To quickly compare syntax of various programming languages, take a look at the list of Hello, World! program examples:\nProlog syntax and semantics\nPerl syntax\nPHP syntax and semantics\nC syntax\nC++ syntax\nJava syntax\nJavaScript syntax\nPython syntax and semantics\nLua syntax\nHaskell syntax\nReferences[edit]External links[edit]\nVarious syntactic constructs used in computer programming languages\n", "subtitles": ["Levels of syntax", "Syntax definition", "Syntax versus semantics", "See also", "References", "External links"], "title": "Syntax (programming languages)"},
{"content": "In mathematical logic and computer science, the \u03bc-recursive functions are a class of partial functions from natural numbers to natural numbers that are computable in an intuitive sense. In computability theory, it is shown that the \u03bc-recursive functions are precisely the functions that can be computed by Turing machines. The \u03bc-recursive functions are closely related to primitive recursive functions, and their inductive definition (below) builds upon that of the primitive recursive functions. However, not every \u03bc-recursive function is a primitive recursive function\u2014the most famous example is the Ackermann function.Other equivalent classes of functions are the \u03bb-recursive functions and the functions that can be computed by Markov algorithms.The set of all recursive functions is known as R in computational complexity theory.Definition[edit]The \u03bc-recursive functions (or partial \u03bc-recursive functions) are partial functions that take finite tuples of natural numbers and return a single natural number. They are the smallest class of partial functions that includes the initial functions and is closed under composition, primitive recursion, and the \u03bc operator.The smallest class of functions including the initial functions and closed under composition and primitive recursion (i.e. without minimisation) is the class of primitive recursive functions. While all primitive recursive functions are total, this is not true of partial recursive functions; for example, the minimisation of the successor function is undefined. The primitive recursive functions are a subset of the total recursive functions, which are a subset of the partial recursive functions. For example, the Ackermann function can be proven to be total recursive, but not primitive.Initial or basic functions: (In the following the subscripting is per Kleene (1952) p. 219. For more about some of the various symbolisms found in the literature see Symbolism below.)\nConstant function: For each natural number \n  \n    \n      \n        n\n        \n      \n    \n    {\\displaystyle n\\,}\n  \n and every \n  \n    \n      \n        k\n        \n      \n    \n    {\\displaystyle k\\,}\n  \n:\n\n\n  \n    \n      \n        f\n        (\n        \n          x\n          \n            1\n          \n        \n        ,\n        ...\n        ,\n        \n          x\n          \n            k\n          \n        \n        )\n        =\n        n\n        \n      \n    \n    {\\displaystyle f(x_{1},\\ldots ,x_{k})=n\\,}\n  \n.\nAlternative definitions use compositions of the successor function and use a zero function, that always returns zero, in place of the constant function.\n\n\nSuccessor function S:\n\n\n  \n    \n      \n        S\n        (\n        x\n        )\n        \n          \n            \n              \n                =\n              \n              \n                \n                  d\n                  e\n                  f\n                \n              \n            \n          \n        \n        x\n        +\n        1\n        \n      \n    \n    {\\displaystyle S(x){\\stackrel {\\mathrm {def} }{=}}x+1\\,}\n  \n\n\n\nProjection function \n  \n    \n      \n        \n          P\n          \n            i\n          \n          \n            k\n          \n        \n      \n    \n    {\\displaystyle P_{i}^{k}}\n  \n (also called the Identity function \n  \n    \n      \n        \n          I\n          \n            i\n          \n          \n            k\n          \n        \n      \n    \n    {\\displaystyle I_{i}^{k}}\n  \n): For all natural numbers \n  \n    \n      \n        i\n        ,\n        k\n        \n      \n    \n    {\\displaystyle i,k\\,}\n  \n such that \n  \n    \n      \n        1\n      \n    \n    {\\displaystyle 1}\n  \n \u2264 \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n \u2264 \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n  \n:\n\n\n  \n    \n      \n        \n          P\n          \n            i\n          \n          \n            k\n          \n        \n        (\n        \n          x\n          \n            1\n          \n        \n        ,\n        ...\n        ,\n        \n          x\n          \n            k\n          \n        \n        )\n        \n          \n            \n              \n                =\n              \n              \n                \n                  d\n                  e\n                  f\n                \n              \n            \n          \n        \n        \n          x\n          \n            i\n          \n        \n        \n        .\n      \n    \n    {\\displaystyle P_{i}^{k}(x_{1},\\ldots ,x_{k}){\\stackrel {\\mathrm {def} }{=}}x_{i}\\,.}\n  \n\n\n\nOperators:\nComposition operator \n  \n    \n      \n        \u2218\n        \n      \n    \n    {\\displaystyle \\circ \\,}\n  \n (also called the substitution operator): Given an m-ary function \n  \n    \n      \n        h\n        (\n        \n          x\n          \n            1\n          \n        \n        ,\n        ...\n        ,\n        \n          x\n          \n            m\n          \n        \n        )\n        \n      \n    \n    {\\displaystyle h(x_{1},\\ldots ,x_{m})\\,}\n  \n and m k-ary functions \n  \n    \n      \n        \n          g\n          \n            1\n          \n        \n        (\n        \n          x\n          \n            1\n          \n        \n        ,\n        ...\n        ,\n        \n          x\n          \n            k\n          \n        \n        )\n        ,\n        ...\n        ,\n        \n          g\n          \n            m\n          \n        \n        (\n        \n          x\n          \n            1\n          \n        \n        ,\n        ...\n        ,\n        \n          x\n          \n            k\n          \n        \n        )\n      \n    \n    {\\displaystyle g_{1}(x_{1},\\ldots ,x_{k}),\\ldots ,g_{m}(x_{1},\\ldots ,x_{k})}\n  \n:\n\n\n  \n    \n      \n        h\n        \u2218\n        (\n        \n          g\n          \n            1\n          \n        \n        ,\n        ...\n        ,\n        \n          g\n          \n            m\n          \n        \n        )\n        \n          \n            \n              \n                =\n              \n              \n                \n                  d\n                  e\n                  f\n                \n              \n            \n          \n        \n        f\n        \n        \n          where\n        \n        \n        f\n        (\n        \n          x\n          \n            1\n          \n        \n        ,\n        ...\n        ,\n        \n          x\n          \n            k\n          \n        \n        )\n        =\n        h\n        (\n        \n          g\n          \n            1\n          \n        \n        (\n        \n          x\n          \n            1\n          \n        \n        ,\n        ...\n        ,\n        \n          x\n          \n            k\n          \n        \n        )\n        ,\n        ...\n        ,\n        \n          g\n          \n            m\n          \n        \n        (\n        \n          x\n          \n            1\n          \n        \n        ,\n        ...\n        ,\n        \n          x\n          \n            k\n          \n        \n        )\n        )\n        \n        .\n      \n    \n    {\\displaystyle h\\circ (g_{1},\\ldots ,g_{m}){\\stackrel {\\mathrm {def} }{=}}f\\quad {\\text{where}}\\quad f(x_{1},\\ldots ,x_{k})=h(g_{1}(x_{1},\\ldots ,x_{k}),\\ldots ,g_{m}(x_{1},\\ldots ,x_{k}))\\,.}\n  \n\n\n\nPrimitive recursion operator \n  \n    \n      \n        \u03c1\n        \n      \n    \n    {\\displaystyle \\rho \\,}\n  \n: Given the k-ary function \n  \n    \n      \n        g\n        (\n        \n          x\n          \n            1\n          \n        \n        ,\n        ...\n        ,\n        \n          x\n          \n            k\n          \n        \n        )\n        \n      \n    \n    {\\displaystyle g(x_{1},\\ldots ,x_{k})\\,}\n  \n and k+2 -ary function \n  \n    \n      \n        h\n        (\n        y\n        ,\n        z\n        ,\n        \n          x\n          \n            1\n          \n        \n        ,\n        ...\n        ,\n        \n          x\n          \n            k\n          \n        \n        )\n        \n      \n    \n    {\\displaystyle h(y,z,x_{1},\\ldots ,x_{k})\\,}\n  \n:\n\n\n  \n    \n      \n        \n          \n            \n              \n                \u03c1\n                (\n                g\n                ,\n                h\n                )\n              \n              \n                \n                \n                  \n                    \n                      \n                        =\n                      \n                      \n                        \n                          d\n                          e\n                          f\n                        \n                      \n                    \n                  \n                \n                f\n                \n                \n                  where\n                \n              \n            \n            \n              \n                f\n                (\n                0\n                ,\n                \n                  x\n                  \n                    1\n                  \n                \n                ,\n                ...\n                ,\n                \n                  x\n                  \n                    k\n                  \n                \n                )\n              \n              \n                \n                =\n                g\n                (\n                \n                  x\n                  \n                    1\n                  \n                \n                ,\n                ...\n                ,\n                \n                  x\n                  \n                    k\n                  \n                \n                )\n              \n            \n            \n              \n                f\n                (\n                y\n                +\n                1\n                ,\n                \n                  x\n                  \n                    1\n                  \n                \n                ,\n                ...\n                ,\n                \n                  x\n                  \n                    k\n                  \n                \n                )\n              \n              \n                \n                =\n                h\n                (\n                y\n                ,\n                f\n                (\n                y\n                ,\n                \n                  x\n                  \n                    1\n                  \n                \n                ,\n                ...\n                ,\n                \n                  x\n                  \n                    k\n                  \n                \n                )\n                ,\n                \n                  x\n                  \n                    1\n                  \n                \n                ,\n                ...\n                ,\n                \n                  x\n                  \n                    k\n                  \n                \n                )\n                \n                .\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}\\rho (g,h)&{\\stackrel {\\mathrm {def} }{=}}f\\quad {\\text{where}}\\\\f(0,x_{1},\\ldots ,x_{k})&=g(x_{1},\\ldots ,x_{k})\\\\f(y+1,x_{1},\\ldots ,x_{k})&=h(y,f(y,x_{1},\\ldots ,x_{k}),x_{1},\\ldots ,x_{k})\\,.\\end{aligned}}}\n  \n\n\n\nMinimisation operator \n  \n    \n      \n        \u03bc\n        \n      \n    \n    {\\displaystyle \\mu \\,}\n  \n: Given a (k+1)-ary total function \n  \n    \n      \n        f\n        (\n        y\n        ,\n        \n          x\n          \n            1\n          \n        \n        ,\n        ...\n        ,\n        \n          x\n          \n            k\n          \n        \n        )\n        \n      \n    \n    {\\displaystyle f(y,x_{1},\\ldots ,x_{k})\\,}\n  \n:\n\n\n  \n    \n      \n        \n          \n            \n              \n                \u03bc\n                (\n                f\n                )\n                (\n                \n                  x\n                  \n                    1\n                  \n                \n                ,\n                ...\n                ,\n                \n                  x\n                  \n                    k\n                  \n                \n                )\n                =\n                z\n                \n                  \n                    \n                      \n                        \n                        \u27fa\n                        \n                      \n                      \n                        \n                          d\n                          e\n                          f\n                        \n                      \n                    \n                  \n                \n                 \n                f\n                (\n                z\n                ,\n                \n                  x\n                  \n                    1\n                  \n                \n                ,\n                ...\n                ,\n                \n                  x\n                  \n                    k\n                  \n                \n                )\n              \n              \n                \n                =\n                0\n                \n                \n                  and\n                \n              \n            \n            \n              \n                f\n                (\n                i\n                ,\n                \n                  x\n                  \n                    1\n                  \n                \n                ,\n                ...\n                ,\n                \n                  x\n                  \n                    k\n                  \n                \n                )\n              \n              \n                \n                >\n                0\n                \n                \n                  for\n                \n                \n                i\n                =\n                0\n                ,\n                ...\n                ,\n                z\n                \u2212\n                1.\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}\\mu (f)(x_{1},\\ldots ,x_{k})=z{\\stackrel {\\mathrm {def} }{\\iff }}\\ f(z,x_{1},\\ldots ,x_{k})&=0\\quad {\\text{and}}\\\\f(i,x_{1},\\ldots ,x_{k})&>0\\quad {\\text{for}}\\quad i=0,\\ldots ,z-1.\\end{aligned}}}\n  \n\nIntuitively, minimisation seeks\u2014beginning the search from 0 and proceeding upwards\u2014the smallest argument that causes the function to return zero; if there is no such argument, the search never terminates.\n\n\nThe strong equality operator \n  \n    \n      \n        \u2243\n      \n    \n    {\\displaystyle \\simeq }\n  \n can be used to compare partial \u03bc-recursive functions. This is defined for all partial functions f and g so that\n\n  \n    \n      \n        f\n        (\n        \n          x\n          \n            1\n          \n        \n        ,\n        ...\n        ,\n        \n          x\n          \n            k\n          \n        \n        )\n        \u2243\n        g\n        (\n        \n          x\n          \n            1\n          \n        \n        ,\n        ...\n        ,\n        \n          x\n          \n            l\n          \n        \n        )\n      \n    \n    {\\displaystyle f(x_{1},\\ldots ,x_{k})\\simeq g(x_{1},\\ldots ,x_{l})}\n  \n\nholds if and only if for any choice of arguments either both functions are defined and their values are equal or both functions are undefined.Equivalence with other models of computability[edit]In the equivalence of models of computability, a parallel is drawn between Turing machines that do not terminate for certain inputs and an undefined result for that input in the corresponding partial recursive function. The unbounded search operator is not definable by the rules of primitive recursion as those do not provide a mechanism for infinite loops (undefined values).Normal form theorem[edit]A normal form theorem due to Kleene says that for each k there are primitive recursive functions \n  \n    \n      \n        U\n        (\n        y\n        )\n        \n      \n    \n    {\\displaystyle U(y)\\!}\n  \n and \n  \n    \n      \n        T\n        (\n        y\n        ,\n        e\n        ,\n        \n          x\n          \n            1\n          \n        \n        ,\n        ...\n        ,\n        \n          x\n          \n            k\n          \n        \n        )\n        \n      \n    \n    {\\displaystyle T(y,e,x_{1},\\ldots ,x_{k})\\!}\n  \n such that for any \u03bc-recursive function \n  \n    \n      \n        f\n        (\n        \n          x\n          \n            1\n          \n        \n        ,\n        ...\n        ,\n        \n          x\n          \n            k\n          \n        \n        )\n        \n      \n    \n    {\\displaystyle f(x_{1},\\ldots ,x_{k})\\!}\n  \n with k free variables there is an e such that\n\n  \n    \n      \n        f\n        (\n        \n          x\n          \n            1\n          \n        \n        ,\n        ...\n        ,\n        \n          x\n          \n            k\n          \n        \n        )\n        \u2243\n        U\n        (\n        \u03bc\n        y\n        \n        T\n        (\n        y\n        ,\n        e\n        ,\n        \n          x\n          \n            1\n          \n        \n        ,\n        ...\n        ,\n        \n          x\n          \n            k\n          \n        \n        )\n        )\n      \n    \n    {\\displaystyle f(x_{1},\\ldots ,x_{k})\\simeq U(\\mu y\\,T(y,e,x_{1},\\ldots ,x_{k}))}\n  \n.\nThe number e is called an index or Go\u0308del number for the function f. A consequence of this result is that any \u03bc-recursive function can be defined using a single instance of the \u03bc operator applied to a (total) primitive recursive function.Minsky (1967) observes (as does Boolos-Burgess-Jeffrey (2002) pp. 94\u201395) that the U defined above is in essence the \u03bc-recursive equivalent of the universal Turing machine:\nTo construct U is to write down the definition of a general-recursive function U(n, x) that correctly interprets the number n and computes the appropriate function of x. to construct U directly would involve essentially the same amount of effort, and essentially the same ideas, as we have invested in constructing the universal Turing machine. (italics in original, Minsky (1967) p. 189)\nSymbolism[edit]A number of different symbolisms are used in the literature. An advantage to using the symbolism is a derivation of a function by nesting of the operators one inside the other is easier to write in a compact form. In the following we will abbreviate the string of parameters x1, ..., xn as x:\nConstant function: Kleene uses  Cqn(x) = q  and Boolos-Burgess-Jeffrey (2002) (B-B-J) use the abbreviation  constn( x) = n :\n\n\n\ne.g. C137 ( r, s, t, u, v, w, x ) = 13\ne.g. const13 ( r, s, t, u, v, w, x ) = 13\n\n\n\nSuccessor function: Kleene uses x' and S for Successor. As successor is considered to be primitive, most texts use the apostrophe as follows:\n\n\n\nS(a) = a +1 =def a', where 1 =def 0', 2 =def 0 ' ', etc.\n\n\n\nIdentity function: Kleene (1952) uses  Uin  to indicate the identity function over the variables xi; B-B-J use the identity function idin over the variables x1 to xn:\n\nUin( x ) = idin( x ) = xi\ne.g. U37 = id37 ( r, s, t, u, v, w, x ) = t\n\nComposition (Substitution) operator: Kleene uses a bold-face Snm (not to be confused with his S for successor ! ). The superscript m refers to the mth of function fm, whereas the subscript n refers to the nth variable xn:\n\nIf we are given h( x )= g( f1(x), ... , fm(x) )\n\nh(x) = Smn(g, f1, ... , fm )\n\n\n\nIn a similar manner, but without the sub- and superscripts, B-B-J write:\n\nh(x')= Cn[g, f1 ,..., fm](x)\n\n\n\nPrimitive Recursion: Kleene uses the symbol  Rn(base step, induction step)  where n indicates the number of variables, B-B-J use  Pr(base step, induction step)(x). Given:\n\n\n\nbase step: h( 0, x )= f( x ), and\ninduction step: h( y+1, x ) = g( y, h(y, x),x )\n\n\n\nExample: primitive recursion definition of a + b:\n\n\n\nbase step: f( 0, a ) = a = U11(a)\ninduction step: f( b' , a ) = ( f ( b, a ) )' = g( b, f( b, a), a ) = g( b, c, a ) = c' = S(U23( b, c, a ))\n\n\nR2 { U11(a), S [ (U23( b, c, a ) ] }\nPr{ U11(a), S[ (U23( b, c, a ) ] }\n\n\n\n\nExample: Kleene gives an example of how to perform the recursive derivation of f(b, a) = b + a (notice reversal of variables a and b). He starts with 3 initial functions\n\n\nS(a) = a'\nU11(a) = a\nU23( b, c, a ) = c\ng(b, c, a) = S(U23( b, c, a )) = c'\nbase step: h( 0, a ) = U11(a)\n\n\ninduction step: h( b', a ) = g( b, h( b, a ), a )\n\n\nHe arrives at:\n\n\na+b = R2[ U11, S13(S, U23) ]\n\n\nExamples[edit]\nFibonacci number\nMcCarthy 91 function\nSee also[edit]\nRecursion theory\nRecursion\nRecursion (computer science)\nReferences[edit]\nStephen Kleene (1952) Introduction to Metamathematics. Walters-Noordhoff & North-Holland, with corrections (6th imprint 1971); Tenth impression 1991, ISBN 0-7204-2103-9.\nSoare, R. Recursively enumerable sets and degrees. Springer-Verlag 1987.\nMarvin L. Minsky (1967), Computation: Finite and Infinite Machines, Prentice-Hall, Inc. Englewood Cliffs, N.J.\n\nOn pages 210-215 Minsky shows how to create the \u03bc-operator using the register machine model, thus demonstrating its equivalence to the general recursive functions.\n\nGeorge Boolos, John Burgess, Richard Jeffrey (2002), Computability and Logic: Fourth Edition, Cambridge University Press, Cambridge, UK. Cf pp. 70\u201371.\nExternal links[edit]\nStanford Encyclopedia of Philosophy entry\nA compiler for transforming a recursive function into an equivalent Turing machine\n", "subtitles": ["Definition", "Equivalence with other models of computability", "Normal form theorem", "Symbolism", "Examples", "See also", "References", "External links"], "title": "\u03bc-recursive function"},
{"content": "In computer science, Backus\u2013Naur form or Backus normal form (BNF) is a notation technique for context-free grammars, often used to describe the syntax of languages used in computing, such as computer programming languages, document formats, instruction sets and communication protocols. They are applied wherever exact descriptions of languages are needed: for instance, in official language specifications, in manuals, and in textbooks on programming language theory.Many extensions and variants of the original Backus\u2013Naur notation are used; some are exactly defined, including extended Backus\u2013Naur form (EBNF) and augmented Backus\u2013Naur form (ABNF).History[edit]The idea of describing the structure of language using rewriting rules can be traced back to at least the work of Pa\u0304n\u0323ini (ancient Indian Sanskrit grammarian and a revered scholar in Hinduism who lived sometime between the 7th and 4th century BCE).[1][2] His notation to describe Sanskrit word structure notation is equivalent in power to that of Backus and has many similar properties.In Western society, grammar was long regarded as a subject for teaching, rather than scientific study; descriptions were informal and targeted at practical usage. In the first half of the 20th century, linguists such as Leonard Bloomfield and Zellig Harris started attempts to formalize the description of language, including phrase structure.Meanwhile, string rewriting rules as formal, abstract systems were introduced and studied by mathematicians such as Axel Thue (in 1914), Emil Post (1920s\u201340s) and Alan Turing (1936). Noam Chomsky, teaching linguistics to students of information theory at MIT, combined linguistics and mathematics by taking what is essentially Thue's formalism as the basis for the description of the syntax of natural language. He also introduced a clear distinction between generative rules (those of context-free grammars) and transformation rules (1956).[3][4]John Backus, a programming language designer at IBM, proposed a metalanguage of metalinguistic formulas[5][6][7] to describe the syntax of the new programming language IAL, known today as ALGOL 58 (1959). His notation was first used in the ALGOL 60 report.BNF is a notation for Chomsky's context-free grammars. Apparently, Backus was familiar with Chomsky's work.[8]As proposed by Backus, the formula defined classes whose names are enclosed in angle brackets. For example, <ab>. Each of these names denotes a class of basic symbols.[5]Further development of ALGOL led to ALGOL 60. In the committee's 1963 report, Peter Naur called Backus's notation Backus normal form. Donald Knuth argued that BNF should rather be read as Backus\u2013Naur form, as it is not a normal form in the conventional sense,[9] unlike, for instance, Chomsky normal form. The name Pa\u0304n\u0323ini Backus form was also once suggested in view of the fact that the expansion Backus normal form may not be accurate, and that Pa\u0304n\u0323ini had independently developed a similar notation earlier. [10]BNF, as described by Peter Naur in the ALGOL 60 report is metalinguistic formula. Sequences of characters enclosed in the brackets <> represent metalinguistic variables whose values are sequences of symbols. The marks ::= and | (the latter with the meaning of or) are metalinguistic connectives. Any mark in a formula, which is not a variable or a connective, denotes itself. Juxtaposition of marks or variables in a formula signifies juxtaposition of the sequence denoted.[11]Another example from the ALGOL 60 report illustrates a major difference between the BNF metalanguage and a Chomsky context-free grammar. Metalingustic variables do not require a rule defining their formation. Their formation may simply be described in natural language within the <> brackets. The following ALGOL 60 report section 2.3 comments specification, exemplifies how this works:\nFor the purpose of including text among the symbols of a program the following comment conventions hold:\n\n\nThe sequence of basic symbols:\nis equivalent to\n\n\n; comment <any sequence not containing ';'>;\n;\n\n\nbegin comment <any sequence not containing ';'>;\nbegin\n\n\nend <any sequence not containing 'end' or ';' or 'else'>\nend\n\n\nBy equivalence is here meant that any of the three structures shown in the left column may be replaced, in any occurrence outside of strings, by the symbol shown in the same line in the right column without any effect on the action of the program.\nNaur changed two of Backus's symbols to commonly available characters. The ::= symbol was originally a :\u2261. The | symbol was originally the word or (with a bar over it).[6]:14[clarification needed] Working for IBM, Backus would have had a non-disclosure agreement and couldn't have talked about his source if it came from an IBM proprietary project. BNF is very similar to canonical-form boolean algebra equations that are, and were at the time, used in logic-circuit design. Backus was a mathematician and the designer of the FORTRAN programming language. Studies of boolean algebra is commonly part of a mathematics. What we do know is that neither Backus nor Naur described the names enclosed in < > as non-terminals. Chomsky terminology was not originally used in describing BNF. Naur later described them as classes in ALGOL course materials.[5] In the ALGOL 60 report they were called metalinguistic variables. Anything other than the metasymbols ::=, |, and class names enclosed in <,> are symbols of the language being defined. The metasymbols ::= is to be interpreted as is defined as. The | is used to separate alternative definitions and is interpreted as or. The metasymbols <,> are delimiters enclosing a class name. BNF is described as a metalanguage for talking about ALGOL by Peter Naur and Saul Rosen.[5] In 1947 Saul Rosen became involved in the activities of the fledgling Association for Computing Machinery, first on the languages committee that became the IAL group and eventually led to ALGOL. He was the first managing editor of the Communications of the ACM.[clarification needed] What we do know is that BNF was first used as a metalanguage to talk about the ALGOL language in the ALGOL 60 report. That is how it is explained in ALGOL programming course material developed by Peter Naur in 1962.[5] Early ALGOL manuals by IBM, Honeywell, Burroughs and Digital Equipment Corporation followed the ALGOL 60 report using it as a metalanguage. Saul Rosen in his book[12] describes BNF as a metalanguage for talking about ALGOL. An example of its use as a metalanguage would be in defining an arithmetic expression:\n\n\n\n<expr> ::= <term>|<expr><addop><term>\n\n\nThe first symbol of an alternative may be the class being defined, the repetition, as explained by Naur, having the function of specifying that the alternative sequence can recursively begin with a previous alternative and can be repeated any number of times.[5] For example, above <expr> is defined as a <term> followed by any number of <addop> <term>.In some later metalanguages such as Schorre's META II the BNF recursive repeat construct is replaced by a sequence operator and target language symbols defined using quoted strings. The < and > bracket removed. Mathematical grouping ( ) were added. The <expr> rule would appear in META II as\n\n\n\nEXPR = TERM $('+' TERM .OUT('ADD') | '-' TERM .OUT('SUB'));\n\n\nThese changes made that META II and its derivative programming languages able to define and extend their own metalanguage. In so doing the ability to use a natural language description, metalinguistic variable, language construct description was lost. Many spin-off metalanguages were inspired by BNF. See META II, TREE-META, and Metacompiler.A BNF class describes a language construct formation, with formation defined as a pattern or the action of forming the pattern. The class name expr is described in a natural language as a <term> followed by a sequence <addop> <term>. A class is an abstraction, we can talk about it independent of its formation. We can talk about term, independent of its definition, as being added or subtracted in expr. We can talk about a term being a specific data type and how an expr is to be evaluated having specific combinations of data types. Or even reordering an expression to group data types and evaluation results of mixed types. The natural-language supplement provided specific details of the language class semantics to be used by a compiler implementation and a programmer writing an ALGOL program. Natural-language description further supplemented the syntax as well. The integer rule is a good example of natural and metalanguage used to describe syntax:\n\n\n\n<integer> ::= <digit>|<integer><digit>\n\n\nThere are no specifics on white space in the above. As far as the rule states, we could have space between the digits. In the natural language we complement the BNF metalanguage by explaining that the digit sequence can have no white space between the digits. English is only one of the possible natural languages. Translations of the ALGOL reports were available in many natural languages.The origin of BNF is not as important as its impact on programming language development. During the period immediately following the publication of the ALGOL 60 report BNF was the basis of many compiler-compiler systems. Some directly used BNF like A Syntax Directed Compiler for ALGOL 60 developed by Edgar T. Irons and A Compiler Building System Developed by Brooker and Morris. Others changed it to a programming language. The Schorre Metacompilers made it a programming language with only a few changes. <class name> became symbol identifiers dropping the enclosing <,> and using quoted strings for symbols of the target language. Arithmetic like grouping provided simplification that removed using classes where grouping was its only value. The META II arithmetic expression rule shows grouping use. Output expressions placed in a META II rule are used to output code and labels in an assembly language. Rules in META II are equivalent to a class definitions in BNF. The Unix utility yacc is based on BNF with code production similar to META II. Though yacc is most commonly used as a parser generator, its roots are obviously BNF. BNF today is one of the oldest computer-related languages still in use.Introduction[edit]A BNF specification is a set of derivation rules, written aswhere <symbol>[5] is a nonterminal, and the __expression__ consists of one or more sequences of symbols; more sequences are separated by the vertical bar |, indicating a choice, the whole being a possible substitution for the symbol on the left. Symbols that never appear on a left side are terminals. On the other hand, symbols that appear on a left side are non-terminals and are always enclosed between the pair <>.[5]The ::= means that the symbol on the left must be replaced with the expression on the right.Example[edit]As an example, consider this possible BNF for a U.S. postal address:This translates into English as:\nA postal address consists of a name-part, followed by a street-address part, followed by a zip-code part.\nA name-part consists of either: a personal-part followed by a last name followed by an optional suffix (Jr., Sr., or dynastic number) and end-of-line, or a personal part followed by a name part (this rule illustrates the use of recursion in BNFs, covering the case of people who use multiple first and middle names and initials).\nA personal-part consists of either a first name or an initial followed by a dot.\nA street address consists of a house number, followed by a street name, followed by an optional apartment specifier, followed by an end-of-line.\nA zip-part consists of a town-name, followed by a comma, followed by a state code, followed by a ZIP-code followed by an end-of-line.\nA opt-suffix-part consists of a suffix, such as Sr., Jr. or a roman-numeral, or an empty string (i.e. nothing).\nA opt-apt-num consists of an apartment number or an empty string (i.e. nothing).\nNote that many things (such as the format of a first-name, apartment specifier, ZIP-code, and Roman numeral) are left unspecified here. If necessary, they may be described using additional BNF rules.Further examples[edit]BNF's syntax itself may be represented with a BNF like the following:Note that  is the empty string.The original BNF did not use quotes as shown in <literal> rule. This assumes that no whitespace is necessary for proper interpretation of the rule.<EOL> represents the appropriate line-end specifier (in ASCII, carriage-return, line-feed or both depending on the operating system). <rule-name> and <text> are to be substituted with a declared rule's name/label or literal text, respectively.In the U.S. postal address example above, the entire block-quote is a syntax. Each line or unbroken grouping of lines is a rule; for example one rule begins with <name-part> ::=. The other part of that rule (aside from a line-end) is an expression, which consists of two lists separated by a pipe |. These two lists consists of some terms (three terms and two terms, respectively). Each term in this particular rule is a rule-name.Variants[edit]There are many variants and extensions of BNF, generally either for the sake of simplicity and succinctness, or to adapt it to a specific application. One common feature of many variants is the use of regular expression repetition operators such as * and +. The extended Backus\u2013Naur form (EBNF) is a common one.Another common extension is the use of square brackets around optional items. Although not present in the original ALGOL 60 report (instead introduced a few years later in IBM's PL/I definition), the notation is now universally recognised.Augmented Backus\u2013Naur form (ABNF) and Routing Backus\u2013Naur form (RBNF)[13] are extensions commonly used to describe Internet Engineering Task Force (IETF) protocols.Parsing expression grammars build on the BNF and regular expression notations to form an alternative class of formal grammar, which is essentially analytic rather than generative in character.Many BNF specifications found online today are intended to be human-readable and are non-formal. These often include many of the following syntax rules and extensions:\nOptional items enclosed in square brackets: [<item-x>].\nItems existing 0 or more times are enclosed in curly brackets or suffixed with an asterisk (*) such as <word> ::= <letter> {<letter>} or <word> ::= <letter> <letter>* respectively.\nItems existing 1 or more times are suffixed with an addition (plus) symbol, +.\nTerminals may appear in bold rather than italics, and non-terminals in plain text rather than angle brackets.\nWhere items are grouped, they are enclosed in simple parentheses.\nSoftware using BNF[edit]\nANTLR, another parser generator written in Java\nBNF Converter (BNFC[14]), operating on a variant called labeled Backus\u2013Naur form (LBNF). In this variant, each production for a given non-terminal is given a label, which can be used as a constructor of an algebraic data type representing that nonterminal. The converter is capable of producing types and parsers for abstract syntax in several languages, including Haskell and Java.\nCoco/R, compiler generator accepting an attributed grammar in EBNF\nDMS Software Reengineering Toolkit, program analysis and transformation system for arbitrary languages\nGOLD BNF parser\nGNU bison, GNU version of yacc\nRPA BNF parser.[15] Online (PHP) demo parsing: JavaScript, XML\nXACT X4MR System,[16] a rule-based expert system for programming language translation\nXPL Analyzer, a tool which accepts simplified BNF for a language and produces a parser for that language in XPL; it may be integrated into the supplied SKELETON program, with which the language may be debugged[17] (a SHARE contributed program, which was preceded by A Compiler Generator, ISBN 978-0-13-155077-3)\nYacc, parser generator (used with Lex preprocessor)\nbnfparser2,[18] a universal syntax verification utility\nbnf2xml,[19] Markup input with XML tags using advanced BNF matching.\nJavaCC,[20] Java Compiler Compiler tm (JavaCC tm) - The Java Parser Generator.\nRacket's parser tools, lex and yacc-style Parsing (Beautiful Racket edition)\nSee also[edit]\nCompiler Description Language (CDL)\nSyntax diagram \u2013 railroad diagram\nTranslational Backus\u2013Naur form (TBNF)\nWirth syntax notation \u2013 an alternative to BNF from 1977\nDefinite clause grammar \u2013 a more expressive alternative to BNF used in Prolog\nVan Wijngaarden grammar \u2013 used in preference to BNF to define Algol68\nReferences[edit]This article is based on material taken from the Free On-line Dictionary of Computing prior to 1 November 2008 and incorporated under the relicensing terms of the GFDL, version 1.3 or later.External links[edit]\nMorrison, Kelly (July 2, 1993), Backus Normal Form vs. Backus-Naur Form, comp.compilers (newsgroup), IECC  posting quotes Alan J. Perlis and Peter Naur from ALGOL Section of Richard L. Wexelblat, editor, History of Programming languages (1981)\nGarshol, Lars Marius, BNF and EBNF: What are they and how do they work?, NO: Priv .\nRFC 4234 \u2014 Augmented BNF for Syntax Specifications: ABNF .\nRFC 5511 \u2014 Routing BNF: A Syntax Used in Various Protocol Specifications .\nISO/IEC 14977:1996(E) Information technology \u2013 Syntactic metalanguage \u2013 Extended BNF, available from Publicly available, Standards, ISO  or from Kuhn, Marcus, Iso 14977 (PDF), UK: CAM  (the latter is missing the cover page, but is otherwise much cleaner)\nLanguage grammars[edit]\nBernhard, Algol-60 BNF, DE: LRZ Mu\u0308nchen, archived from the original on 2006-09-25 , the original BNF.\nBNF grammars for SQL-92, SQL-99 and SQL-2003, Savage, AU: Net , freely available BNF grammars for SQL.\nBNF Web Club, DB research, CH: Unige , freely available BNF grammars for SQL, Ada, Java.\nFree Programming Language Grammars for Compiler Construction, Source code, The free country , freely available BNF/EBNF grammars for C/C++, Pascal, COBOL, Ada 95, PL/I.\nBNF files related to the STEP standard, Exp engine (SVN), Source forge . Includes parts 11, 14, and 21 of the ISO 10303 (STEP) standard.\n", "subtitles": ["History", "Introduction", "Example", "Further examples", "Variants", "See also", "References", "External links"], "title": "Backus\u2013Naur form"},
{"content": "In computer science, an object can be a variable, a data structure, a function, or a method, and as such, is a location in memory having a value and referenced by an identifier.In the class-based object-oriented programming paradigm, object refers to a particular instance of a class where the object can be a combination of variables, functions, and data structures.In relational database management, an object can be a table or column, or an association between data and a database entity (such as relating a person's age to a specific person).[1]Object-based languages[edit]An important distinction in programming languages is the difference between an object-oriented language and an object-based language. A language is usually considered object-based if it includes the basic capabilities for an object: identity, properties, and attributes. A language is considered object-oriented if it is object-based and also has the capability of polymorphism and inheritance. Polymorphism refers to the ability to overload the name of a function with multiple behaviors based on which object(s) are passed to it. Conventional message passing discriminates only on the first object and considers that to be sending a message to that object. However, some OOP languages such as Flavors and the Common Lisp Object System (CLOS) enable discriminating on more than the first parameter of the function.[2] Inheritance is the ability to subclass an object class, to create a new class that is a subclass of an existing one and inherits all the data constraints and behaviors of its parents but also adds new and/or changes one or more of them.[3][4]Object-oriented programming[edit]Object-oriented programming is an approach to designing modular reusable software systems. The object-oriented approach is an evolution of good design practices that go back to the very beginning of computer programming. Object-orientation is simply the logical extension of older techniques such as structured programming and abstract data types. An object is an abstract data type with the addition of polymorphism and inheritance.Rather than structure programs as code and data, an object-oriented system integrates the two using the concept of an object. An object has state (data) and behavior (code). Objects can correspond to things found in the real world. So for example, a graphics program will have objects such as circle, square, menu. An online shopping system will have objects such as shopping cart, customer, product. The shopping system will support behaviors such as place order, make payment, and offer discount. The objects are designed as class hierarchies. So for example with the shopping system there might be high level classes such as electronics product, kitchen product, and book. There may be further refinements for example under electronic products: CD Player, DVD player, etc. These classes and subclasses correspond to sets and subsets in mathematical logic.[5][6]Specialized objects[edit]An important concept for objects is the design pattern. A design pattern provides a reusable template to address a common problem. The following object descriptions are examples of some of the most common design patterns for objects.[7]\nFunction object: an object with a single method (in C++, this method would be the function operator, operator()) that acts much like a function (like a C/C++ pointer to a function).\nImmutable object: an object set up with a fixed state at creation time and which does not change afterward.\nFirst-class object: an object that can be used without restriction.\nContainer object: an object that can contain other objects.\nFactory object: an object whose purpose is to create other objects.\nMetaobject: an object from which other objects can be created (compare with a class, which is not necessarily an object).\nPrototype object: a specialized metaobject from which other objects can be created by copying\nGod object: an object that knows or does too much (it is an example of an anti-pattern).\nSingleton object: an object that is the only instance of its class during the lifetime of the program.\nFilter object.\nDistributed objects[edit]The object-oriented approach is not just a programming model. It can be used equally well as an interface definition language for distributed systems. The objects in a distributed computing model tend to be larger grained, longer lasting, and more service-oriented than programming objects.A standard method to package distributed objects is via an Interface Definition Language (IDL). An IDL shields the client of all of the details of the distributed server object. Details such as which computer the object resides on, what programming language it uses, what operating system, and other platform specific issues. The IDL is also usually part of a distributed environment that provides services such as transactions and persistence to all objects in a uniform manner. Two of the most popular standards for distributed objects are the Object Management Group's CORBA standard and Microsoft's DCOM.[8]In addition to distributed objects, a number of other extensions to the basic concept of an object have been proposed to enable distributed computing:\nProtocol objects are components of a protocol stack that enclose network communication within an object-oriented interface.\nReplicated objects are groups of distributed objects (called replicas) that run a distributed multi-party protocol to achieve high consistency between their internal states, and that respond to requests in a coordinated way. Examples include fault-tolerant CORBA objects.\nLive distributed objects (or simply live objects)[9] generalize the replicated object concept to groups of replicas that might internally use any distributed protocol, perhaps resulting in only a weak consistency between their local states.\nSome of these extensions, such as distributed objects and protocol objects, are domain-specific terms for special types of ordinary objects used in a certain context (such as remote method invocation or protocol composition). Others, such as replicated objects and live distributed objects, are more non-standard, in that they abandon the usual case that an object resides in a single location at a time, and apply the concept to groups of entities (replicas) that might span across multiple locations, might have only weakly consistent state, and whose membership might dynamically change.The Semantic Web[edit]The Semantic Web is essentially a distributed objects framework. Two key technologies in the Semantic Web are the Web Ontology Language (OWL) and the Resource Description Framework (RDF). RDF provides the capability to define basic objects\u2014names, properties, attributes, relations\u2014that are accessible via the Internet. OWL adds a richer object model, based on set theory, that provides additional modeling capabilities such as multiple inheritance.OWL objects are not like standard large grained distributed objects accessed via an Interface Definition Language. Such an approach would not be appropriate for the Internet because the Internet is constantly evolving and standardization on one set of interfaces is difficult to achieve. OWL objects tend to be similar to the kind of objects used to define application domain models in programming languages such as Java and C++.However, there are important distinctions between OWL objects and traditional object-oriented programming objects. Where as traditional objects get compiled into static hierarchies usually with single inheritance, OWL objects are dynamic. An OWL object can change its structure at run time and can become an instance of new or different classes.Another critical difference is the way the model treats information that is currently not in the system. Programming objects and most database systems use the closed-world assumption. If a fact is not known to the system that fact is assumed to be false. Semantic Web objects use the open-world assumption, a statement is only considered false if there is actual relevant information that it is false, otherwise it is assumed to be unknown, neither true nor false.OWL objects are actually most like objects in artificial intelligence frame languages such as KL-ONE and Loom.The following table contrasts traditional objects from Object-Oriented programming languages such as Java or C++ with Semantic Web Objects:[10][11]See also[edit]\nObject lifetime\nObject copy\nDesign pattern (computer science)\nBusiness object (computer science)\nActor model\nReferences[edit]External links[edit]\nWhat Is an Object? from The Java Tutorials\nTHE COMPUTER OBJECTS LOOKING FOR THEIR SOCIAL AND ORGANIZATIONAL IMPLICATIONS. http://revistas.face.ufmg.br/index.php/farol/article/view/2709\n", "subtitles": ["Object-based languages", "Object-oriented programming", "Specialized objects", "Distributed objects", "The Semantic Web", "See also", "References", "External links"], "title": "Object (computer science)"},
{"content": "In computer science, a tagged pointer is a pointer (concretely a memory address) with additional data associated with it, such as an indirection bit or reference count. This additional data is often folded into the pointer, meaning stored inline in the data representing the address, taking advantage of certain properties of memory addressing. The name comes from tagged union, and the additional data is called a tag or tags, though strictly speaking tag refers to data specifying a type, not other data; however, the usage tagged pointer is standard.Folding tags into the pointer[edit]There are various techniques for folding tags into a pointer.[1]Most architectures are byte-addressable (memory addresses are in bytes), but certain types of data will often be aligned to the size of the data, often a word or multiple thereof. This discrepancy leaves a few of the least significant bits of the pointer unused, which can be used for tags \u2013 most often as a bit field (each bit a separate tag) \u2013 as long as code that uses the pointer masks out these bits before accessing memory. E.g., on a 32-bit architecture (for both addresses and word size), a word is 32 bits = 4 bytes, so word-aligned addresses are always a multiple of 4, hence end in 00, leaving the last 2 bits available; while on a 64-bit architecture, a word is 64 bits word = 8 bytes, so word-aligned addresses end in 000, leaving the last 3 bits available. In cases where data is aligned at a multiple of word size, further bits are available. In case of word-addressable architectures, word-aligned data does not leave any bits available, as there is no discrepancy between alignment and addressing, but data aligned at a multiple of word size does.Conversely, in some operating systems, virtual addresses are narrower than the overall architecture width, which leaves the most significant bits available for tags; this can be combined with the previous technique in case of aligned addresses. This is particularly the case on 64-bit architectures, as 64 bits of address space are far above the data requirements of all but the largest applications, and thus many practical 64-bit processors have narrower addresses. Note that the virtual address width may be narrower than the physical address width, which in turn may be narrower than the architecture width; for tagging of pointers in user space, the virtual address space provided by the operating system (in turn provided by the memory management unit) is the relevant width. In fact, some processors specifically forbid use of such tagged pointers at the processor level, notably x86-64, which requires the use of canonical form addresses by the operating system, with most significant bits all 0s or all 1s.Lastly, the virtual memory system in most modern operating systems reserves a block of logical memory around address 0 as unusable. This means that, for example, a pointer to 0 is never a valid pointer and can be used as a special null pointer value. Unlike the previously mentioned techniques, this only allows a single special pointer value, not extra data for pointers generally.Examples[edit]A significant example of the use of tagged pointers is the Objective-C runtime on iOS 7 on ARM64, notably used on the iPhone 5S. In iOS 7, virtual addresses are 33 bits (byte-aligned), so word-aligned addresses only use 30 bits (3 least significant bits are 0), leaving 34 bits for tags. Objective-C class pointers are word-aligned, and the tag fields are used for many purposes, such as storing a reference count and whether the object has a destructor.[2][3]Early versions of MacOS used tagged addresses called Handles to store references to data objects. The high bits of the address indicated whether the data object was locked, purgeable, and/or originated from a resource file, respectively. This caused compatibility problems when MacOS addressing advanced from 24 bits to 32 bits in System 7.[4]Null versus aligned pointer[edit]Use of zero to represent a null pointer is extremely common, with many programming languages (such as Ada) explicitly relying on this behavior. In theory, other values in an operating system-reserved block of logical memory could be used to tag conditions other than a null pointer, but these uses appear to be rare, perhaps because they are at best non-portable. It is generally accepted practice in software design that if a special pointer value distinct from null (such as a sentinel in certain data structures) is needed, the programmer should explicitly provide for it.Taking advantage of the alignment of pointers provides more flexibility than null pointers/sentinels because it allows pointers to be tagged with information about the type of data pointed to, conditions under which it may be accessed, or other similar information about the pointer's use. This information can be provided along with every valid pointer. In contrast, null pointers/sentinels provide only a finite number of tagged values distinct from valid pointers.In a tagged architecture, a number of bits in every word of memory are reserved to act as a tag. Tagged architectures, such as the Lisp machines, often have hardware support for interpreting and processing tagged pointers.GNU libc malloc() always returns 8-byte aligned memory addresses.[citation needed] Larger alignment values can be obtained with posix_memalign().[5]Examples[edit]Example 1[edit]In the following C code, the value of zero is used to indicate a null pointer:Example 2[edit]Here, the programmer has provided a global variable, whose address is then used as a sentinel:Example 3[edit]Assume we have a data structure table_entry that is always aligned to a 16 byte boundary. In other words, the least significant 4 bits of a table entry's address are always 0 (\n  \n    \n      \n        \n          2\n          \n            4\n          \n        \n        =\n        16\n      \n    \n    {\\displaystyle 2^{4}=16}\n  \n). We could use these 4 bits to mark the table entry with extra information. For example, bit 0 might mean read only, bit 1 might mean dirty (the table entry needs to be updated), and so on.If pointers are 16-bit values, then:\n0x3421 is a read-only pointer to the table_entry at address 0x3420\n0xf472 is a pointer to a dirty table_entry at address 0xf470\nAdvantages[edit]The major advantage of tagged pointers is that they take up less space than a pointer along with a separate tag field. This can be especially important when a pointer is a return value from a function. It can also be important in large tables of pointers.A more subtle advantage is that by storing a tag in the same place as the pointer, it is often possible to guarantee the atomicity of an operation that updates both the pointer and its tag without external synchronization mechanisms. This can be an extremely large performance gain, especially in operating systems.Disadvantages[edit]Tagged pointers have some of the same difficulties as xor linked lists, although to a lesser extent. For example, not all debuggers will be able to properly follow tagged pointers; however, this is not an issue for a debugger that is designed with tagged pointers in mind.The use of zero to represent a null pointer does not suffer from these disadvantages: it is pervasive, most programming languages treat zero as a special null value, and it has thoroughly proven its robustness. An exception is the way that zero participates in overload resolution in C++, where zero is treated as an integer rather than a pointer; for this reason the special value nullptr is preferred over the integer zero. However, with tagged pointers zeros are usually not used to represent null pointers.References[edit]", "subtitles": ["Folding tags into the pointer", "Examples", "Null versus aligned pointer", "Examples", "Advantages", "Disadvantages", "References"], "title": "Tagged pointer"},
{"content": "Phillip James (Bill) Plauger[2] (born January 13, 1944, Petersburg, West Virginia) is an author, entrepreneur and computer programmer. He has written and co-written articles and books about programming style, software tools, and the C programming language, as well as works of science fiction.Plauger worked at Bell Labs from 1969 to 1975,[1] where he coauthored Elements of Programming Style and Software Tools with Brian Kernighan. In 1978, he founded Whitesmiths, the first company to sell a C compiler and Unix-like operating system (Idris). He has since been involved in C and C++ standardization and is now the president of Dinkumware. In January 2009 he became the convener of the ISO C++ standards committee, but in October 2009 he tendered his resignation after failing to pass a resolution to stop processing any new features in order to facilitate the promised shipping date for the C++0x standard.[3][4]Plauger has been credited with inventing pair programming while leading Whitesmiths Ltd.[5]Plauger wrote a science fiction short story, Child of All Ages, first published in Analog in the March 1975 issue, whose protagonist was granted immortality before attaining puberty and finds that being a child who never grows up is far removed from an idyllic Peter Pan-like existence. The story was nominated for a Nebula Award in 1975 and a Hugo Award in 1976. He won the John W. Campbell Award for Best New Writer in 1975\u2014notably beating John Varley for the award\u2014and subsequently sold a story to The Last Dangerous Visions.Plauger holds a bachelor's degree in physics from Princeton University and a PhD in nuclear physics from Michigan State University.[6]Dinkumware[edit]Dinkumware is an American software company specializing in core libraries for C/C++, owned and operated by P. J. Plauger. It is based in Concord, Massachusetts (US).The company has provided the C++ Standard Library implementation that ships with Microsoft Visual C++ since 1996, and supplies C++ and Embedded C++ libraries to the embedded community.They also provide libraries for Java and other tools, including proofers to test for library adherence to the standard.Bibliography[edit]\nSpectroscopy in the Titanium Isotopes (1969)\nThe Elements of Programming Style (1974, revised 1978) with Brian W. Kernighan\nSoftware Tools (1976) with Brian W. Kernighan\nSoftware Tools in Pascal (1981) with Brian W. Kernighan\nThe Standard C library (1992)\nProgramming on Purpose, collected essays from the magazine Computer Language\n\nVolume I: Essays on Software Design (1992)\nVolume II: Essays on Software People (1993)\nVolume III: Essays on Software Technology (1993)\n\n\nThe Draft Standard C++ Library (1995)\nStandard C: A Reference (1989, revised 1992, revised 1996) with Jim Brodie\nThe C++ Standard Template Library (2001) with Alexander Stepanov, Meng Lee, and David R. Musser\nReferences[edit]External links[edit]\nPersonal website\nDinkumware company website\nP. J. Plauger at the Internet Speculative Fiction Database\n", "subtitles": ["Dinkumware", "Bibliography", "References", "External links"], "title": "P. J. Plauger"},
{"content": "The syste\u0300me universitaire de documentation or SUDOC is a system used by the libraries of French universities and higher education establishments to identify, track and manage the documents in their possession. The catalog, which contains more than 10 million references, allows students and researcher to search for bibliographical and location information in over 3,400 documentation centers. It is maintained by the Bibliographic Agency for Higher Education (fr) (ABES).External links[edit]\nOfficial website\n", "subtitles": [], "title": "Syst\u00e8me universitaire de documentation"},
{"content": "In computer science, lexical analysis, lexing or tokenization is the process of converting a sequence of characters (such as in a computer program or web page) into a sequence of tokens (strings with an assigned and thus identified meaning). A program that performs lexical analysis may be termed a lexer, tokenizer,[1] or scanner, though scanner is also a term for the first stage of a lexer. A lexer is generally combined with a parser, which together analyze the syntax of programming languages, web pages, and so forth.Applications[edit]A lexer forms the first phase of a compiler frontend in modern processing. Analysis generally occurs in one pass.In older languages such as ALGOL, the initial stage was instead line reconstruction, which performed unstropping and removed whitespace and comments (and had scannerless parsers, with no separate lexer). These steps are now done as part of the lexer.Lexers and parsers are most often used for compilers, but can be used for other computer language tools, such as prettyprinters or linters. Lexing can be divided into two stages: the scanning, which segments the input string into syntactic units called lexemes and categorizes these into token classes; and the evaluating, which converts lexemes into processed values.Lexers are generally quite simple, with most of the complexity deferred to the parser or semantic analysis phases, and can often be generated by a lexer generator, notably lex or derivatives. However, lexers can sometimes include some complexity, such as phrase structure processing to make input easier and simplify the parser, and may be written partly or fully by hand, either to support more features or for performance.Lexeme[edit]A lexeme is a sequence of characters in the source program that matches the pattern for a token and is identified by the lexical analyzer as an instance of that token.[2]Some authors term this a token, using token interchangeably to represent the string being tokenized, and the token data structure resulting from putting this string through the tokenization process.[3][4]The word lexeme in computer science is defined differently than lexeme in linguistics. A lexeme in computer science roughly corresponds to what might be termed a word in linguistics (the term word in computer science has a different meaning than word in linguistics), although in some cases it may be more similar to a morpheme.Token[edit]A lexical token or simply token is a string with an assigned and thus identified meaning. It is structured as a pair consisting of a token name and an optional token value. The token name is a category of lexical unit.[2] Common token names are\nidentifiers: names the programmer chooses;\nkeywords: names already in the programming language;\nseparators (also known as punctuators): punctuation characters and paired-delimiters;\noperators: symbols that operate on arguments and produce results;\nliterals: numeric, logical, textual, reference literals;\ncomments: line, block.\nConsider this expression in the C programming language:\nx = a + b * 2;\nThe lexical analysis of this expression yields the following sequence of tokens:\n[(identifier, x), (operator, =), (identifier, a), (operator, +), (identifier, b), (operator, *), (literal, 2), (separator, ;)]\nA token name is what might be termed a part of speech in linguistics.Lexical grammar[edit]The specification of a programming language often includes a set of rules, the lexical grammar, which defines the lexical syntax. The lexical syntax is usually a regular language, with the grammar rules consisting of regular expressions; they define the set of possible character sequences (lexemes) of a token. A lexer recognizes strings, and for each kind of string found the lexical program takes an action, most simply producing a token.Two important common lexical categories are white space and comments. These are also defined in the grammar and processed by the lexer, but may be discarded (not producing any tokens) and considered non-significant, at most separating two tokens (as in if x instead of ifx). There are two important exceptions to this. First, in off-side rule languages that delimit blocks with indenting, initial whitespace is significant, as it determines block structure, and is generally handled at the lexer level; see phrase structure, below. Secondly, in some uses of lexers, comments and whitespace must be preserved \u2013 for examples, a prettyprinter also needs to output the comments and some debugging tools may provide messages to the programmer showing the original source code. In the 1960s, notably for ALGOL, whitespace and comments were eliminated as part of the line reconstruction phase (the initial phase of the compiler frontend), but this separate phase has been eliminated and these are now handled by the lexer.Tokenization[edit]Tokenization is the process of demarcating and possibly classifying sections of a string of input characters. The resulting tokens are then passed on to some other form of processing. The process can be considered a sub-task of parsing input.(Note: Tokenization in the field of computer security has a different meaning.)For example, in the text string:\nThe quick brown fox jumps over the lazy dog\nthe string isn't implicitly segmented on spaces, as a natural language speaker would do. The raw input, the 43 characters, must be explicitly split into the 9 tokens with a given space delimiter (i.e., matching the string   or regular expression /\\s{1}/).The tokens could be represented in XML,Or as an s-expression,When a token class represents more than one possible lexeme, the lexer often saves enough information to reproduce the original lexeme, so that it can be used in semantic analysis. The parser typically retrieves this information from the lexer and stores it in the abstract syntax tree. This is necessary in order to avoid information loss in the case of numbers and identifiers.Tokens are identified based on the specific rules of the lexer. Some methods used to identify tokens include: regular expressions, specific sequences of characters termed a flag, specific separating characters called delimiters, and explicit definition by a dictionary. Special characters, including punctuation characters, are commonly used by lexers to identify tokens because of their natural use in written and programming languages.Tokens are often categorized by character content or by context within the data stream. Categories are defined by the rules of the lexer. Categories often involve grammar elements of the language used in the data stream. Programming languages often categorize tokens as identifiers, operators, grouping symbols, or by data type. Written languages commonly categorize tokens as nouns, verbs, adjectives, or punctuation. Categories are used for post-processing of the tokens either by the parser or by other functions in the program.A lexical analyzer generally does nothing with combinations of tokens, a task left for a parser. For example, a typical lexical analyzer recognizes parentheses as tokens, but does nothing to ensure that each ( is matched with a ).When a lexer feeds tokens to the parser, the representation used is typically an enumerated list of number representations. For example, Identifier is represented with 0, Assignment operator with 1, Addition operator with 2, etc.Tokens are defined often by regular expressions, which are understood by a lexical analyzer generator such as lex. The lexical analyzer (generated automatically by a tool like lex, or hand-crafted) reads in a stream of characters, identifies the lexemes in the stream, and categorizes them into tokens. This is termed tokenizing. If the lexer finds an invalid token, it will report an error.Following tokenizing is parsing. From there, the interpreted data may be loaded into data structures for general use, interpretation, or compiling.Scanner[edit]The first stage, the scanner, is usually based on a finite-state machine (FSM). It has encoded within it information on the possible sequences of characters that can be contained within any of the tokens it handles (individual instances of these character sequences are termed lexemes). For example, an integer token may contain any sequence of numerical digit characters. In many cases, the first non-whitespace character can be used to deduce the kind of token that follows and subsequent input characters are then processed one at a time until reaching a character that is not in the set of characters acceptable for that token (this is termed the maximal munch, or longest match, rule). In some languages, the lexeme creation rules are more complex and may involve backtracking over previously read characters. For example, in C, one 'L' character is not enough to distinguish between an identifier that begins with 'L' and a wide-character string literal.Evaluator[edit]A lexeme, however, is only a string of characters known to be of a certain kind (e.g., a string literal, a sequence of letters). In order to construct a token, the lexical analyzer needs a second stage, the evaluator, which goes over the characters of the lexeme to produce a value. The lexeme's type combined with its value is what properly constitutes a token, which can be given to a parser. Some tokens such as parentheses do not really have values, and so the evaluator function for these can return nothing: only the type is needed. Similarly, sometimes evaluators can suppress a lexeme entirely, concealing it from the parser, which is useful for whitespace and comments. The evaluators for identifiers are usually simple (literally representing the identifier), but may include some unstropping. The evaluators for integer literals may pass the string on (deferring evaluation to the semantic analysis phase), or may perform evaluation themselves, which can be involved for different bases or floating point numbers. For a simple quoted string literal, the evaluator needs to remove only the quotes, but the evaluator for an escaped string literal incorporates a lexer, which unescapes the escape sequences.For example, in the source code of a computer program, the string\nnet_worth_future = (assets - liabilities);\nmight be converted into the following lexical token stream; whitespace is suppressed and special characters have no value:\nIDENTIFIER net_worth_future\nEQUALS\nOPEN_PARENTHESIS\nIDENTIFIER assets\nMINUS\nIDENTIFIER liabilities\nCLOSE_PARENTHESIS\nSEMICOLON\nThough it is possible and sometimes necessary, due to licensing restrictions of existing parsers or if the list of tokens is small, to write a lexer by hand, lexers are often generated by automated tools. These tools generally accept regular expressions that describe the tokens allowed in the input stream. Each regular expression is associated with a production rule in the lexical grammar of the programming language that evaluates the lexemes matching the regular expression. These tools may generate source code that can be compiled and executed or construct a state transition table for a finite-state machine (which is plugged into template code for compiling and executing).Regular expressions compactly represent patterns that the characters in lexemes might follow. For example, for an English-based language, an IDENTIFIER token might be any English alphabetic character or an underscore, followed by any number of instances of ASCII alphanumeric characters and/or underscores. This could be represented compactly by the string [a-zA-Z_][a-zA-Z_0-9]*. This means any character a-z, A-Z or _, followed by 0 or more of a-z, A-Z, _ or 0-9.Regular expressions and the finite-state machines they generate are not powerful enough to handle recursive patterns, such as n opening parentheses, followed by a statement, followed by n closing parentheses. They are unable to keep count, and verify that n is the same on both sides, unless a finite set of permissible values exists for n. It takes a full parser to recognize such patterns in their full generality. A parser can push parentheses on a stack and then try to pop them off and see if the stack is empty at the end (see example[5] in the Structure and Interpretation of Computer Programs book).Obstacles[edit]Typically, tokenization occurs at the word level. However, it is sometimes difficult to define what is meant by a word. Often a tokenizer relies on simple heuristics, for example:\nPunctuation and whitespace may or may not be included in the resulting list of tokens.\nAll contiguous strings of alphabetic characters are part of one token; likewise with numbers.\nTokens are separated by whitespace characters, such as a space or line break, or by punctuation characters.\nIn languages that use inter-word spaces (such as most that use the Latin alphabet, and most programming languages), this approach is fairly straightforward. However, even here there are many edge cases such as contractions, hyphenated words, emoticons, and larger constructs such as URIs (which for some purposes may count as single tokens). A classic example is New York-based, which a naive tokenizer may break at the space even though the better break is (arguably) at the hyphen.Tokenization is particularly difficult for languages written in scriptio continua which exhibit no word boundaries such as Ancient Greek, Chinese,[6] or Thai. Agglutinative languages, such as Korean, also make tokenization tasks complicated.Some ways to address the more difficult problems include developing more complex heuristics, querying a table of common special-cases, or fitting the tokens to a language model that identifies collocations in a later processing step.Software[edit]\nApache OpenNLP includes rule based and statistical tokenizers which support many languages\nU-Tokenizer is an API over HTTP that can cut Mandarin and Japanese sentences at word boundary. English is supported as well.\nHPE Haven OnDemand Text Tokenization API (Commercial product, with freemium access) uses Advanced Probabilistic Concept Modelling to determine the weight that the term holds in the specified text indexes\nThe Lex tool and its compiler is designed to generate code for fast lexical analysers based on a formal description of the lexical syntax. It is generally considered insufficient for applications with a complex set of lexical rules and severe performance requirements. For example, the GNU Compiler Collection (GCC) uses hand-written lexers.\nLexer generator[edit]Lexers are often generated by a lexer generator, analogous to parser generators, and such tools often come together. The most established is lex, paired with the yacc parser generator, and the free equivalents flex/bison. These generators are a form of domain-specific language, taking in a lexical specification \u2013 generally regular expressions with some markup \u2013 and emitting a lexer.These tools yield very fast development, which is very important in early development, both to get a working lexer and because a language specification may change often. Further, they often provide advanced features, such as pre- and post-conditions which are hard to program by hand. However, an automatically generated lexer may lack flexibility, and thus may require some manual modification, or an all-manually written lexer.Lexer performance is a concern, and optimizing is worthwhile, more so in stable languages where the lexer is run very often (such as C or HTML). lex/flex-generated lexers are reasonably fast, but improvements of two to three times are possible using more tuned generators. Hand-written lexers are sometimes used, but modern lexer generators produce faster lexers than most hand-coded ones. The lex/flex family of generators uses a table-driven approach which is much less efficient than the directly coded approach.[dubious \u2013 discuss] With the latter approach the generator produces an engine that directly jumps to follow-up states via goto statements. Tools like re2c[7] have proven to produce engines that are between two and three times faster than flex produced engines.[citation needed] It is in general difficult to hand-write analyzers that perform better than engines generated by these latter tools.List of lexer generators[edit]\nANTLR \u2013 can generate lexical analyzers and parsers\nDFASTAR \u2013 generates DFA matrix table-driven lexers in C++\nFlex \u2013 variant of the classic lex for C/C++\nRagel \u2013 state machine and lexer generator with output in C, C++, and Assembly\nre2c \u2013 lexer generator for C and C++\nThe following lexical analysers can handle Unicode:\nJavaCC \u2013 generates lexical analyzers written in Java\nJFLex \u2013 lexical analyzer generator for Java\nAnnoFlex - annotation-based code generator for lexical scanners for Java\nRE/flex - a fast variant of lex/flex for C++ generates scanners with tables or direct code\nQuex \u2013 fast universal lexical analyzer generator for C and C++ written in Python\nFsLex \u2013 lexer generator for byte and Unicode character input for F#\nre2c \u2013 lexer generator for C and C++[8]\nPLY - the Python module ply.lex enables the lexical analysis part\nPhrase structure[edit]Lexical analysis mainly segments the input stream of characters into tokens, simply grouping the characters into pieces and categorizing them. However, the lexing may be significantly more complex; most simply, lexers may omit tokens or insert added tokens. Omitting tokens, notably whitespace and comments, is very common, when these are not needed by the compiler. Less commonly, added tokens may be inserted. This is done mainly to group tokens into statements, or statements into blocks, to simplify the parser.Line continuation[edit]Line continuation is a feature of some languages where a newline is normally a statement terminator. Most often, ending a line with a backslash (immediately followed by a newline) results in the line being continued \u2013 the following line is joined to the prior line. This is generally done in the lexer: the backslash and newline are discarded, rather than the newline being tokenized. Examples include bash,[9] other shell scripts and Python.[10]Semicolon insertion[edit]Many languages use the semicolon as a statement terminator. Most often this is mandatory, but in some languages the semicolon is optional in many contexts. This is mainly done at the lexer level, where the lexer outputs a semicolon into the token stream, despite one not being present in the input character stream, and is termed semicolon insertion or automatic semicolon insertion. In these cases, semicolons are part of the formal phrase grammar of the language, but may not be found in input text, as they can be inserted by the lexer. Optional semicolons or other terminators or separators are also sometimes handled at the parser level, notably in the case of trailing commas or semicolons.Semicolon insertion is a feature of BCPL and its distant descendent Go,[11] though it is absent in B or C.[12] Semicolon insertion is present in JavaScript, though the rules are somewhat complex and much-criticized; to avoid bugs, some recommend always using semicolons, while others use initial semicolons, termed defensive semicolons, at the start of potentially ambiguous statements.Semicolon insertion (in languages with semicolon-terminated statements) and line continuation (in languages with newline-terminated statements) can be seen as complementary: semicolon insertion adds a token, even though newlines generally do not generate tokens, while line continuation prevents a token from being generated, even though newlines generally do generate tokens.Off-side rule[edit]The off-side rule (blocks determined by indenting) can be implemented in the lexer, as in Python, where increasing the indenting results in the lexer emitting an INDENT token, and decreasing the indenting results in the lexer emitting a DEDENT token.[13] These tokens correspond to the opening brace { and closing brace } in languages that use braces for blocks, and means that the phrase grammar does not depend on whether braces or indenting are used. This requires that the lexer hold state, namely the current indent level, and thus can detect changes in indenting when this changes, and thus the lexical grammar is not context-free: INDENT\u2013DEDENT depend on the contextual information of prior indent level.Context-sensitive lexing[edit]Generally lexical grammars are context-free, or almost so, and thus require no looking back or ahead, or backtracking, which allows a simple, clean, and efficient implementation. This also allows simple one-way communication from lexer to parser, without needing any information flowing back to the lexer.There are exceptions, however. Simple examples include: semicolon insertion in Go, which requires looking back one token; concatenation of consecutive string literals in Python,[14] which requires holding one token in a buffer before emitting it (to see if the next token is another string literal); and the off-side rule in Python, which requires maintaining a count of indent level (indeed, a stack of each indent level). These examples all only require lexical context, and while they complicate a lexer somewhat, they are invisible to the parser and later phases.A more complex example is the lexer hack in C, where the token class of a sequence of characters cannot be determined until the semantic analysis phase, since typedef names and variable names are lexically identical but constitute different token classes. Thus in the hack, the lexer calls the semantic analyzer (say, symbol table) and checks if the sequence requires a typedef name. In this case, information must flow back not from the parser only, but from the semantic analyzer back to the lexer, which complicates design.Notes[edit]References[edit]Sources[edit]External links[edit]\nYang, W.; Tsay, Chey-Woei; Chan, Jien-Tsai (2002). On the applicability of the longest-match rule in lexical analysis. Computer Languages, Systems and Structures. Elsevier Science. 28 (3): 273\u2013288. doi:10.1016/S0096-0551(02)00014-0. NSC 86-2213-E-009-021 and NSC 86-2213-E-009-079. \nTrim, Craig (Jan 23, 2013). The Art of Tokenization. Developer Works. IBM. \nWord Mention Segmentation Task, an analysis\n", "subtitles": ["Applications", "Lexeme", "Token", "Lexical grammar", "Tokenization", "Lexer generator", "Phrase structure", "Context-sensitive lexing", "Notes", "References", "External links"], "title": "Lexical analysis"},
{"content": "In programming languages, a closure (also lexical closure or function closure) is a technique for implementing lexically scoped name binding in a language with first-class functions. Operationally, a closure is a record storing a function[a] together with an environment.[1] The environment is a mapping associating each free variable of the function (variables that are used locally, but defined in an enclosing scope) with the value or reference to which the name was bound when the closure was created.[b] A closure\u2014unlike a plain function\u2014allows the function to access those captured variables through the closure's copies of their values or references, even when the function is invoked outside their scope.Example. The following program fragment defines a higher-order function (function returning a function) add with a parameter x and a nested function addX. The nested function addX has access to x, because x is in the lexical scope of addX. The function add returns a closure; this closure contains (1) a reference to the function addX, and (2) a copy of the environment around addX in which x has the value given in that specific invocation of add.\nfunction add(x)\n   function addX(y)\n       return y + x\n   return addX\n\nvariable add1 = add(1)\nvariable add5 = add(5)\n\nassert add1(3) = 4\nassert add5(3) = 8\nNote that, as add returns a value of function type, the variables add1 and add5 are also of function type. Invoking add1(3) will return 4, because it assigns 3 to parameter y in the call to addX, using the saved environment for addX where x is 1. Invoking add5(3) will return 8, because while it also assigns 3 to parameter y in the call to addX, it is now using another saved environment for addX where x is 5. So while add1 and add5 both use the same function addX, the associated environments differ, and invoking the closures will bind the name x to two different values in the two invocations, thus evaluating the function to two different results.History and etymology[edit]The concept of closures was developed in the 1960s for the mechanical evaluation of expressions in the \u03bb-calculus and was first[2] fully implemented in 1970 as a language feature in the PAL programming language to support lexically scoped first-class functions.[2]:Turner's section 2, note 8 contains his claim about M-expressionsPeter J. Landin defined the term closure in 1964 as having an environment part and a control part as used by his SECD machine for evaluating expressions.[3] Joel Moses credits Landin with introducing the term closure to refer to a lambda expression whose open bindings (free variables) have been closed by (or bound in) the lexical environment, resulting in a closed expression, or closure.[4][5] This usage was subsequently adopted by Sussman and Steele when they defined Scheme in 1975,[6] a lexically scoped variant of LISP, and became widespread.Anonymous functions[edit]The term closure is often mistakenly used to mean anonymous function. This is probably because many programmers learn about both concepts at the same time, in the form of small helper functions that are anonymous closures. An anonymous function is a function literal without a name, while a closure is an instance of a function, a value, whose non-local variables have been bound either to values or to storage locations (depending on the language; see the lexical environment section below).For example, in the following Python code:the values of a and b are closures, in both cases produced by returning a nested function with a free variable from the enclosing function, so that the free variable binds to the value of parameter x of the enclosing function. The closures in a and b are functionally identical. The only difference in implementation is that in the first case we used a nested function with a name, g, while in the second case we used an anonymous nested function (using the Python keyword lambda for creating an anonymous function). The original name, if any, used in defining them is irrelevant.A closure is a value like any other value. It doesn't need to be assigned to a variable and can instead be used directly, as shown in the last two lines of the example. This usage may be deemed an anonymous closure.Note especially that the nested function definitions are not themselves closures: they have a free variable which is not yet bound. Only once the enclosing function is evaluated with a value for the parameter is the free variable of the nested function bound, creating a closure, which is then returned from the enclosing function.Lastly, a closure is only distinct from a function with free variables when outside of the scope of the non-local variables, otherwise the defining environment and the execution environment coincide and there is nothing to distinguish these (static and dynamic binding can't be distinguished because the names resolve to the same values). For example, in the below program, functions with a free variable x (bound to the non-local variable x with global scope) are executed in the same environment where x is defined, so it is immaterial whether these are actually closures:This is most often achieved by a function return, since the function must be defined within the scope of the non-local variables, in which case typically its own scope will be smaller.This can also be achieved by variable shadowing (which reduces the scope of the non-local variable), though this is less common in practice, as it is less useful and shadowing is discouraged. In this example f can be seen to be a closure because x in the body of f is bound to the x in the global namespace, not the x local to g:Applications[edit]The use of closures is associated with languages where functions are first-class objects, in which functions can be returned as results from higher-order functions, or passed as arguments to other function calls; if functions with free variables are first-class, then returning one creates a closure. This includes functional programming languages such as Lisp and ML, as well as many modern garbage-collected imperative languages, such as Python. Closures are also frequently used with callbacks, particularly for event handlers, such as in JavaScript, where they are used for interactions with a dynamic web page. Traditional imperative languages such as Algol, C and Pascal either do not support nested functions (C) or do not support calling nested functions after the enclosing function has exited (GNU C, Pascal), thus avoiding the need to use closures.Closures are used to implement continuation-passing style, and in this manner, hide state. Constructs such as objects and control structures can thus be implemented with closures. In some languages, a closure may occur when a function is defined within another function, and the inner function refers to local variables of the outer function. At run-time, when the outer function executes, a closure is formed, consisting of the inner function\u2019s code and references (the upvalues) to any variables of the outer function required by the closure.First-class functions[edit]Closures typically appear in languages in which functions are first-class values\u2014in other words, such languages enable functions to be passed as arguments, returned from function calls, bound to variable names, etc., just like simpler types such as strings and integers. For example, consider the following Scheme function:In this example, the lambda expression (lambda (book) (>= (book-sales book) threshold)) appears within the function best-selling-books. When the lambda expression is evaluated, Scheme creates a closure consisting of the code for the lambda expression and a reference to the threshold variable, which is a free variable inside the lambda expression.The closure is then passed to the filter function, which calls it repeatedly to determine which books are to be added to the result list and which are to be discarded. Because the closure itself has a reference to threshold, it can use that variable each time filter calls it. The function filter itself might be defined in a completely separate file.Here is the same example rewritten in JavaScript, another popular language with support for closures:The function keyword is used here instead of lambda, and an Array.filter method[7] instead of a global filter function, but otherwise the structure and the effect of the code are the same.A function may create a closure and return it, as in the following example:Because the closure in this case outlives the execution of the function that creates it, the variables f and dx live on after the function derivative returns, even though execution has left their scope and they are no longer visible. In languages without closures, the lifetime of an automatic local variable coincides with the execution of the stack frame where that variable is declared. In languages with closures, variables must continue to exist as long as any existing closures have references to them. This is most commonly implemented using some form of garbage collection.State representation[edit]A closure can be used to associate a function with a set of private variables, which persist over several invocations of the function. The scope of the variable encompasses only the closed-over function, so it cannot be accessed from other program code.In stateful languages, closures can thus be used to implement paradigms for state representation and information hiding, since the closure's upvalues (its closed-over variables) are of indefinite extent, so a value established in one invocation remains available in the next. Closures used in this way no longer have referential transparency, and are thus no longer pure functions; nevertheless, they are commonly used in impure functional languages such as Scheme.Other uses[edit]Closures have many uses:\nBecause closures delay evaluation\u2014i.e., they do not do anything until they are called\u2014they can be used to define control structures. For example, all of Smalltalk's standard control structures, including branches (if/then/else) and loops (while and for), are defined using objects whose methods accept closures. Users can easily define their own control structures also.\nIn languages which implement assignment, multiple functions can be produced that close over the same environment, enabling them to communicate privately by altering that environment. In Scheme:\n\nClosures can be used to implement object systems.[8]\nNote: Some speakers call any data structure that binds a lexical environment a closure, but the term usually refers specifically to functions.Implementation and theory[edit]Closures are typically implemented with a special data structure that contains a pointer to the function code, plus a representation of the function's lexical environment (i.e., the set of available variables) at the time when the closure was created. The referencing environment binds the non-local names to the corresponding variables in the lexical environment at the time the closure is created, additionally extending their lifetime to at least as long as the lifetime of the closure itself. When the closure is entered at a later time, possibly with a different lexical environment, the function is executed with its non-local variables referring to the ones captured by the closure, not the current environment.A language implementation cannot easily support full closures if its run-time memory model allocates all automatic variables on a linear stack. In such languages, a function's automatic local variables are deallocated when the function returns. However, a closure requires that the free variables it references survive the enclosing function's execution. Therefore, those variables must be allocated so that they persist until no longer needed, typically via heap allocation, rather than on the stack, and their lifetime must be managed so they survive until all closures referencing them are no longer in use.This explains why, typically, languages that natively support closures also use garbage collection. The alternatives are manual memory management of non-local variables (explicitly allocating on the heap and freeing when done), or, if using stack allocation, for the language to accept that certain use cases will lead to undefined behaviour, due to dangling pointers to freed automatic variables, as in lambda expressions in C++11[9] or nested functions in GNU C.[10] The funarg problem (or functional argument problem) describes the difficulty of implementing functions as first class objects in a stack-based programming language such as C or C++. Similarly in D version 1, it is assumed that the programmer knows what to do with delegates and automatic local variables, as their references will be invalid after return from its definition scope (automatic local variables are on the stack) \u2013 this still permits many useful functional patterns, but for complex cases needs explicit heap allocation for variables. D version 2 solved this by detecting which variables must be stored on the heap, and performs automatic allocation. Because D uses garbage collection, in both versions, there is no need to track usage of variables as they are passed.In strict functional languages with immutable data (e.g. Erlang), it is very easy to implement automatic memory management (garbage collection), as there are no possible cycles in variables' references. For example, in Erlang, all arguments and variables are allocated on the heap, but references to them are additionally stored on the stack. After a function returns, references are still valid. Heap cleaning is done by incremental garbage collector.In ML, local variables are lexically scoped, and hence define a stack-like model, but since they are bound to values and not to objects, an implementation is free to copy these values into the closure's data structure in a way that is invisible to the programmer.Scheme, which has an ALGOL-like lexical scope system with dynamic variables and garbage collection, lacks a stack programming model and does not suffer from the limitations of stack-based languages. Closures are expressed naturally in Scheme. The lambda form encloses the code, and the free variables of its environment persist within the program as long as they can possibly be accessed, and so they can be used as freely as any other Scheme expression.[citation needed]Closures are closely related to Actors in the Actor model of concurrent computation where the values in the function's lexical environment are called acquaintances. An important issue for closures in concurrent programming languages is whether the variables in a closure can be updated and, if so, how these updates can be synchronized. Actors provide one solution.[11]Closures are closely related to function objects; the transformation from the former to the latter is known as defunctionalization or lambda lifting; see also closure conversion.[citation needed]Differences in semantics[edit]Lexical environment[edit]As different languages do not always have a common definition of the lexical environment, their definitions of closure may vary also. The commonly held minimalist definition of the lexical environment defines it as a set of all bindings of variables in the scope, and that is also what closures in any language have to capture. However the meaning of a variable binding also differs. In imperative languages, variables bind to relative locations in memory that can store values. Although the relative location of a binding does not change at runtime, the value in the bound location can. In such languages, since closure captures the binding, any operation on the variable, whether done from the closure or not, are performed on the same relative memory location. This is often called capturing the variable by reference. Here is an example illustrating the concept in ECMAScript, which is one such language:Note how function foo and the closures referred to by variables f and g all use the same relative memory location signified by local variable x.On the other hand, many functional languages, such as ML, bind variables directly to values. In this case, since there is no way to change the value of the variable once it is bound, there is no need to share the state between closures\u2014they just use the same values. This is often called capturing the variable by value. Java's local and anonymous classes also fall into this category\u2014they require captured local variables to be final, which also means there is no need to share state.Some languages enable you to choose between capturing the value of a variable or its location. For example, in C++11, captured variables are either declared with [&], which means captured by reference, or with [=], which means captured by value.Yet another subset, lazy functional languages such as Haskell, bind variables to results of future computations rather than values. Consider this example in Haskell:The binding of r captured by the closure defined within function foo is to the computation (x / y)\u2014which in this case results in division by zero. However, since it is the computation that is captured, and not the value, the error only manifests itself when the closure is invoked, and actually attempts to use the captured binding.Closure leaving[edit]Yet more differences manifest themselves in the behavior of other lexically scoped constructs, such as return, break and continue statements. Such constructs can, in general, be considered in terms of invoking an escape continuation established by an enclosing control statement (in case of break and continue, such interpretation requires looping constructs to be considered in terms of recursive function calls). In some languages, such as ECMAScript, return refers to the continuation established by the closure lexically innermost with respect to the statement\u2014thus, a return within a closure transfers control to the code that called it. However, in Smalltalk, the superficially similar operator ^ invokes the escape continuation established for the method invocation, ignoring the escape continuations of any intervening nested closures. The escape continuation of a particular closure can only be invoked in Smalltalk implicitly by reaching the end of the closure's code. The following examples in ECMAScript and Smalltalk highlight the difference:The above code snippets will behave differently because the Smalltalk ^ operator and the JavaScript return operator are not analogous. In the ECMAScript example, return x will leave the inner closure to begin a new iteration of the forEach loop, whereas in the Smalltalk example, ^x will abort the loop and return from the method foo.Common Lisp provides a construct that can express either of the above actions: Lisp (return-from foo x) behaves as Smalltalk ^x, while Lisp (return-from nil x) behaves as JavaScript return x. Hence, Smalltalk makes it possible for a captured escape continuation to outlive the extent in which it can be successfully invoked. Consider:When the closure returned by the method foo is invoked, it attempts to return a value from the invocation of foo that created the closure. Since that call has already returned and the Smalltalk method invocation model does not follow the spaghetti stack discipline to facilitate multiple returns, this operation results in an error.Some languages, such as Ruby, enable the programmer to choose the way return is captured. An example in Ruby:Both Proc.new and lambda in this example are ways to create a closure, but semantics of the closures thus created are different with respect to the return statement.In Scheme, definition and scope of the return control statement is explicit (and only arbitrarily named 'return' for the sake of the example). The following is a direct translation of the Ruby sample.Closure-like constructs[edit]Features of some languages simulate some features of closures. Language features include some object-oriented techniques, for example in Java, C++, Objective-C, C#, D.Callbacks (C)[edit]Some C libraries support callbacks. This is sometimes implemented by providing two values when registering the callback with the library: a function pointer and a separate void* pointer to arbitrary data of the user's choice. When the library executes the callback function, it passes along the data pointer. This enables the callback to maintain state and to refer to information captured at the time it was registered with the library. The idiom is similar to closures in functionality, but not in syntax. The void* pointer is not type safe so this C idiom differs from type-safe closures in C#, Haskell or ML.Nested function and function pointer(C)With a gcc extension, a nested function can be used and a function pointer can emulate closures, providing the containing function does not exit. The example below is invalid:Local classes and lambda functions (Java)[edit]Java enables classes to be defined inside methods. These are called local classes. When such classes are not named, they are known as anonymous classes (or anonymous inner classes). A local class (either named or anonymous) may refer to names in lexically enclosing classes, or read-only variables (marked as final) in the lexically enclosing method.The capturing of final variables enables you to capture variables by value. Even if the variable you want to capture is non-final, you can always copy it to a temporary final variable just before the class.Capturing of variables by reference can be emulated by using a final reference to a mutable container, for example, a single-element array. The local class will not be able to change the value of the container reference itself, but it will be able to change the contents of the container.With the advent of Java 8's lambda expressions,[12] the closure causes the above code to be executed as:Local classes are one of the types of inner class that are declared within the body of a method. Java also supports inner classes that are declared as non-static members of an enclosing class.[13] They are normally referred to just as inner classes.[14] These are defined in the body of the enclosing class and have full access to instance variables of the enclosing class. Due to their binding to these instance variables, an inner class may only be instantiated with an explicit binding to an instance of the enclosing class using a special syntax.[15]Upon execution, this will print the integers from 0 to 9. Beware to not confuse this type of class with the nested class, which is declared in the same way with an accompanied usage of the static modifier; those have not the desired effect but are instead just classes with no special binding defined in an enclosing class.As of Java 8, Java supports functions as first class objects. Lambda expressions of this form are considered of type Function<T,U> with T being the domain and U the image type. The expression can be called with its .apply(T t) method, but not with a standard method call.Blocks (C, C++, Objective-C 2.0)[edit]Apple introduced Blocks, a form of closure, as a nonstandard extension into C, C++, Objective-C 2.0 and in Mac OS X 10.6 Snow Leopard and iOS 4.0. Apple made their implementation available for the GCC and clang compilers.Pointers to block and block literals are marked with ^. Normal local variables are captured by value when the block is created, and are read-only inside the block. Variables to be captured by reference are marked with __block. Blocks that need to persist outside of the scope they are created in may need to be copied.[16][17]Delegates (C#, D)[edit]C# anonymous methods and lambda expressions support closure:In D, closures are implemented by delegates, a function pointer paired with a context pointer (e.g. a class instance, or a stack frame on the heap in the case of closures).D version 1, has limited closure support. For example, the above code will not work correctly, because the variable a is on the stack, and after returning from test(), it is no longer valid to use it (most probably calling foo via dg(), will return a 'random' integer). This can be solved by explicitly allocating the variable 'a' on heap, or using structs or class to store all needed closed variables and construct a delegate from a method implementing the same code. Closures can be passed to other functions, as long as they are only used while the referenced values are still valid (for example calling another function with a closure as a callback parameter), and are useful for writing generic data processing code, so this limitation, in practice, is often not an issue.This limitation was fixed in D version 2 - the variable 'a' will be automatically allocated on the heap because it is used in the inner function, and a delegate of that function can escape the current scope (via assignment to dg or return). Any other local variables (or arguments) that are not referenced by delegates or that are only referenced by delegates that don't escape the current scope, remain on the stack, which is simpler and faster than heap allocation. The same is true for inner's class methods that references a function's variables.Function objects (C++)[edit]C++ enables defining function objects by overloading operator(). These objects behave somewhat like functions in a functional programming language. They may be created at runtime and may contain state, but they do not implicitly capture local variables as closures do. As of the 2011 revision, the C++ language also supports closures, which are a type of function object constructed automatically from a special language construct called lambda-expression. A C++ closure may capture its context either by storing copies of the accessed variables as members of the closure object or by reference. In the latter case, if the closure object escapes the scope of a referenced object, invoking its operator() causes undefined behavior since C++ closures do not extend the lifetime of their context.Inline agents (Eiffel)[edit]Eiffel includes inline agents defining closures. An inline agent is an object representing a routine, defined by giving the code of the routine in-line. For example, inthe argument to subscribe is an agent, representing a procedure with two arguments; the procedure finds the country at the corresponding coordinates and displays it. The whole agent is subscribed to the event type click_event for a certain button, so that whenever an instance of the event type occurs on that button \u2014 because a user has clicked the button \u2014 the procedure will be executed with the mouse coordinates being passed as arguments for x and y.The main limitation of Eiffel agents, which distinguishes them from closures in other languages, is that they cannot reference local variables from the enclosing scope. This design decision helps in avoiding ambiguity when talking about a local variable value in a closure - should it be the latest value of the variable or the value captured when the agent is created? Only Current (a reference to current object, analogous to this in Java), its features, and arguments of the agent itself can be accessed from within the agent body. The values of the outer local variables can be passed by providing additional closed operands to the agent.See also[edit]\nAnonymous function\nBlocks (C language extension)\nCommand pattern\nContinuation\nCurrying\nFunarg problem\nLambda calculus\nLazy evaluation\nPartial application\nSpaghetti stack\nSyntactic closure\nValue-level programming\nNotes[edit]References[edit]External links[edit]\nThe Original Lambda Papers: A classic series of papers by Guy Steele and Gerald Sussman discussing, among other things, the versatility of closures in the context of Scheme (where they appear as lambda expressions).\nNeal Gafter (2007-01-28). A Definition of Closures. \nGilad Bracha, Neal Gafter, James Gosling, Peter von der Ahe\u0301. Closures for the Java Programming Language (v0.5). CS1 maint: Multiple names: authors list (link)\nClosures: An article about closures in dynamically typed imperative languages, by Martin Fowler.\nCollection closure methods: An example of a technical domain where using closures is convenient, by Martin Fowler.\n", "subtitles": ["History and etymology", "Anonymous functions", "Applications", "Implementation and theory", "Differences in semantics", "Closure-like constructs", "See also", "Notes", "References", "External links"], "title": "Closure (computer programming)"},
{"content": "In computability theory, the Ackermann function, named after Wilhelm Ackermann, is one of the simplest[1] and earliest-discovered examples of a total computable function that is not primitive recursive. All primitive recursive functions are total and computable, but the Ackermann function illustrates that not all total computable functions are primitive recursive.After Ackermann's publication[2] of his function (which had three nonnegative integer arguments), many authors modified it to suit various purposes, so that today the Ackermann function may refer to any of numerous variants of the original function. One common version, the two-argument Ackermann\u2013Pe\u0301ter function, is defined as follows for nonnegative integers m and n:\n\n  \n    \n      \n        A\n        (\n        m\n        ,\n        n\n        )\n        =\n        \n          \n            {\n            \n              \n                \n                  n\n                  +\n                  1\n                \n                \n                  \n                    \n                      if \n                    \n                  \n                  m\n                  =\n                  0\n                \n              \n              \n                \n                  A\n                  (\n                  m\n                  \u2212\n                  1\n                  ,\n                  1\n                  )\n                \n                \n                  \n                    \n                      if \n                    \n                  \n                  m\n                  >\n                  0\n                  \n                    \n                       and \n                    \n                  \n                  n\n                  =\n                  0\n                \n              \n              \n                \n                  A\n                  (\n                  m\n                  \u2212\n                  1\n                  ,\n                  A\n                  (\n                  m\n                  ,\n                  n\n                  \u2212\n                  1\n                  )\n                  )\n                \n                \n                  \n                    \n                      if \n                    \n                  \n                  m\n                  >\n                  0\n                  \n                    \n                       and \n                    \n                  \n                  n\n                  >\n                  0.\n                \n              \n            \n            \n          \n        \n      \n    \n    {\\displaystyle A(m,n)={\\begin{cases}n+1&{\\mbox{if }}m=0\\\\A(m-1,1)&{\\mbox{if }}m>0{\\mbox{ and }}n=0\\\\A(m-1,A(m,n-1))&{\\mbox{if }}m>0{\\mbox{ and }}n>0.\\end{cases}}}\n  \n\nIts value grows rapidly, even for small inputs. For example, A(4,2) is an integer of 19,729 decimal digits.[3]History[edit]In the late 1920s, the mathematicians Gabriel Sudan and Wilhelm Ackermann, students of David Hilbert, were studying the foundations of computation. Both Sudan and Ackermann are credited[4] with discovering total computable functions (termed simply recursive in some references) that are not primitive recursive. Sudan published the lesser-known Sudan function, then shortly afterwards and independently, in 1928, Ackermann published his function \n  \n    \n      \n        \u03c6\n      \n    \n    {\\displaystyle \\varphi }\n  \n (the Greek letter phi). Ackermann's three-argument function, \n  \n    \n      \n        \u03c6\n        (\n        m\n        ,\n        n\n        ,\n        p\n        )\n      \n    \n    {\\displaystyle \\varphi (m,n,p)}\n  \n, is defined such that for p = 0, 1, 2, it reproduces the basic operations of addition, multiplication, and exponentiation as\n\n  \n    \n      \n        \u03c6\n        (\n        m\n        ,\n        n\n        ,\n        0\n        )\n        =\n        m\n        +\n        n\n        ,\n      \n    \n    {\\displaystyle \\varphi (m,n,0)=m+n,}\n  \n\n\n  \n    \n      \n        \u03c6\n        (\n        m\n        ,\n        n\n        ,\n        1\n        )\n        =\n        m\n        \u22c5\n        n\n        ,\n      \n    \n    {\\displaystyle \\varphi (m,n,1)=m\\cdot n,}\n  \n\n\n  \n    \n      \n        \u03c6\n        (\n        m\n        ,\n        n\n        ,\n        2\n        )\n        =\n        \n          m\n          \n            n\n          \n        \n        ,\n      \n    \n    {\\displaystyle \\varphi (m,n,2)=m^{n},}\n  \n\nand for p > 2 it extends these basic operations in a way that can be compared to the hyperoperations:\n\n  \n    \n      \n        \u03c6\n        (\n        m\n        ,\n        n\n        ,\n        3\n        )\n        =\n        m\n        \n          \u2191\n          \n            2\n          \n        \n        (\n        n\n        +\n        1\n        )\n        ,\n        \n        \n      \n    \n    {\\displaystyle \\varphi (m,n,3)=m\\uparrow ^{2}(n+1),\\,\\!}\n  \n\n\n  \n    \n      \n        \u03c6\n        (\n        m\n        ,\n        n\n        ,\n        p\n        )\n        \u2a86\n        m\n        \n          \u2191\n          \n            p\n            \u2212\n            1\n          \n        \n        (\n        n\n        +\n        1\n        )\n         \n         \n        (\n        p\n        \u2265\n        4\n        )\n        .\n        \n        \n      \n    \n    {\\displaystyle \\varphi (m,n,p)\\gtrapprox m\\uparrow ^{p-1}(n+1)\\ \\ (p\\geq 4).\\,\\!}\n  \n\n(Aside from its historic role as a total-computable-but-not-primitive-recursive function, Ackermann's original function is seen to extend the basic arithmetic operations beyond exponentiation, although not as seamlessly as do variants of Ackermann's function that are specifically designed for that purpose\u2014such as Goodstein's hyperoperation sequence.)In On the Infinite, David Hilbert hypothesized that the Ackermann function was not primitive recursive, but it was Ackermann, Hilbert\u2019s personal secretary and former student, who actually proved the hypothesis in his paper On Hilbert\u2019s Construction of the Real Numbers.[2][5]Ro\u0301zsa Pe\u0301ter and Raphael Robinson later developed a two-variable version of the Ackermann function that became preferred by many authors.[6]Definition and properties[edit]Ackermann's original three-argument function \n  \n    \n      \n        \u03c6\n        (\n        m\n        ,\n        n\n        ,\n        p\n        )\n      \n    \n    {\\displaystyle \\varphi (m,n,p)}\n  \n is defined recursively as follows for nonnegative integers m, n, and p:\n\n  \n    \n      \n        \n          \n            \n              \n                \u03c6\n                (\n                m\n                ,\n                n\n                ,\n                0\n                )\n                =\n                m\n                +\n                n\n              \n            \n            \n              \n                \u03c6\n                (\n                m\n                ,\n                0\n                ,\n                1\n                )\n                =\n                0\n              \n            \n            \n              \n                \u03c6\n                (\n                m\n                ,\n                0\n                ,\n                2\n                )\n                =\n                1\n              \n            \n            \n              \n                \u03c6\n                (\n                m\n                ,\n                0\n                ,\n                p\n                )\n                =\n                m\n                \n                   for \n                \n                p\n                >\n                2\n              \n            \n            \n              \n                \u03c6\n                (\n                m\n                ,\n                n\n                ,\n                p\n                )\n                =\n                \u03c6\n                (\n                m\n                ,\n                \u03c6\n                (\n                m\n                ,\n                n\n                \u2212\n                1\n                ,\n                p\n                )\n                ,\n                p\n                \u2212\n                1\n                )\n                \n                   for \n                \n                n\n                >\n                0\n                \n                   and \n                \n                p\n                >\n                0.\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{array}{lr}\\varphi (m,n,0)=m+n\\\\\\varphi (m,0,1)=0\\\\\\varphi (m,0,2)=1\\\\\\varphi (m,0,p)=m{\\text{ for }}p>2\\\\\\varphi (m,n,p)=\\varphi (m,\\varphi (m,n-1,p),p-1){\\text{ for }}n>0{\\text{ and }}p>0.\\end{array}}}\n  \n\nOf the various two-argument versions, the one developed by Pe\u0301ter and Robinson (called the Ackermann function by some authors) is defined for nonnegative integers m and n as follows:\n\n  \n    \n      \n        A\n        (\n        m\n        ,\n        n\n        )\n        =\n        \n          \n            {\n            \n              \n                \n                  n\n                  +\n                  1\n                \n                \n                  \n                    \n                      if \n                    \n                  \n                  m\n                  =\n                  0\n                \n              \n              \n                \n                  A\n                  (\n                  m\n                  \u2212\n                  1\n                  ,\n                  1\n                  )\n                \n                \n                  \n                    \n                      if \n                    \n                  \n                  m\n                  >\n                  0\n                  \n                    \n                       and \n                    \n                  \n                  n\n                  =\n                  0\n                \n              \n              \n                \n                  A\n                  (\n                  m\n                  \u2212\n                  1\n                  ,\n                  A\n                  (\n                  m\n                  ,\n                  n\n                  \u2212\n                  1\n                  )\n                  )\n                \n                \n                  \n                    \n                      if \n                    \n                  \n                  m\n                  >\n                  0\n                  \n                    \n                       and \n                    \n                  \n                  n\n                  >\n                  0.\n                \n              \n            \n            \n          \n        \n      \n    \n    {\\displaystyle A(m,n)={\\begin{cases}n+1&{\\mbox{if }}m=0\\\\A(m-1,1)&{\\mbox{if }}m>0{\\mbox{ and }}n=0\\\\A(m-1,A(m,n-1))&{\\mbox{if }}m>0{\\mbox{ and }}n>0.\\end{cases}}}\n  \n\nIt may not be immediately obvious that the evaluation of \n  \n    \n      \n        A\n        (\n        m\n        ,\n        n\n        )\n      \n    \n    {\\displaystyle A(m,n)}\n  \n always terminates. However, the recursion is bounded because in each recursive application either m decreases, or m remains the same and n decreases. Each time that n reaches zero, m decreases, so m eventually reaches zero as well. (Expressed more technically, in each case the pair (m, n) decreases in the lexicographic order on pairs, which is a well-ordering, just like the ordering of single non-negative integers; this means one cannot go down in the ordering infinitely many times in succession.) However, when m decreases there is no upper bound on how much n can increase\u2014and it will often increase greatly.The Pe\u0301ter-Ackermann function can also be expressed in terms of various other versions of the Ackermann function:\nthe indexed version of Knuth's up-arrow notation (extended to integer indices \u2265 -2):\n\n\n\n\n  \n    \n      \n        A\n        (\n        m\n        ,\n        n\n        )\n        =\n        2\n        \n          \u2191\n          \n            m\n            \u2212\n            2\n          \n        \n        (\n        n\n        +\n        3\n        )\n        \u2212\n        3.\n      \n    \n    {\\displaystyle A(m,n)=2\\uparrow ^{m-2}(n+3)-3.}\n  \n\n\n\nThe part of the definition A(m, 0) = A(m-1, 1) corresponds to \n  \n    \n      \n        2\n        \n          \u2191\n          \n            m\n            +\n            1\n          \n        \n        3\n        =\n        2\n        \n          \u2191\n          \n            m\n          \n        \n        4.\n      \n    \n    {\\displaystyle 2\\uparrow ^{m+1}3=2\\uparrow ^{m}4.}\n  \n\n\nConway chained arrow notation:\n\n\n\n\n  \n    \n      \n        A\n        (\n        m\n        ,\n        n\n        )\n        =\n        (\n        2\n        \u2192\n        (\n        n\n        +\n        3\n        )\n        \u2192\n        (\n        m\n        \u2212\n        2\n        )\n        )\n        \u2212\n        3\n      \n    \n    {\\displaystyle A(m,n)=(2\\rightarrow (n+3)\\rightarrow (m-2))-3}\n  \n for \n  \n    \n      \n        m\n        \u2265\n        3\n      \n    \n    {\\displaystyle m\\geq 3}\n  \n\n\n\nhence\n\n\n  \n    \n      \n        2\n        \u2192\n        n\n        \u2192\n        m\n        =\n        A\n        (\n        m\n        +\n        2\n        ,\n        n\n        \u2212\n        3\n        )\n        +\n        3\n      \n    \n    {\\displaystyle 2\\rightarrow n\\rightarrow m=A(m+2,n-3)+3}\n  \n for \n  \n    \n      \n        n\n        >\n        2\n      \n    \n    {\\displaystyle n>2}\n  \n.\n\n\n(n=1 and n=2 would correspond with A(m,\u22122) = \u22121 and A(m,\u22121) = 1, which could logically be added.)\nFor small values of m like 1, 2, or 3, the Ackermann function grows relatively slowly with respect to n (at most exponentially). For m \u2265 4, however, it grows much more quickly; even A(4, 2) is about 2\u00d71019728, and the decimal expansion of A(4, 3) is very large by any typical measure.One interesting aspect of the Ackermann function is that the only arithmetic operations it ever uses are addition and subtraction of 1. Its properties come solely from the power of unlimited recursion. This also implies that its running time is at least proportional to its output, and so is also extremely huge. In actuality, for most cases the running time is far larger than the output; see below.A single-argument version f(n) = A(n, n) that increases both m and n at the same time dwarfs every primitive recursive function, including very fast-growing functions such as the exponential function, the factorial function, multi- and superfactorial functions, and even functions defined using Knuth's up-arrow notation (except when the indexed up-arrow is used). It can be seen that f(n) is roughly comparable to f\u03c9(n) in the fast-growing hierarchy. This extreme growth can be exploited to show that f, which is obviously computable on a machine with infinite memory such as a Turing machine and so is a computable function, grows faster than any primitive recursive function and is therefore not primitive recursive.In a category with exponentials, using the isomorphism \n  \n    \n      \n        (\n        (\n        X\n        \u00d7\n        Y\n        )\n        \u2192\n        Z\n        )\n        \u2245\n        (\n        X\n        \u2192\n        (\n        Y\n        \u2192\n        Z\n        )\n        )\n      \n    \n    {\\displaystyle ((X\\times Y)\\rightarrow Z)\\cong (X\\rightarrow (Y\\rightarrow Z))}\n  \n (in computer science, this is called currying), the Ackermann function may be defined via primitive recursion over higher-order functionals as follows:\n\n  \n    \n      \n        \n          \n            \n              \n                Ack\n                \u2061\n                (\n                0\n                )\n              \n              \n                =\n              \n              \n                S\n              \n            \n            \n              \n                Ack\n                \u2061\n                (\n                m\n                +\n                1\n                )\n              \n              \n                =\n              \n              \n                Iter\n                \u2061\n                (\n                Ack\n                \u2061\n                (\n                m\n                )\n                )\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{array}{lcl}\\operatorname {Ack} (0)&=&\\operatorname {S} \\\\\\operatorname {Ack} (m+1)&=&\\operatorname {Iter} (\\operatorname {Ack} (m))\\end{array}}}\n  \n\nwhere S(n) = n + 1 is the usual successor function and Iter denotes the functional power operator, defined by primitive recursion as well:\n\n  \n    \n      \n        \n          \n            \n              \n                Iter\n                \u2061\n                (\n                f\n                )\n                (\n                0\n                )\n              \n              \n                =\n              \n              \n                f\n                (\n                1\n                )\n              \n            \n            \n              \n                Iter\n                \u2061\n                (\n                f\n                )\n                (\n                n\n                +\n                1\n                )\n              \n              \n                =\n              \n              \n                f\n                (\n                Iter\n                \u2061\n                (\n                f\n                )\n                (\n                n\n                )\n                )\n                .\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{array}{lcl}\\operatorname {Iter} (f)(0)&=&f(1)\\\\\\operatorname {Iter} (f)(n+1)&=&f(\\operatorname {Iter} (f)(n)).\\end{array}}}\n  \n\nThe function \n  \n    \n      \n        \n          A\n          c\n          k\n        \n      \n    \n    {\\displaystyle \\mathrm {Ack} }\n  \n defined in this way agrees with the Ackermann function \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n defined above: \n  \n    \n      \n        \n          A\n          c\n          k\n        \n        (\n        m\n        )\n        (\n        n\n        )\n        =\n        A\n        (\n        m\n        ,\n        n\n        )\n      \n    \n    {\\displaystyle \\mathrm {Ack} (m)(n)=A(m,n)}\n  \n.Example expansions[edit]To see how the Ackermann function grows so quickly, it helps to expand out some simple expressions using the rules in the original definition. For example, we can fully evaluate \n  \n    \n      \n        A\n        (\n        1\n        ,\n        2\n        )\n      \n    \n    {\\displaystyle A(1,2)}\n  \n in the following way:\n\n  \n    \n      \n        \n          \n            \n              \n                A\n                (\n                1\n                ,\n                2\n                )\n              \n              \n                \n                =\n                A\n                (\n                0\n                ,\n                A\n                (\n                1\n                ,\n                1\n                )\n                )\n              \n            \n            \n              \n              \n                \n                =\n                A\n                (\n                0\n                ,\n                A\n                (\n                0\n                ,\n                A\n                (\n                1\n                ,\n                0\n                )\n                )\n                )\n              \n            \n            \n              \n              \n                \n                =\n                A\n                (\n                0\n                ,\n                A\n                (\n                0\n                ,\n                A\n                (\n                0\n                ,\n                1\n                )\n                )\n                )\n              \n            \n            \n              \n              \n                \n                =\n                A\n                (\n                0\n                ,\n                A\n                (\n                0\n                ,\n                2\n                )\n                )\n              \n            \n            \n              \n              \n                \n                =\n                A\n                (\n                0\n                ,\n                3\n                )\n              \n            \n            \n              \n              \n                \n                =\n                4.\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}A(1,2)&=A(0,A(1,1))\\\\&=A(0,A(0,A(1,0)))\\\\&=A(0,A(0,A(0,1)))\\\\&=A(0,A(0,2))\\\\&=A(0,3)\\\\&=4.\\end{aligned}}}\n  \n\nTo demonstrate how \n  \n    \n      \n        A\n        (\n        4\n        ,\n        3\n        )\n      \n    \n    {\\displaystyle A(4,3)}\n  \n's computation results in many steps and in a large number:\n\n  \n    \n      \n        \n          \n            \n              \n                A\n                (\n                4\n                ,\n                3\n                )\n              \n              \n                \n                =\n                A\n                (\n                3\n                ,\n                A\n                (\n                4\n                ,\n                2\n                )\n                )\n              \n            \n            \n              \n              \n                \n                =\n                A\n                (\n                3\n                ,\n                A\n                (\n                3\n                ,\n                A\n                (\n                4\n                ,\n                1\n                )\n                )\n                )\n              \n            \n            \n              \n              \n                \n                =\n                A\n                (\n                3\n                ,\n                A\n                (\n                3\n                ,\n                A\n                (\n                3\n                ,\n                A\n                (\n                4\n                ,\n                0\n                )\n                )\n                )\n                )\n              \n            \n            \n              \n              \n                \n                =\n                A\n                (\n                3\n                ,\n                A\n                (\n                3\n                ,\n                A\n                (\n                3\n                ,\n                A\n                (\n                3\n                ,\n                1\n                )\n                )\n                )\n                )\n              \n            \n            \n              \n              \n                \n                =\n                A\n                (\n                3\n                ,\n                A\n                (\n                3\n                ,\n                A\n                (\n                3\n                ,\n                A\n                (\n                2\n                ,\n                A\n                (\n                3\n                ,\n                0\n                )\n                )\n                )\n                )\n                )\n              \n            \n            \n              \n              \n                \n                =\n                A\n                (\n                3\n                ,\n                A\n                (\n                3\n                ,\n                A\n                (\n                3\n                ,\n                A\n                (\n                2\n                ,\n                A\n                (\n                2\n                ,\n                1\n                )\n                )\n                )\n                )\n                )\n              \n            \n            \n              \n              \n                \n                =\n                A\n                (\n                3\n                ,\n                A\n                (\n                3\n                ,\n                A\n                (\n                3\n                ,\n                A\n                (\n                2\n                ,\n                A\n                (\n                1\n                ,\n                A\n                (\n                2\n                ,\n                0\n                )\n                )\n                )\n                )\n                )\n                )\n              \n            \n            \n              \n              \n                \n                =\n                A\n                (\n                3\n                ,\n                A\n                (\n                3\n                ,\n                A\n                (\n                3\n                ,\n                A\n                (\n                2\n                ,\n                A\n                (\n                1\n                ,\n                A\n                (\n                1\n                ,\n                1\n                )\n                )\n                )\n                )\n                )\n                )\n              \n            \n            \n              \n              \n                \n                =\n                A\n                (\n                3\n                ,\n                A\n                (\n                3\n                ,\n                A\n                (\n                3\n                ,\n                A\n                (\n                2\n                ,\n                A\n                (\n                1\n                ,\n                A\n                (\n                0\n                ,\n                A\n                (\n                1\n                ,\n                0\n                )\n                )\n                )\n                )\n                )\n                )\n                )\n              \n            \n            \n              \n              \n                \n                =\n                A\n                (\n                3\n                ,\n                A\n                (\n                3\n                ,\n                A\n                (\n                3\n                ,\n                A\n                (\n                2\n                ,\n                A\n                (\n                1\n                ,\n                A\n                (\n                0\n                ,\n                A\n                (\n                0\n                ,\n                1\n                )\n                )\n                )\n                )\n                )\n                )\n                )\n              \n            \n            \n              \n              \n                \n                =\n                A\n                (\n                3\n                ,\n                A\n                (\n                3\n                ,\n                A\n                (\n                3\n                ,\n                A\n                (\n                2\n                ,\n                A\n                (\n                1\n                ,\n                A\n                (\n                0\n                ,\n                2\n                )\n                )\n                )\n                )\n                )\n                )\n              \n            \n            \n              \n              \n                \n                =\n                A\n                (\n                3\n                ,\n                A\n                (\n                3\n                ,\n                A\n                (\n                3\n                ,\n                A\n                (\n                2\n                ,\n                A\n                (\n                1\n                ,\n                3\n                )\n                )\n                )\n                )\n                )\n              \n            \n            \n              \n              \n                \n                =\n                A\n                (\n                3\n                ,\n                A\n                (\n                3\n                ,\n                A\n                (\n                3\n                ,\n                A\n                (\n                2\n                ,\n                A\n                (\n                0\n                ,\n                A\n                (\n                1\n                ,\n                2\n                )\n                )\n                )\n                )\n                )\n                )\n              \n            \n            \n              \n              \n                \n                =\n                A\n                (\n                3\n                ,\n                A\n                (\n                3\n                ,\n                A\n                (\n                3\n                ,\n                A\n                (\n                2\n                ,\n                A\n                (\n                0\n                ,\n                A\n                (\n                0\n                ,\n                A\n                (\n                1\n                ,\n                1\n                )\n                )\n                )\n                )\n                )\n                )\n                )\n              \n            \n            \n              \n              \n                \n                =\n                A\n                (\n                3\n                ,\n                A\n                (\n                3\n                ,\n                A\n                (\n                3\n                ,\n                A\n                (\n                2\n                ,\n                A\n                (\n                0\n                ,\n                A\n                (\n                0\n                ,\n                A\n                (\n                0\n                ,\n                A\n                (\n                1\n                ,\n                0\n                )\n                )\n                )\n                )\n                )\n                )\n                )\n                )\n              \n            \n            \n              \n              \n                \n                =\n                A\n                (\n                3\n                ,\n                A\n                (\n                3\n                ,\n                A\n                (\n                3\n                ,\n                A\n                (\n                2\n                ,\n                A\n                (\n                0\n                ,\n                A\n                (\n                0\n                ,\n                A\n                (\n                0\n                ,\n                A\n                (\n                0\n                ,\n                1\n                )\n                )\n                )\n                )\n                )\n                )\n                )\n                )\n              \n            \n            \n              \n              \n                \n                =\n                A\n                (\n                3\n                ,\n                A\n                (\n                3\n                ,\n                A\n                (\n                3\n                ,\n                A\n                (\n                2\n                ,\n                A\n                (\n                0\n                ,\n                A\n                (\n                0\n                ,\n                A\n                (\n                0\n                ,\n                2\n                )\n                )\n                )\n                )\n                )\n                )\n                )\n              \n            \n            \n              \n              \n                \n                =\n                A\n                (\n                3\n                ,\n                A\n                (\n                3\n                ,\n                A\n                (\n                3\n                ,\n                A\n                (\n                2\n                ,\n                A\n                (\n                0\n                ,\n                A\n                (\n                0\n                ,\n                3\n                )\n                )\n                )\n                )\n                )\n                )\n              \n            \n            \n              \n              \n                \n                =\n                A\n                (\n                3\n                ,\n                A\n                (\n                3\n                ,\n                A\n                (\n                3\n                ,\n                A\n                (\n                2\n                ,\n                A\n                (\n                0\n                ,\n                4\n                )\n                )\n                )\n                )\n                )\n              \n            \n            \n              \n              \n                \n                =\n                A\n                (\n                3\n                ,\n                A\n                (\n                3\n                ,\n                A\n                (\n                3\n                ,\n                A\n                (\n                2\n                ,\n                5\n                )\n                )\n                )\n                )\n              \n            \n            \n              \n              \n                \n                =\n                ...\n              \n            \n            \n              \n              \n                \n                =\n                A\n                (\n                3\n                ,\n                A\n                (\n                3\n                ,\n                A\n                (\n                3\n                ,\n                13\n                )\n                )\n                )\n              \n            \n            \n              \n              \n                \n                =\n                ...\n              \n            \n            \n              \n              \n                \n                =\n                A\n                (\n                3\n                ,\n                A\n                (\n                3\n                ,\n                65533\n                )\n                )\n              \n            \n            \n              \n              \n                \n                =\n                ...\n              \n            \n            \n              \n              \n                \n                =\n                A\n                (\n                3\n                ,\n                \n                  2\n                  \n                    65536\n                  \n                \n                \u2212\n                3\n                )\n              \n            \n            \n              \n              \n                \n                =\n                ...\n              \n            \n            \n              \n              \n                \n                =\n                \n                  2\n                  \n                    \n                      2\n                      \n                        \n                          \n                          65536\n                        \n                      \n                    \n                  \n                \n                \u2212\n                3.\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}A(4,3)&=A(3,A(4,2))\\\\&=A(3,A(3,A(4,1)))\\\\&=A(3,A(3,A(3,A(4,0))))\\\\&=A(3,A(3,A(3,A(3,1))))\\\\&=A(3,A(3,A(3,A(2,A(3,0)))))\\\\&=A(3,A(3,A(3,A(2,A(2,1)))))\\\\&=A(3,A(3,A(3,A(2,A(1,A(2,0))))))\\\\&=A(3,A(3,A(3,A(2,A(1,A(1,1))))))\\\\&=A(3,A(3,A(3,A(2,A(1,A(0,A(1,0)))))))\\\\&=A(3,A(3,A(3,A(2,A(1,A(0,A(0,1)))))))\\\\&=A(3,A(3,A(3,A(2,A(1,A(0,2))))))\\\\&=A(3,A(3,A(3,A(2,A(1,3)))))\\\\&=A(3,A(3,A(3,A(2,A(0,A(1,2))))))\\\\&=A(3,A(3,A(3,A(2,A(0,A(0,A(1,1)))))))\\\\&=A(3,A(3,A(3,A(2,A(0,A(0,A(0,A(1,0))))))))\\\\&=A(3,A(3,A(3,A(2,A(0,A(0,A(0,A(0,1))))))))\\\\&=A(3,A(3,A(3,A(2,A(0,A(0,A(0,2)))))))\\\\&=A(3,A(3,A(3,A(2,A(0,A(0,3))))))\\\\&=A(3,A(3,A(3,A(2,A(0,4)))))\\\\&=A(3,A(3,A(3,A(2,5))))\\\\&=\\ldots \\\\&=A(3,A(3,A(3,13)))\\\\&=\\ldots \\\\&=A(3,A(3,65533))\\\\&=\\ldots \\\\&=A(3,2^{65536}-3)\\\\&=\\ldots \\\\&=2^{2^{\\overset {65536}{}}}-3.\\\\\\end{aligned}}}\n  \n\nTable of values[edit]Computing the Ackermann function can be restated in terms of an infinite table. We place the natural numbers along the top row. To determine a number in the table, take the number immediately to the left, then look up the required number in the previous row, at the position given by the number just taken. If there is no number to its left, simply look at the column headed 1 in the previous row. Here is a small upper-left portion of the table:The numbers here which are only expressed with recursive exponentiation or Knuth arrows are very large and would take up too much space to notate in plain decimal digits.Despite the large values occurring in this early section of the table, some even larger numbers have been defined, such as Graham's number, which cannot be written with any small number of Knuth arrows. This number is constructed with a technique similar to applying the Ackermann function to itself recursively.This is a repeat of the above table, but with the values replaced by the relevant expression from the function definition to show the pattern clearly:Proof that the Ackermann function is not primitive recursive[edit]In a sense, the Ackermann function grows faster than any primitive recursive function and therefore is not itself primitive recursive.Specifically, one shows that to every primitive recursive function \n  \n    \n      \n        f\n        (\n        \n          x\n          \n            1\n          \n        \n        ,\n        ...\n        ,\n        \n          x\n          \n            n\n          \n        \n        )\n      \n    \n    {\\displaystyle f(x_{1},\\ldots ,x_{n})}\n  \n there exists a non-negative integer \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n  \n such that for all non-negative integers \n  \n    \n      \n        \n          x\n          \n            1\n          \n        \n        ,\n        ...\n        ,\n        \n          x\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle x_{1},\\ldots ,x_{n}}\n  \n we have \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n\n  \n    \n      \n        \n          x\n          \n            1\n          \n        \n        =\n        \n          x\n          \n            2\n          \n        \n        =\n        t\n      \n    \n    {\\displaystyle x_{1}=x_{2}=t}\n  \n\n  \n    \n      \n        A\n        (\n        t\n        ,\n        t\n        )\n        <\n        A\n        (\n        t\n        ,\n        t\n        )\n      \n    \n    {\\displaystyle A(t,t)<A(t,t)}\n  \nThe proof[7] proceeds as follows: we define the class \n  \n    \n      \n        \n          \n            A\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {A}}}\n  \n of all functions that grow slower than the Ackermann function\n  \n    \n      \n        \n          \n            A\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {A}}}\n  \n\n  \n    \n      \n        \n          \n            A\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {A}}}\n  \nInverse[edit]Since the function  f (n) = A(n, n) considered above grows very rapidly, its inverse function, f\u22121, grows very slowly. This inverse Ackermann function f\u22121 is usually denoted by \u03b1. In fact, \u03b1(n) is less than 5 for any practical input size n, since A(4, 4) is on the order of \n  \n    \n      \n        \n          2\n          \n            \n              2\n              \n                \n                  2\n                  \n                    \n                      2\n                      \n                        16\n                      \n                    \n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle 2^{2^{2^{2^{16}}}}}\n  \n.This inverse appears in the time complexity of some algorithms, such as the disjoint-set data structure and Chazelle's algorithm for minimum spanning trees. Sometimes Ackermann's original function or other variations are used in these settings, but they all grow at similarly high rates. In particular, some modified functions simplify the expression by eliminating the \u22123 and similar terms.A two-parameter variation of the inverse Ackermann function can be defined as follows, where \n  \n    \n      \n        \u230a\n        x\n        \u230b\n      \n    \n    {\\displaystyle \\lfloor x\\rfloor }\n  \n is the floor function:\n\n  \n    \n      \n        \u03b1\n        (\n        m\n        ,\n        n\n        )\n        =\n        min\n        {\n        i\n        \u2265\n        1\n        :\n        A\n        (\n        i\n        ,\n        \u230a\n        m\n        \n          /\n        \n        n\n        \u230b\n        )\n        \u2265\n        \n          log\n          \n            2\n          \n        \n        \u2061\n        n\n        }\n        .\n      \n    \n    {\\displaystyle \\alpha (m,n)=\\min\\{i\\geq 1:A(i,\\lfloor m/n\\rfloor )\\geq \\log _{2}n\\}.}\n  \n\nThis function arises in more precise analyses of the algorithms mentioned above, and gives a more refined time bound. In the disjoint-set data structure, m represents the number of operations while n represents the number of elements; in the minimum spanning tree algorithm, m represents the number of edges while n represents the number of vertices. Several slightly different definitions of \u03b1(m, n) exist; for example, log2 n is sometimes replaced by n, and the floor function is sometimes replaced by a ceiling.Other studies might define an inverse function of one where m is set to a constant, such that the inverse applies to a particular row.[8]Use as benchmark[edit]The Ackermann function, due to its definition in terms of extremely deep recursion, can be used as a benchmark of a compiler's ability to optimize recursion. The first published use of Ackermann's function in this way was in 1971 by Yngve Sundblad.[9]Sundblad's seminal paper was taken up by Brian Wichmann (co-author of the Whetstone benchmark) in a trilogy of papers written between 1975 and 1982.[10][11][12]See also[edit]\nComputability theory\nDouble recursion\nFast-growing hierarchy\nGoodstein function\nPrimitive recursive function\nRecursion (computer science)\nReferences[edit]External links[edit]\nHazewinkel, Michiel, ed. (2001) [1994], Ackermann function, Encyclopedia of Mathematics, Springer Science+Business Media B.V. / Kluwer Academic Publishers, ISBN 978-1-55608-010-4 \nWeisstein, Eric W. Ackermann function. MathWorld. \n This article incorporates public domain material from the NIST document: Black, Paul E. Ackermann's function. Dictionary of Algorithms and Data Structures. \nAn animated Ackermann function calculator\nScott Aaronson, Who can name the biggest number? (1999)\nAckermann functions. Includes a table of some values.\nHyper-operations: Ackermann's Function and New Arithmetical Operation\nRobert Munafo's Large Numbers describes several variations on the definition of A.\nGabriel Nivasch, Inverse Ackermann without pain on the inverse Ackermann function.\nRaimund Seidel, Understanding the inverse Ackermann function (PDF presentation).\nThe Ackermann function written in different programming languages, (on Rosetta Code)\nAckermann's Function (Archived 2009-10-24)\u2014Some study and programming by Harry J. Smith.\n", "subtitles": ["History", "Definition and properties", "Example expansions", "Table of values", "Proof that the Ackermann function is not primitive recursive", "Inverse", "Use as benchmark", "See also", "References", "External links"], "title": "Ackermann function"},
{"content": "James Arthur Gosling, OC (born May 19, 1955) is a Canadian computer scientist, best known as the founder and lead designer behind the Java programming language.[3]Education and career[edit]James Gosling received a Bachelor of Science from the University of Calgary [4] and his M.A. and Ph.D. from Carnegie Mellon University.[2][5][6] He wrote a version of Emacs called Gosling Emacs (Gosmacs) while working toward his doctorate. He built a multi-processor version of Unix for a 16-way computer system[7] while at Carnegie Mellon University, before joining Sun Microsystems. He also developed several compilers and mail systems there.Gosling was with Sun Microsystems between 1984 and 2010 (26 years). He is known as the father of the Java programming language.[8][9] He got the idea for the Java VM while writing a program to port software from a PERQ by translating Perq Q-Code to VAX assembler and emulating the hardware. He left Sun Microsystems on April 2, 2010 after it was acquired by the Oracle Corporation,[8] citing reductions in pay, status, and decision-making ability, along with change of role and ethical challenges.[10] He has since taken a very critical stance towards Oracle in interviews, noting that during the integration meetings between Sun and Oracle, where we were being grilled about the patent situation between Sun and Google, we could see the Oracle lawyer's eyes sparkle.[9] He clarified his position during the Oracle v Google trial over Android: While I have differences with Oracle, in this case, they are on the right. Google totally slimed Sun. We were all really disturbed, even Jonathan Schwartz; he just decided to put on a happy face and tried to turn lemons into lemonade, which annoyed a lot of folks on Sun.[11] However, he approved of the court's ruling that APIs should not be copyrightable.[12]In March 2011, Gosling left Oracle to work at Google.[13] Six months later, he followed his colleague Bill Vass and joined a startup called Liquid Robotics.[1] In late 2016, Liquid Robotics was acquired by Boeing.[14] Following the aquisition, Gosling left Liquid Robotics to work at Amazon Web Services as Distinguished Engineer in May 2017.[15]He is an adviser at the Scala company Typesafe Inc.,[16] Independent Director at Jelastic,[17] and Strategic Advisor for Eucalyptus[18], and is a board member of DIRTT Environmental Solutions.[19]He is known for his love of proving the unknown and has noted that his favorite irrational number is \u221a2. He has a framed picture of the first 1,000 digits of \u221a2 in his office.[20]Contributions[edit]Gosling initially became known as the author of Gosling Emacs, and also invented the windowing system NeWS, which lost out to X Window because Sun did not give it an open source license. He is generally credited with having invented the Java programming language in 1994.[21][22][23] He created the original design of Java and implemented the language's original compiler and virtual machine.[24] Gosling traces the origins of the approach to his early graduate-student days, when he created a p-code virtual machine for the lab's DEC VAX computer, so that his professor could run programs written in UCSD Pascal. In the work leading to Java at Sun, he saw that architecture-neutral execution for widely distributed programs could be achieved by implementing a similar philosophy: always program for the same virtual machine.[25]For his achievement, the National Academy of Engineering in the United States elected him as a Foreign Associate member.[26] Another contribution of Gosling's was co-writing the bundle program, a utility thoroughly detailed in Brian Kernighan and Rob Pike's book The Unix Programming Environment.Honors[edit]\n2002: he was awarded The Economist Innovation Award.[27]\n2002: he was awarded The Flame Award USENIX Lifetime Achievement Award.[28]\n2007: he was made an Officer of the Order of Canada.[29] The Order is Canada's second highest civilian honor. Officers are the second highest grade within the Order.\n2013: he became a fellow of the Association for Computing Machinery.[30]\n2015: awarded IEEE John von Neumann Medal[31]\nBooks[edit]\nKen Arnold, James Gosling, David Holmes, The Java Programming Language, Fourth Edition, Addison-Wesley Professional, 2005, ISBN 0-321-34980-6\nJames Gosling, Bill Joy, Guy L. Steele Jr., Gilad Bracha, The Java Language Specification, Third Edition, Addison-Wesley Professional, 2005, ISBN 0-321-24678-0\nKen Arnold, James Gosling, David Holmes, The Java Programming Language, Third Edition, Addison-Wesley Professional, 2000, ISBN 0-201-70433-1\nJames Gosling, Bill Joy, Guy L. Steele Jr., Gilad Bracha, The Java Language Specification, Second Edition, Addison-Wesley, 2000, ISBN 0-201-31008-2\nGregory Bollella (Editor), Benjamin Brosgol, James Gosling, Peter Dibble, Steve Furr, David Hardin, Mark Turnbull, The Real-Time Specification for Java, Addison Wesley Longman, 2000, ISBN 0-201-70323-8\nKen Arnold, James Gosling, The Java programming language Second Edition, Addison-Wesley, 1997, ISBN 0-201-31006-6\nKen Arnold, James Gosling, The Java programming language, Addison-Wesley, 1996, ISBN 0-201-63455-4\nJames Gosling, Bill Joy, Guy L. Steele Jr., The Java Language Specification, Addison Wesley Publishing Company, 1996, ISBN 0-201-63451-1\nJames Gosling, Frank Yellin, The Java Team, The Java Application Programming Interface, Volume 2: Window Toolkit and Applets, Addison-Wesley, 1996, ISBN 0-201-63459-7\nJames Gosling, Frank Yellin, The Java Team, The Java Application Programming Interface, Volume 1: Core Packages, Addison-Wesley, 1996, ISBN 0-201-63453-8\nJames Gosling, Henry McGilton, The Java language Environment: A white paper, Sun Microsystems, 1996\nJames Gosling, David S. H. Rosenthal, Michelle J. Arden, The NeWS Book : An Introduction to the Network/Extensible Window System (Sun Technical Reference Library), Springer, 1989, ISBN 0-387-96915-2\nSee also[edit]\nHistory of Java\nFallacies of distributed computing\nReferences[edit]External links[edit]\nJames Gosling's personal weblog\nPresentation by James Gosling about the origins of Java, from the JVM Languages Summit 2008\nSlide show depicting Gosling's life\nThe Process of Innovation \u2013 James Gosling's talk at Stanford University (video archive)\nJames Gosling interview on Triangulation (TWiT.tv). Recorded 2016-04-11\n", "subtitles": ["Education and career", "Contributions", "Honors", "Books", "See also", "References", "External links"], "title": "James Gosling"},
{"content": "Hy (alternately, Hylang) is a dialect of the Lisp programming language designed to interact with Python by translating expressions into Python's abstract syntax tree (AST). Hy was introduced at PyCon 2013 by Paul Tagliamonte.[1]Similar to Clojure's and Kawa's mapping of s-expressions onto the JVM,[2] Hy is meant to operate as a transparent Lisp front end to Python's abstract syntax.[3] Because Lisp allows operating on code as data, Hy can be used to write domain-specific languages.[4] Hy also allows Python libraries (including the standard library) to be imported and accessed alongside Hy code with a compilation[note 1] step converting the data structure of both into Python's AST.[note 2][5][6]Example code[edit]From the language documentation:[7]See also[edit]\nClojure\nKawa (Scheme implementation)\nPixie (programming language)\nNotes[edit]References[edit]External links[edit]\nDocumentation\nGithub project\nVideo of 2014 PyCon talk\n", "subtitles": [], "title": "Hy"},
{"content": "The Sierpinski triangle (also with the original orthography Sierpin\u0301ski), also called the Sierpinski gasket or the Sierpinski Sieve, is a fractal and attractive fixed set with the overall shape of an equilateral triangle, subdivided recursively into smaller equilateral triangles. Originally constructed as a curve, this is one of the basic examples of self-similar sets, i.e., it is a mathematically generated pattern that can be reproducible at any magnification or reduction. It is named after the Polish mathematician Wac\u0142aw Sierpin\u0301ski, but appeared as a decorative pattern many centuries prior to the work of Sierpin\u0301ski.[1]Constructions[edit]There are many different ways of constructing the Sierpinski triangle.Removing triangles[edit]The Sierpinski triangle may be constructed from an equilateral triangle by repeated removal of triangular subsets:\nStart with an equilateral triangle.\nSubdivide it into four smaller congruent equilateral triangles and remove the central triangle.\nRepeat step 2 with each of the remaining smaller triangles forever.\n\n\nEach removed triangle (a trema) is topologically an open set.[2] This process of recursively removing triangles is an example of a finite subdivision rule.Shrinking and duplication[edit]The same sequence of shapes, converging to the Sierpinski triangle, can alternatively be generated by the following steps:\nStart with any triangle in a plane (any closed, bounded region in the plane will actually work). The canonical Sierpinski triangle uses an equilateral triangle with a base parallel to the horizontal axis (first image).\nShrink the triangle to 1/2 height and 1/2 width, make three copies, and position the three shrunken triangles so that each triangle touches the two other triangles at a corner (image 2). Note the emergence of the central hole\u2014because the three shrunken triangles can between them cover only 3/4 of the area of the original. (Holes are an important feature of Sierpinski's triangle.)\nRepeat step 2 with each of the smaller triangles (image 3 and so on).\nNote that this infinite process is not dependent upon the starting shape being a triangle\u2014it is just clearer that way. The first few steps starting, for example, from a square also tend towards a Sierpinski triangle. Michael Barnsley used an image of a fish to illustrate this in his paper V-variable fractals and superfractals.[3]\n\nThe actual fractal is what would be obtained after an infinite number of iterations. More formally, one describes it in terms of functions on closed sets of points. If we let dA denote the dilation by a factor of 1/2 about a point A, then the Sierpinski triangle with corners A, B, and C is the fixed set of the transformation dA \u222a dB \u222a dC.This is an attractive fixed set, so that when the operation is applied to any other set repeatedly, the images converge on the Sierpinski triangle. This is what is happening with the triangle above, but any other set would suffice.Chaos game[edit]If one takes a point and applies each of the transformations dA, dB, and dC to it randomly, the resulting points will be dense in the Sierpinski triangle, so the following algorithm will again generate arbitrarily close approximations to it:[4]Start by labeling p1, p2 and p3 as the corners of the Sierpinski triangle, and a random point v1. Set vn+1 = 1/2(vn + prn), where rn is a random number 1, 2 or 3. Draw the points v1 to v\u221e. If the first point v1 was a point on the Sierpin\u0301ski triangle, then all the points vn lie on the Sierpinski triangle. If the first point v1 to lie within the perimeter of the triangle is not a point on the Sierpinski triangle, none of the points vn will lie on the Sierpinski triangle, however they will converge on the triangle. If v1 is outside the triangle, the only way vn will land on the actual triangle, is if vn is on what would be part of the triangle, if the triangle was infinitely large.Or more simply:\nTake 3 points in a plane to form a triangle, you need not draw it.\nRandomly select any point inside the triangle and consider that your current position.\nRandomly select any one of the 3 vertex points.\nMove half the distance from your current position to the selected vertex.\nPlot the current position.\nRepeat from step 3.\nNote: This method is also called the chaos game, and is an example of an iterated function system. You can start from any point outside or inside the triangle, and it would eventually form the Sierpinski Gasket with a few leftover points (if the starting point lies on the outline of the triangle, there are no leftover points). It is interesting to do this with pencil and paper. A brief outline is formed after placing approximately one hundred points, and detail begins to appear after a few hundred. An interactive version of the chaos game can be found here.Arrowhead curve[edit]Another construction for the Sierpinski triangle shows that it can be constructed as a curve in the plane. It is formed by a process of repeated modification of simpler curves, analogous to the construction of the Koch snowflake:\nStart with a single line segment in the plane\nRepeatedly replace each line segment of the curve with three shorter segments, forming 120\u00b0 angles at each junction between two consecutive segments, with the first and last segments of the curve either parallel to the original line segment or forming a 60\u00b0 angle with it.\nThe resulting fractal curve is called the Sierpin\u0301ski arrowhead curve, and its limiting shape is the Sierpinski triangle.[5]Cellular automata[edit]The Sierpinski triangle also appears in certain cellular automata (such as Rule 90), including those relating to Conway's Game of Life. For instance, the Life-like cellular automaton B1/S12 when applied to a single cell will generate four approximations of the Sierpinski triangle.[6] A very long one cell thick line in standard life will create two mirrored Sierpinski triangles. The time-space diagram of a replicator pattern in a cellular automaton also often resembles a Sierpinski triangle, such as that of the common replicator in HighLife.[7]Pascal's triangle[edit]If one takes Pascal's triangle with 2n rows and colors the even numbers white, and the odd numbers black, the result is an approximation to the Sierpinski triangle. More precisely, the limit as n approaches infinity of this parity-colored 2n-row Pascal triangle is the Sierpinski triangle.[8]Towers of Hanoi[edit]The Towers of Hanoi puzzle involves moving disks of different sizes between three pegs, maintaining the property that no disk is ever placed on top of a smaller disk. The states of an n-disk puzzle, and the allowable moves from one state to another, form an undirected graph that can be represented geometrically as the intersection graph of the set of triangles remaining after the nth step in the construction of the Sierpinski triangle. Thus, in the limit as n goes to infinity, this sequence of graphs can be interpreted as a discrete analogue of the Sierpinski triangle.[9]Properties[edit]For integer number of dimensions d, when doubling a side of an object, 2d copies of it are created, i.e. 2 copies for 1-dimensional object, 4 copies for 2-dimensional object and 8 copies for 3-dimensional object. For the Sierpinski triangle, doubling its side creates 3 copies of itself. Thus the Sierpinski triangle has Hausdorff dimension log(3)/log(2) = log2 3 \u2248 1.585, which follows from solving 2d = 3 for d.[10]The area of a Sierpinski triangle is zero (in Lebesgue measure). The area remaining after each iteration is clearly 3/4 of the area from the previous iteration, and an infinite number of iterations results in zero.[11]The points of a Sierpinski triangle have a simple characterization in barycentric coordinates.[12] If a point has coordinates (0.u1u2u3..., 0.v1v2v3..., 0.w1w2w3...), expressed as binary numerals, then the point is in Sierpinski's triangle if and only if ui + vi + wi = 1 for all i.Generalization to other moduli[edit]A generalization of the Sierpinski triangle can also be generated using Pascal's triangle if a different Modulo is used. Iteration n can be generated by taking a Pascal's triangle with Pn rows and coloring numbers by their value for x mod P. As n approaches infinity, a fractal is generated.The same fractal can be achieved by dividing a triangle into a tessellation of P2 similar triangles and removing the triangles that are upside-down from the original, then iterating this step with each smaller triangle.Conversely, the fractal can also be generated by beginning with a triangle and duplicating it and arranging n(n + 1)/2 of the new figures in the same orientation into a larger similar triangle with the vertices of the previous figures touching, then iterating that step.[13]Analogues in higher dimensions[edit]The Sierpinski tetrahedron or tetrix is the three-dimensional analogue of the Sierpinski triangle, formed by repeatedly shrinking a regular tetrahedron to one half its original height, putting together four copies of this tetrahedron with corners touching, and then repeating the process. This can also be done with a square pyramid and five copies instead.A tetrix constructed from an initial tetrahedron of side-length L has the property that the total surface area remains constant with each iteration. The initial surface area of the (iteration-0) tetrahedron of side-length L is L2\u221a3. The next iteration consists of four copies with side length L/2, so the total area is 4(L/2)2\u221a3 = 4L2\u221a3/4 = L2\u221a3 again. Meanwhile the volume of the construction is halved at every step and therefore approaches zero. The limit of this process has neither volume nor surface but, like the Sierpinski gasket, is an intricately connected curve. Its Hausdorff dimension is log(4)/log(2) = 2. If all points are projected onto a plane that is parallel to two of the outer edges, they exactly fill a square of side length L/\u221a2 without overlap.Numerical generation[edit]A short code in the Mathematica internal language: the recursive procedure SiPyramid generates a 3D pyramid of arbitrary order n as the displayable graphic object Graphics3D:History[edit]Wac\u0142aw Sierpin\u0301ski described the Sierpinski triangle in 1915. However, similar patterns appear already in the 13th-century Cosmati mosaics in the cathedral of Anagni, Italy,[14] and other places of central Italy, for carpets in many places such as the nave of the Roman Basilica of Santa Maria in Cosmedin,[15] and for isolated triangles positioned in rotae in several churches and Basiliche.[1] In the case of the isolated triangle, it is interesting to notice that the iteration is at least of three levels.The Apollonian gasket was first described by Apollonius of Perga (3rd century BC) and further analyzed by Gottfried Leibniz (17th century), and is a curved precursor of the 20th-century Sierpin\u0301ski triangle.[16]Etymology[edit]The usage of the word gasket to refer to the Sierpinski triangle refers to gaskets such as are found in motors, and which sometimes feature a series of holes of decreasing size, similar to the fractal; this usage was coined by Benoi\u0302t Mandelbrot, who thought the fractal looked similar to the part that prevents leaks in motors.[17]See also[edit]\nApollonian gasket, a set of mutually tangent circles with the same combinatorial structure as the Sierpinski triangle\nList of fractals by Hausdorff dimension\nSierpinski carpet, another fractal named after Sierpinski and formed by repeatedly removing squares from a larger square\nTriforce\nReferences[edit]External links[edit]\nHazewinkel, Michiel, ed. (2001) [1994], Sierpinski gasket, Encyclopedia of Mathematics, Springer Science+Business Media B.V. / Kluwer Academic Publishers, ISBN 978-1-55608-010-4 \nWeisstein, Eric W. Sierpinski Sieve. MathWorld. \nRothemund, Paul W. K.; Papadakis, Nick; Winfree, Erik (2004). Algorithmic Self-Assembly of DNA Sierpinski Triangles. PLoS Biology. 2 (12): e424. doi:10.1371/journal.pbio.0020424. PMC 534809 . PMID 15583715. \nSierpinski Gasket by Trema Removal at cut-the-knot\nSierpinski Gasket and Tower of Hanoi at cut-the-knot\nReal-time GPU generated Sierpinski Triangle in 3D\nPythagorean triangles, Waclaw Sierpinski, Courier Corporation, 2003\n", "subtitles": ["Constructions", "Properties", "Generalization to other moduli", "Analogues in higher dimensions", "History", "Etymology", "See also", "References", "External links"], "title": "Sierpinski triangle"},
{"content": "In programming languages, a type system is a set of rules that assigns a property called type to the various constructs of a computer program, such as variables, expressions, functions or modules.[1] These types formalize and enforce the otherwise implicit categories the programmer uses for data structures and components (e.g. string, array of float, function returning boolean). The main purpose of a type system is to reduce possibilities for bugs in computer programs[2] by defining interfaces between different parts of a computer program, and then checking that the parts have been connected in a consistent way. This checking can happen statically (at compile time), dynamically (at run time), or as a combination of static and dynamic checking. Type systems have other purposes as well, such as expressing business rules, enabling certain compiler optimizations, allowing for multiple dispatch, providing a form of documentation, etc.A type system associates a type with each computed value and, by examining the flow of these values, attempts to ensure or prove that no type errors can occur. The given type system in question determines exactly what constitutes a type error, but in general the aim is to prevent operations expecting a certain kind of value from being used with values for which that operation does not make sense (logic errors). Type systems are often specified as part of programming languages, and built into the interpreters and compilers for them; although the type system of a language can be extended by optional tools that perform added kinds of checks using the language's original type syntax and grammar.Usage overview[edit]An example of a simple type system is that of the C language. The portions of a C program are the function definitions. One function is invoked by another function. The interface of a function states the name of the function and a list of values that are passed to the function's code. The code of an invoking function states the name of the invoked, along with the names of variables that hold values to pass to it. During execution, the values are placed into temporary storage, then execution jumps to the code of the invoked function. The invoked function's code accesses the values and makes use of them. If the instructions inside the function are written with the assumption of receiving an integer value, but the calling code passed a floating-point value, then the wrong result will be computed by the invoked function. The C compiler checks the type declared for each variable sent, against the type declared for each variable in the interface of the invoked function. If the types do not match, the compiler throws a compile-time error.A compiler may also use the static type of a value to optimize the storage it needs and the choice of algorithms for operations on the value. In many C compilers the float data type, for example, is represented in 32 bits, in accord with the IEEE specification for single-precision floating point numbers. They will thus use floating-point-specific microprocessor operations on those values (floating-point addition, multiplication, etc.).The depth of type constraints and the manner of their evaluation affect the typing of the language. A programming language may further associate an operation with various resolutions for each type, in the case of type polymorphism. Type theory is the study of type systems. The concrete types of some programming languages, such as integers and strings, depend on practical issues of computer architecture, compiler implementation, and language design.Fundamentals[edit]Formally, type theory studies type systems. A programming language must have occurrence to type check using the type system whether at compile time or runtime, manually annotated or automatically inferred. As Mark Manasse concisely put it:[3]\nThe fundamental problem addressed by a type theory is to ensure that programs have meaning. The fundamental problem caused by a type theory is that meaningful programs may not have meanings ascribed to them. The quest for richer type systems results from this tension.\nAssigning a data type, termed typing, gives meaning to a sequence of bits such as a value in memory or some object such as a variable. The hardware of a general purpose computer is unable to discriminate between for example a memory address and an instruction code, or between a character, an integer, or a floating-point number, because it makes no intrinsic distinction between any of the possible values that a sequence of bits might mean.[note 1] Associating a sequence of bits with a type conveys that meaning to the programmable hardware to form a symbolic system composed of that hardware and some program.A program associates each value with at least one specific type, but it also can occur that one value is associated with many subtypes. Other entities, such as objects, modules, communication channels, and dependencies can become associated with a type. Even a type can become associated with a type. An implementation of a type system could in theory associate identifications called data type (a type of a value), class (a type of an object), and kind (a type of a type, or metatype). These are the abstractions that typing can go through, on a hierarchy of levels contained in a system.When a programming language evolves a more elaborate type system, it gains a more finely grained rule set than basic type checking, but this comes at a price when the type inferences (and other properties) become undecidable, and when more attention must be paid by the programmer to annotate code or to consider computer-related operations and functioning. It is challenging to find a sufficiently expressive type system that satisfies all programming practices in a type safe manner.The more type restrictions that are imposed by the compiler, the more strongly typed a programming language is. Strongly typed languages often require the programmer to make explicit conversions in contexts where an implicit conversion would cause no harm. Pascal's type system has been described as too strong because, for example, the size of an array or string is part of its type, making some programming tasks difficult.[4][5] Haskell is also strongly typed but its types are automatically inferred so that explicit conversions are often (but not always) unnecessary.A programming language compiler can also implement a dependent type or an effect system, which enables even more program specifications to be verified by a type checker. Beyond simple value-type pairs, a virtual region of code is associated with an effect component describing what is being done with what, and enabling for example to throw an error report. Thus the symbolic system may be a type and effect system, which endows it with more safety checking than type checking alone.Whether automated by the compiler or specified by a programmer, a type system makes program behavior illegal if outside the type-system rules. Advantages provided by programmer-specified type systems include:\nAbstraction (or modularity) \u2013 Types enable programmers to think at a higher level than the bit or byte, not bothering with low-level implementation. For example, programmers can begin to think of a string as a set of character values instead of as a mere array of bytes. Higher still, types enable programmers to think about and express interfaces between two of any-sized subsystems. This enables more levels of localization so that the definitions required for interoperability of the subsystems remain consistent when those two subsystems communicate.\nDocumentation \u2013 In more expressive type systems, types can serve as a form of documentation clarifying the intent of the programmer. For example, if a programmer declares a function as returning a timestamp type, this documents the function when the timestamp type can be explicitly declared deeper in the code to be an integer type.\nAdvantages provided by compiler-specified type systems include:\nOptimization \u2013 Static type-checking may provide useful compile-time information. For example, if a type requires that a value must align in memory at a multiple of four bytes, the compiler may be able to use more efficient machine instructions.\nSafety \u2013 A type system enables the compiler to detect meaningless or probably invalid code. For example, we can identify an expression 3 / Hello, World as invalid, when the rules do not specify how to divide an integer by a string. Strong typing offers more safety, but cannot guarantee complete type safety.\nType safety contributes to program correctness, but can only guarantee correctness at the cost of making the type checking itself an undecidable problem.[citation needed] In a type system with automated type checking a program may prove to run incorrectly yet be safely typed, and produce no compiler errors. Division by zero is an unsafe and incorrect operation, but a type checker running at compile time only doesn't scan for division by zero in most languages, and then it is left as a runtime error. To prove the absence of these more-general-than-types defects, other kinds of formal methods, collectively known as program analyses, are in common use. Alternatively, a sufficiently expressive type system, such as in dependently typed languages, can prevent these kinds of errors (for example, expressing the type of non-zero numbers). In addition software testing is an empirical method for finding errors that the type checker cannot detect.Type checking[edit]The process of verifying and enforcing the constraints of types\u2014type checking\u2014may occur either at compile-time (a static check) or at run-time. If a language specification requires its typing rules strongly (i.e., more or less allowing only those automatic type conversions that do not lose information), one can refer to the process as strongly typed, if not, as weakly typed. The terms are not usually used in a strict sense.Static type checking[edit]Static type checking is the process of verifying the type safety of a program based on analysis of a program's text (source code). If a program passes a static type checker, then the program is guaranteed to satisfy some set of type safety properties for all possible inputs.Static type checking can be considered a limited form of program verification (see type safety), and in a type-safe language, can be considered also an optimization. If a compiler can prove that a program is well-typed, then it does not need to emit dynamic safety checks, allowing the resulting compiled binary to run faster and to be smaller.Static type checking for Turing-complete languages is inherently conservative. That is, if a type system is both sound (meaning that it rejects all incorrect programs) and decidable (meaning that it is possible to write an algorithm that determines whether a program is well-typed), then it must be incomplete (meaning there are correct programs, which are also rejected, even though they do not encounter runtime errors).[6] For example, consider a program containing the code:if <complex test> then <do something> else <generate type error>Even if the expression <complex test> always evaluates to true at run-time, most type checkers will reject the program as ill-typed, because it is difficult (if not impossible) for a static analyzer to determine that the else branch will not be taken.[7] Conversely, a static type checker will quickly detect type errors in rarely used code paths. Without static type checking, even code coverage tests with 100% coverage may be unable to find such type errors. The tests may fail to detect such type errors, because the combination of all places where values are created and all places where a certain value is used must be taken into account.A number of useful and common programming language features cannot be checked statically, such as downcasting. Thus, many languages will have both static and dynamic type checking; the static type checker verifies what it can, and dynamic checks verify the rest.Many languages with static type checking provide a way to bypass the type checker. Some languages allow programmers to choose between static and dynamic type safety. For example, C# distinguishes between statically-typed and dynamically-typed variables. Uses of the former are checked statically, whereas uses of the latter are checked dynamically. Other languages allow writing code that is not type-safe. For example, in C, programmers can freely cast a value between any two types that have the same size.For a list of languages with static type checking, see the category for statically typed languages.Dynamic type checking and runtime type information[edit]Dynamic type checking is the process of verifying the type safety of a program at runtime. Implementations of dynamically type-checked languages generally associate each runtime object with a type tag (i.e., a reference to a type) containing its type information. This runtime type information (RTTI) can also be used to implement dynamic dispatch, late binding, downcasting, reflection, and similar features.Most type-safe languages include some form of dynamic type checking, even if they also have a static type checker.[citation needed] The reason for this is that many useful features or properties are difficult or impossible to verify statically. For example, suppose that a program defines two types, A and B, where B is a subtype of A. If the program tries to convert a value of type A to type B, which is known as downcasting, then the operation is legal only if the value being converted is actually a value of type B. Thus, a dynamic check is needed to verify that the operation is safe. This requirement is one of the criticisms of downcasting.By definition, dynamic type checking may cause a program to fail at runtime. In some programming languages, it is possible to anticipate and recover from these failures. In others, type-checking errors are considered fatal.Programming languages that include dynamic type checking but not static type checking are often called dynamically typed programming languages. For a list of such languages, see the category for dynamically typed programming languages.Combining static and dynamic type checking[edit]Some languages allow both static and dynamic typing (type checking), sometimes called soft typing. For example, Java and some other ostensibly statically typed languages support downcasting types to their subtypes, querying an object to discover its dynamic type, and other type operations that depend on runtime type information. More generally, most programming languages include mechanisms for dispatching over different 'kinds' of data, such as disjoint unions, subtype polymorphism, and variant types. Even when not interacting with type annotations or type checking, such mechanisms are materially similar to dynamic typing implementations. See programming language for more discussion of the interactions between static and dynamic typing.Objects in object-oriented languages are usually accessed by a reference whose static target type (or manifest type) is equal to either the object's run-time type (its latent type) or a supertype thereof. This is conformant with the Liskov substitution principle, which states that all operations performed on an instance of a given type can also be performed on an instance of a subtype. This concept is also known as subsumption. In some languages subtypes may also possess covariant or contravariant return types and argument types respectively.Certain languages, for example Clojure, Common Lisp, or Cython are dynamically type-checked by default, but allow programs to opt into static type checking by providing optional annotations. One reason to use such hints would be to optimize the performance of critical sections of a program. This is formalized by gradual typing. The programming environment DrRacket, a pedagogic environment based on Lisp, and a precursor of the language Racket was also soft-typed.Conversely, as of version 4.0, the C# language provides a way to indicate that a variable should not be statically type-checked. A variable whose type is dynamic will not be subject to static type checking. Instead, the program relies on runtime type information to determine how the variable may be used.[8]Static and dynamic type checking in practice[edit]The choice between static and dynamic typing requires certain trade-offs.Static typing can find type errors reliably at compile time, which should increase the reliability of the delivered program. However, programmers disagree over how commonly type errors occur, resulting in further disagreements over the proportion of those bugs that are coded that would be caught by appropriately representing the designed types in code.[9][10] Static typing advocates[who?] believe programs are more reliable when they have been well type-checked, whereas dynamic-typing advocates[who?] point to distributed code that has proven reliable and to small bug databases.[citation needed] The value of static typing, then, presumably[vague] increases as the strength of the type system is increased. Advocates of dependent typing,[who?] implemented in languages such as Dependent ML and Epigram, have suggested that almost all bugs can be considered type errors, if the types used in a program are properly declared by the programmer or correctly inferred by the compiler.[11]Static typing usually results in compiled code that executes faster. When the compiler knows the exact data types that are in use (which is necessary for static verification, either through declaration or inference) it can produce optimized machine code. Some dynamically typed languages such as Common Lisp allow optional type declarations for optimization for this reason.By contrast, dynamic typing may allow compilers to run faster and interpreters to dynamically load new code, because changes to source code in dynamically typed languages may result in less checking to perform and less code to revisit.[clarification needed] This too may reduce the edit-compile-test-debug cycle.Statically typed languages that lack type inference (such as C and Java) require that programmers declare the types that a method or function must use. This can serve as added program documentation, that is active and dynamic, instead of static. This allows a compiler to prevent it from drifting out of synchrony, and from being ignored by programmers. However, a language can be statically typed without requiring type declarations (examples include Haskell, Scala, OCaml, F#, and to a lesser extent C# and C++), so explicit type declaration is not a necessary requirement for static typing in all languages.Dynamic typing allows constructs that some static type checking would reject as illegal. For example, eval functions, which execute arbitrary data as code, become possible. An eval function is possible with static typing, but requires advanced uses of algebraic data types. Further, dynamic typing better accommodates transitional code and prototyping, such as allowing a placeholder data structure (mock object) to be transparently used in place of a full data structure (usually for the purposes of experimentation and testing).Dynamic typing typically allows duck typing (which enables easier code reuse). Many[specify] languages with static typing also feature duck typing or other mechanisms like generic programming that also enable easier code reuse.Dynamic typing typically makes metaprogramming easier to use. For example, C++ templates are typically more cumbersome to write than the equivalent Ruby or Python code since C++ has stronger rules regarding type definitions (for both functions and variables). This forces a developer to write more boilerplate code for a template than a Python developer would need to. More advanced run-time constructs such as metaclasses and introspection are often harder to use in statically typed languages. In some languages, such features may also be used e.g. to generate new types and behaviors on the fly, based on run-time data. Such advanced constructs are often provided by dynamic programming languages; many of these are dynamically typed, although dynamic typing need not be related to dynamic programming languages.Strong and weak type systems[edit]Languages are often colloquially referred to as strongly typed or weakly typed. In fact, there is no universally accepted definition of what these terms mean. In general, there are more precise terms to represent the differences between type systems that lead people to call them strong or weak.Type safety and memory safety[edit]A third way of categorizing the type system of a programming language uses the safety of typed operations and conversions. Computer scientists consider a language type-safe if it does not allow operations or conversions that violate the rules of the type system.Some observers use the term memory-safe language (or just safe language) to describe languages that do not allow programs to access memory that has not been assigned for their use. For example, a memory-safe language will check array bounds, or else statically guarantee (i.e., at compile time before execution) that array accesses out of the array boundaries will cause compile-time and perhaps runtime errors.Consider the following program of a language that is both type-safe and memory-safe:[12]In this example, the variable z will have the value 42. Although this may not be what the programmer anticipated, it is a well-defined result. If y were a different string, one that could not be converted to a number (e.g. Hello World), the result would be well-defined as well. Note that a program can be type-safe or memory-safe and still crash on an invalid operation; in fact, if a program encounters an operation that is not type-safe, terminating the program is often the only option.Now consider a similar example in C:In this example z will point to a memory address five characters beyond y, equivalent to three characters after the terminating zero character of the string pointed to by y. This is memory that the program is not expected to access. It may contain garbage data, and it certainly doesn't contain anything useful. As this example shows, C is neither a memory-safe nor a type-safe language.In general, type-safety and memory-safety go hand in hand. For example, a language that supports pointer arithmetic and number-to-pointer conversions (like C) is neither memory-safe nor type-safe, because it allows arbitrary memory to be accessed as if it were valid memory of any type.For more information, see memory safety.Variable levels of type checking[edit]Some languages allow different levels of checking to apply to different regions of code. Examples include:\nThe use strict directive in JavaScript[13][14][15] and Perl applies stronger checking.\nThe declare(strict_types=1) in PHP[16] on a per-file basis allows only a variable of exact type of the type declaration will be accepted, or a TypeError will be thrown.\nThe Option Strict On in VB.NET allows the compiler to require a conversion between objects.\nAdditional tools such as lint and IBM Rational Purify can also be used to achieve a higher level of strictness.Optional type systems[edit]It has been proposed, chiefly by Gilad Bracha, that the choice of type system be made independent of choice of language; that a type system should be a module that can be plugged into a language as needed. He believes this is advantageous, because what he calls mandatory type systems make languages less expressive and code more fragile.[17] The requirement that types do not affect the semantics of the language is difficult to fulfill.Optional typing is related to gradual typing, but still distinct from it.[18][better source needed]Polymorphism and types[edit]The term polymorphism refers to the ability of code (especially, functions or classes) to act on values of multiple types, or to the ability of different instances of the same data structure to contain elements of different types. Type systems that allow polymorphism generally do so in order to improve the potential for code re-use: in a language with polymorphism, programmers need only implement a data structure such as a list or an associative array once, rather than once for each type of element with which they plan to use it. For this reason computer scientists sometimes call the use of certain forms of polymorphism generic programming. The type-theoretic foundations of polymorphism are closely related to those of abstraction, modularity and (in some cases) subtyping.Duck typing[edit]In duck typing,[19] a statement calling a method m on an object does not rely on the declared type of the object; only that the object, of whatever type, must supply an implementation of the method called, when called, at run-time.Duck typing differs from structural typing in that, if the part (of the whole module structure) needed for a given local computation is present at runtime, the duck type system is satisfied in its type identity analysis. On the other hand, a structural type system would require the analysis of the whole module structure at compile time to determine type identity or type dependence.Duck typing differs from a nominative type system in a number of aspects. The most prominent ones are that for duck typing, type information is determined at runtime (as contrasted to compile time), and the name of the type is irrelevant to determine type identity or type dependence; only partial structure information is required for that for a given point in the program execution.Duck typing uses the premise that (referring to a value) if it walks like a duck, and quacks like a duck, then it is a duck (this is a reference to the duck test that is attributed to James Whitcomb Riley). The term may have been coined[citation needed] by Alex Martelli in a 2000 message[20] to the comp.lang.python newsgroup (see Python).While one controlled experiment showed an increase in developer productivity for duck typing in single developer projects,[21] other controlled experiments on API usability show the opposite.[22][23]Specialized type systems[edit]Many type systems have been created that are specialized for use in certain environments with certain types of data, or for out-of-band static program analysis. Frequently, these are based on ideas from formal type theory and are only available as part of prototype research systems.Dependent types[edit]Dependent types are based on the idea of using scalars or values to more precisely describe the type of some other value. For example, \n  \n    \n      \n        \n          m\n          a\n          t\n          r\n          i\n          x\n        \n        (\n        3\n        ,\n        3\n        )\n      \n    \n    {\\displaystyle \\mathrm {matrix} (3,3)}\n  \n might be the type of a \n  \n    \n      \n        3\n        \u00d7\n        3\n      \n    \n    {\\displaystyle 3\\times 3}\n  \n matrix. We can then define typing rules such as the following rule for matrix multiplication:\n  \n    \n      \n        \n          \n            m\n            a\n            t\n            r\n            i\n            x\n          \n          \n            \n              m\n              u\n              l\n              t\n              i\n              p\n              l\n              y\n            \n          \n        \n        :\n        \n          m\n          a\n          t\n          r\n          i\n          x\n        \n        (\n        k\n        ,\n        m\n        )\n        \u00d7\n        \n          m\n          a\n          t\n          r\n          i\n          x\n        \n        (\n        m\n        ,\n        n\n        )\n        \u2192\n        \n          m\n          a\n          t\n          r\n          i\n          x\n        \n        (\n        k\n        ,\n        n\n        )\n      \n    \n    {\\displaystyle \\mathrm {matrix} _{\\mathrm {multiply} }:\\mathrm {matrix} (k,m)\\times \\mathrm {matrix} (m,n)\\to \\mathrm {matrix} (k,n)}\n  \nwhere \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n  \n, \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n  \n, \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n are arbitrary positive integer values. A variant of ML called Dependent ML has been created based on this type system, but because type checking for conventional dependent types is undecidable, not all programs using them can be type-checked without some kind of limits. Dependent ML limits the sort of equality it can decide to Presburger arithmetic.Other languages such as Epigram make the value of all expressions in the language decidable so that type checking can be decidable. However, in general proof of decidability is undecidable, so many programs require hand-written annotations that may be very non-trivial. As this impedes the development process, many language implementations provide an easy way out in the form of an option to disable this condition. This, however, comes at the cost of making the type-checker run in an infinite loop when fed programs that do not type-check, causing the compilation to fail.Linear types[edit]Linear types, based on the theory of linear logic, and closely related to uniqueness types, are types assigned to values having the property that they have one and only one reference to them at all times. These are valuable for describing large immutable values such as files, strings, and so on, because any operation that simultaneously destroys a linear object and creates a similar object (such as 'str= str + a') can be optimized under the hood into an in-place mutation. Normally this is not possible, as such mutations could cause side effects on parts of the program holding other references to the object, violating referential transparency. They are also used in the prototype operating system Singularity for interprocess communication, statically ensuring that processes cannot share objects in shared memory in order to prevent race conditions. The Clean language (a Haskell-like language) uses this type system in order to gain a lot of speed (compared to performing a deep copy) while remaining safe.Intersection types[edit]Intersection types are types describing values that belong to both of two other given types with overlapping value sets. For example, in most implementations of C the signed char has range -128 to 127 and the unsigned char has range 0 to 255, so the intersection type of these two types would have range 0 to 127. Such an intersection type could be safely passed into functions expecting either signed or unsigned chars, because it is compatible with both types.Intersection types are useful for describing overloaded function types: For example, if int \u2192 int is the type of functions taking an integer argument and returning an integer, and float \u2192 float is the type of functions taking a float argument and returning a float, then the intersection of these two types can be used to describe functions that do one or the other, based on what type of input they are given. Such a function could be passed into another function expecting an int \u2192 int function safely; it simply would not use the float \u2192 float functionality.In a subclassing hierarchy, the intersection of a type and an ancestor type (such as its parent) is the most derived type. The intersection of sibling types is empty.The Forsythe language includes a general implementation of intersection types. A restricted form is refinement types.Union types[edit]Union types are types describing values that belong to either of two types. For example, in C, the signed char has a -128 to 127 range, and the unsigned char has a 0 to 255 range, so the union of these two types would have an overall virtual range of -128 to 255 that may be used partially depending on which union member is accessed. Any function handling this union type would have to deal with integers in this complete range. More generally, the only valid operations on a union type are operations that are valid on both types being unioned. C's union concept is similar to union types, but is not typesafe, as it permits operations that are valid on either type, rather than both. Union types are important in program analysis, where they are used to represent symbolic values whose exact nature (e.g., value or type) is not known.In a subclassing hierarchy, the union of a type and an ancestor type (such as its parent) is the ancestor type. The union of sibling types is a subtype of their common ancestor (that is, all operations permitted on their common ancestor are permitted on the union type, but they may also have other valid operations in common).Existential types[edit]Existential types are frequently used in connection with record types to represent modules and abstract data types, due to their ability to separate implementation from interface. For example, the type T = \u2203X { a: X; f: (X \u2192 int); } describes a module interface that has a data member named a of type X and a function named f that takes a parameter of the same type X and returns an integer. This could be implemented in different ways; for example:\nintT = { a: int; f: (int \u2192 int); }\nfloatT = { a: float; f: (float \u2192 int); }\nThese types are both subtypes of the more general existential type T and correspond to concrete implementation types, so any value of one of these types is a value of type T. Given a value t of type T, we know that t.f(t.a) is well-typed, regardless of what the abstract type X is. This gives flexibility for choosing types suited to a particular implementation while clients that use only values of the interface type\u2014the existential type\u2014are isolated from these choices.In general it's impossible for the typechecker to infer which existential type a given module belongs to. In the above example intT { a: int; f: (int \u2192 int); } could also have the type \u2203X { a: X; f: (int \u2192 int); }. The simplest solution is to annotate every module with its intended type, e.g.:\nintT = { a: int; f: (int \u2192 int); } as \u2203X { a: X; f: (X \u2192 int); }\nAlthough abstract data types and modules had been implemented in programming languages for quite some time, it wasn't until 1988 that John C. Mitchell and Gordon Plotkin established the formal theory under the slogan: Abstract [data] types have existential type.[24] The theory is a second-order typed lambda calculus similar to System F, but with existential instead of universal quantification.Gradual typing[edit]Gradual typing is a type system in which variables may be typed either at compile-time (which is static typing) or at run-time (which is dynamic typing), allowing software developers to choose either type paradigm as appropriate, from within a single language.[25] In particular, gradual typing uses a special type named dynamic to represent statically-unknown types, and gradual typing replaces the notion of type equality with a new relation called consistency that relates the dynamic type to every other type. The consistency relation is symmetric but not transitive.[26]Explicit or implicit declaration and inference[edit]Many static type systems, such as those of C and Java, require type declarations: The programmer must explicitly associate each variable with a specific type. Others, such as Haskell's, perform type inference: The compiler draws conclusions about the types of variables based on how programmers use those variables. For example, given a function f(x, y) that adds x and y together, the compiler can infer that x and y must be numbers \u2013 since addition is only defined for numbers. Thus, any call to f elsewhere in the program that specifies a non-numeric type (such as a string or list) as an argument would signal an error.Numerical and string constants and expressions in code can and often do imply type in a particular context. For example, an expression 3.14 might imply a type of floating-point, while [1, 2, 3] might imply a list of integers \u2013 typically an array.Type inference is in general possible, if it is decidable in the type theory in question. Moreover, even if inference is undecidable in general for a given type theory, inference is often possible for a large subset of real-world programs. Haskell's type system, a version of Hindley\u2013Milner, is a restriction of System F\u03c9 to so-called rank-1 polymorphic types, in which type inference is decidable. Most Haskell compilers allow arbitrary-rank polymorphism as an extension, but this makes type inference undecidable. (Type checking is decidable, however, and rank-1 programs still have type inference; higher rank polymorphic programs are rejected unless given explicit type annotations.)Unified type system[edit]Some languages like Perl 6 or C# have a unified type system.[27] This means that all C# types including primitive types inherit from a single root object. Every type in C# inherits from the Object class. Java has several primitive types that are not objects. Java provides wrapper object types that exist together with the primitive types so developers can use either the wrapper object types or the simpler non-object primitive types.Compatibility: equivalence and subtyping[edit]A type-checker for a statically typed language must verify that the type of any expression is consistent with the type expected by the context in which that expression appears. For example, in an assignment statement of the form x := e, the inferred type of the expression e must be consistent with the declared or inferred type of the variable x. This notion of consistency, called compatibility, is specific to each programming language.If the type of e and the type of x are the same, and assignment is allowed for that type, then this is a valid expression. Thus, in the simplest type systems, the question of whether two types are compatible reduces to that of whether they are equal (or equivalent). Different languages, however, have different criteria for when two type expressions are understood to denote the same type. These different equational theories of types vary widely, two extreme cases being structural type systems, in which any two types that describe values with the same structure are equivalent, and nominative type systems, in which no two syntactically distinct type expressions denote the same type (i.e., types must have the same name in order to be equal).In languages with subtyping, the compatibility relation is more complex. In particular, if A is a subtype of B, then a value of type A can be used in a context where one of type B is expected, even if the reverse is not true. Like equivalence, the subtype relation is defined differently for each programming language, with many variations possible. The presence of parametric or ad hoc polymorphism in a language may also have implications for type compatibility.Notes[edit]See also[edit]\nComparison of type systems\nCovariance and contravariance (computer science)\nPolymorphism in object-oriented programming\nType rules\nType signature\nType theory\nReferences[edit]Further reading[edit]\nCardelli, Luca; Wegner, Peter (December 1985). On Understanding Types, Data Abstraction, and Polymorphism (PDF). ACM Computing Surveys. New York, NY, USA: ACM. 17 (4): 471\u2013523. doi:10.1145/6041.6042. ISSN 0360-0300. \nPierce, Benjamin C. (2002). Types and Programming Languages. MIT Press. ISBN 978-0-262-16209-8. \nCardelli, Luca (2004). Type systems. In Allen B. Tucker. CRC Handbook of Computer Science and Engineering (PDF) (2nd ed.). CRC Press. ISBN 158488360X. \nTratt, Laurence, Dynamically Typed Languages, Advances in Computers, Vol. 77, pp. 149\u2013184, July 2009\nExternal links[edit]\nSmith, Chris, What to Know Before Debating Type Systems\n", "subtitles": ["Usage overview", "Fundamentals", "Type checking", "Polymorphism and types", "Specialized type systems", "Explicit or implicit declaration and inference", "Unified type system", "Compatibility: equivalence and subtyping", "Notes", "See also", "References", "Further reading", "External links"], "title": "Type system"},
{"content": "In computer programming, string interpolation (or variable interpolation, variable substitution, or variable expansion) is the process of evaluating a string literal containing one or more placeholders, yielding a result in which the placeholders are replaced with their corresponding values. It is a form of simple template processing[1] or, in formal terms, a form of quasi-quotation (or logic substitution interpretation). String interpolation allows easier and more intuitive string formatting and content-specification compared with string concatenation.[2]String interpolation is common in many programming languages which make heavy use of string representations of data, such as Apache Groovy, Kotlin, Perl, PHP, Python, Ruby, Scala, and Swift, and most Unix shells. Two modes of literal expression are usually offered: one with interpolation enabled, the other without (termed raw string). Placeholders are usually represented by a bare or a named sigil (typically $ or %), e.g. $placeholder or %123. Expansion of the string usually occurs at run time.Variations[edit]Some languages do not offer string interpolation, instead offering a standard function where one parameter is the printf format string, and other(s) provide the values for each placeholder.Ruby uses the # symbol for interpolation, and allows interpolating any expression, not only variables. Other languages may support more advanced interpolation with a special formatting function, such as printf, in which the first argument, the format, specifies the pattern in which the remaining arguments are substituted.Algorithms[edit]There are two main types of expand variable algorithms for variable interpolation:[3]\nReplace and expand placeholders: creating a new string from the original one, by find-replace operations. Find variable-reference (placeholder), replace it by its variable-value. This algorithm offers no cache strategy.\nSplit and join string: splitting the string into an array, and merging it with the corresponding array of values; then join items by concatenation. The split string can be cached to reuse.\nSecurity issues[edit]String interpolation, like string concatenation, may lead to security problems. If user input data is improperly escaped or filtered, the system will be exposed to SQL injection, script injection, XML External Entity Injection (XXE), and cross-site scripting (XSS) attacks.[4]An SQL injection example:\nquery = SELECT x, y, z FROM Table WHERE id='$id' \nIf $id is replaced with '; DELETE FROM Table; SELECT * FROM Table WHERE id=', executing this query will wipe out all the data in Table.Examples[edit]The following Perl code works identically in PHP:produces the output: Alice said Hello World to the crowd of people.Bash[edit]The output will be:Boo[edit]The output will be:C#[edit][5]The output will be:ColdFusion Markup Language[edit]ColdFusion Markup Language (CFML) script syntax:Tag syntax:The output will be:\nI have 4 apples\nCoffeeScript[edit]The output will be:Dart[edit]The output will be:Groovy[edit]In groovy, interpolated strings are known as GStrings:The output will be:Haxe[edit]The output will be:[6]JavaScript[edit]JavaScript, as of the ECMAScript 2015 (ES6) standard, supports string interpolation using backticks ``. This feature is called template literals.[7] Here is an example:The output will be:Kotlin[edit]The output will be:Nemerle[edit]It also supports advanced formatting features, such as:The output will be:Perl[edit]The output will be:PHP[edit]The output will be:Python[edit][8][9]The output will be:Ruby / Crystal[edit]The output will be:Rust[edit]Rust provides string interpolation via the std::fmt module, which is interfaced with through various macros such as format!, write!, and print!. These macros are converted into Rust source code at compile-time, whereby each argument interacts with a formatter. The formatter supports positional parameters, named parameters, argument types, and defining various formatting traits.The output of each of these will be:\nThere are 4 apples and 3 bananas.\nScala[edit]Scala 2.10+ has implemented the following string interpolators: s, f and raw. It is also possible to write custom ones or override the standard ones.The f interpolator is a compiler macro that rewrites a format string with embedded expressions as an invocation of String.format. It verifies that the format string is well-formed and well-typed.The standard interpolators[edit]Scala 2.10+'s string interpolation allows embedding variable references directly in processed string literals. Here is an example:[10] The output will be:Sciter (tiscript)[edit]In Sciter any function with name starting from $ is considered as interpolating function and so interpolation is customizable and context sensitive:Wheregets compiled to this:Swift[edit]In Swift, a new String value can be created from a mix of constants, variables, literals, and expressions by including their values inside a string literal. Each item inserted into the string literal is wrapped in a pair of parentheses, prefixed by a backslash.The output will be:TypeScript[edit]As of version 1.4, TypeScript supports string interpolation using backticks ``. Here is an example:The output will be:The console.log function can be used as a printf function. The above example can be rewritten, thusly:The output remains the same.See also[edit]\nConcatenation\nImproper input validation\nprintf format string\nQuasi-quotation\nString literal\nNotes[edit]", "subtitles": ["Variations", "Algorithms", "Security issues", "Examples", "See also", "Notes"], "title": "String interpolation"},
{"content": "Daisy Systems Corporation incorporated in 1981 in Mountain View, California, was a computer-aided engineering, company, a pioneer in the electronic design automation (EDA) industry.It was a manufacturer of computer hardware and software for EDA, including schematic capture, logic simulation, parameter extraction and other tools for printed circuit board design and semiconductor chip layout.In mid-1980s, it had a subsidiary in Germany, Daisy Systems GmbH[1] and one in Israel.The company merged with Cadnetix Corporation of Boulder, Colorado in 1988, with the resulting company then known officially as Daisy/Cadnetix, Inc. with the trade name DAZIX. It filed for protection under Chapter 11 of the Federal Bankruptcy Code[2] in 1990 and was acquired by Intergraph later that year. Intergraph incorporated DAZIX into its EDA business unit, which was later spun off as an independent subsidiary named VeriBest, Inc. VeriBest was ultimately acquired by Mentor Graphics in late 1999.Daisy Systems was founded by Aryeh Finegold and David Stamm; its original investors were Fred Adler and Oak Investment Partners.Daisy along with Valid Logic Systems and Mentor Graphics, collectively known as DMV, added front end design to the existing computer-aided design aspects of computer automation.People[edit]Many notable people in the EDA industry once worked for Daisy Systems, including Harvey Jones, who became the CEO of Synopsys, and Vinod Khosla, who later co-founded Sun Microsystems. Aryeh Finegold went on to co-found Mercury Interactive, and Dave Stamm and Don Smith went on to co-found Clarify. Tony Zingale became CEO of Clarify and then CEO of Mercury Interactive. Mike Schuh co-founded Intrinsa Corporation before joining Foundation Capital as General Partner. George T. Haber went on to work at Sun and later founded CompCore Multimedia, GigaPixel, Mobilygen and CrestaTech. Dave Millman and Rick Carlson founded EDAC, later the ESD Alliance, the industry organization for Electronic Design Automation vendors.[3]Software[edit]Daisy applications ran on the Daisy-DNIX operating system, a Unix-like operating system running on Intel 80286 and later processors.In 1983 DABL (Daisy Behavioral Language) was developed at Daisy by Fred Chow. It was a hardware modelling language similar to VHDL. The use of DABL for simulation models of processor interconnection networks is described by Lynn R. Freytag.[4]References[edit]", "subtitles": [], "title": "Daisy Systems"},
{"content": "z/VSE (Virtual Storage Extended) is an operating system for IBM mainframe computers, the latest one in the DOS/360 lineage, which originated in 1965. It is less common than prominent z/OS and is mostly used on smaller machines. Primary z/VSE development occurs in IBM's Bo\u0308blingen labs in Germany.Overview[edit]DOS/360 originally supported 24-bit addressing. As the underlying hardware evolved, VSE/ESA acquired support for 31-bit addressing. IBM released z/VSE Version 4 in 2007. z/VSE Version 4 requires 64-bit z/Architecture hardware and supports 64-bit real mode addressing. With z/VSE 5.1 (available since 2011) z/VSE introduced 64 bit virtual addressing and memory objects (chunks of virtual storage), that are allocated above 2 GB. The latest shipping release is z/VSE 6.2.0 - available since December 2017, which includes the new CICS Transaction Server for z/VSE 2.2.IBM recommends that z/VSE customers run Linux on z Systems alongside, on the same physical system, to provide another 64-bit application environment that can access and extend z/VSE applications and data via Hipersockets using a wide variety of middleware. CICS, one of the most popular enterprise transaction processing systems, is extremely popular among z/VSE users and now supports recent innovations such as Web services. DB2 is also available and popular.Job Control Language (JCL) is z/VSE's batch processing interface. There is also another, special interface for system console operators. z/VSE, like z/OS systems, had traditionally supported 3270 terminal user interfaces. However, most z/VSE installations have at least begun to add Web browser access to z/VSE applications. z/VSE's TCP/IP is a separately priced option for historic reasons, and is available in two different versions from two vendors. Both vendors provide a full function TCP/IP stack with applications, such as telnet and FTP. One TCP/IP stack provides IPv4 communication only, the other IPv4 and IPv6 communication. In addition to the commercially available TCP/IP stacks for z/VSE, IBM also provides the Linux Fastpath method which uses IUCV socket connections to communicate with a Linux guest, also running on the mainframe. Using this method the z/VSE system is able to fully exploit the native Linux TCP/IP stack.[2]Since z/VSE 3.1, Fibre Channel access to SCSI storage devices is supported, although only on IBM's Enterprise Storage Server (ESS), IBM Storage DS8000, IBM Storwize V7000, V5000, V3700 and V9000.Older z/VSE versions[edit]The last VSE/ESA release - VSE/ESA 2.7 - is no longer supported since February 28, 2007.[3] z/VSE 3.1 was the last release, that was compatible with 31-bit mainframes, as opposed to z/VSE Version 4, 5 and 6. z/VSE 3.1 was supported to 2009.[3] z/VSE Version 4 is no longer supported since October 2014 (end of service for z/VSE 4.3).For VSE/ESA, DOS/VSE, VSE/SP, see History_of_IBM_mainframe_operating_systems#DOS/VSSee also[edit]\nz/OS\nz/TPF\nz/VM\nHistory_of_IBM_mainframe_operating_systems#DOS/VS\nHistory_of_IBM_mainframe_operating_systems\nReferences[edit]External links[edit]\nIBM z/VSE website\n", "subtitles": ["Overview", "Older z/VSE versions", "See also", "References", "External links"], "title": "VSE (operating system)"},
{"content": "Michael Fredric Sipser (born September 17, 1954) is a theoretical computer scientist who has made early contributions to computational complexity theory. He is a professor of Applied Mathematics and Dean of Science at the Massachusetts Institute of Technology.Biography[edit]Sipser was born and raised in Brooklyn, New York and moved to Oswego, New York when he was 12 years old. He earned his BA in mathematics from Cornell University in 1974 and his PhD in engineering from the University of California at Berkeley in 1980 under the direction of Manuel Blum.[1]He joined MIT's Laboratory for Computer Science as a research associate in 1979 and became an MIT professor the following year. From 2004 until 2014, he served as head of the MIT Mathematics department. He was appointed Dean of the MIT School of Science in 2014.[2] He is a fellow of the American Academy of Arts and Sciences.[3] In 2015 he was elected as a fellow of the American Mathematical Society for contributions to complexity theory and for leadership and service to the mathematical community.[4] He was elected as an ACM Fellow in 2017.[5]Scientific career[edit]Sipser specializes in algorithms and complexity theory, specifically efficient error correcting codes, interactive proof systems, randomness, quantum computation, and establishing the inherent computational difficulty of problems. He introduced the method of probabilistic restriction for proving super-polynomial lower bounds on circuit complexity in a paper joint with Merrick Furst and James B. Saxe.[6] Their result was later improved to be an exponential lower bound by Andrew Yao and Johan Ha\u030astad.[7]In an early derandomization theorem, Sipser showed that BPP is contained in the polynomial hierarchy,[8] subsequently improved by Peter Ga\u0301cs and Clemens Lautemann to form what is now known as the Sipser-Ga\u0300cs-Lautemann theorem. Sipser also established a connection between expander graphs and derandomization.[9] He and his PhD student Daniel Spielman introduced expander codes, an application of expander graphs.[10] With fellow graduate student David Lichtenstein, Sipser proved that Go is PSPACE hard.[11]In quantum computation theory, he introduced the adiabatic algorithm jointly with Edward Farhi, Jeffrey Goldstone, and Samuel Gutmann.[12]Sipser has long been interested in the P versus NP problem. In 1975, he wagered an ounce of gold with Leonard Adleman that the problem would be solved with a proof that P=\u0338NP by the end of the 20th century. Sipser sent Adleman an American gold eagle coin in 2000 because the problem remained (and remains) unsolved.[13]Notable books[edit]Sipser is the author of Introduction to the Theory of Computation,[14] a textbook for theoretical computer science.Personal life[edit]Sipser lives in Cambridge, Massachusetts with his wife, Ina, and has two children: a daughter, Rachel, who graduated from New York University, and a younger son, Aaron, who is an undergraduate at MIT.[1]References[edit]External links[edit]\nPersonal homepage at MIT\n", "subtitles": ["Biography", "Scientific career", "Notable books", "Personal life", "References", "External links"], "title": "Michael Sipser"},
{"content": "Edsger Wybe Dijkstra (Dutch: [\u02c8\u025btsx\u0259r \u02c8\u028bib\u0259 \u02c8d\u025bikstra] ( listen); 11 May 1930 \u2013 6 August 2002) was a Dutch systems scientist, programmer, software engineer, science essayist,[9] and early pioneer in computing science.[10] He held the Schlumberger Centennial Chair in Computer Sciences at the University of Texas at Austin from 1984 until his retirement in 1999. A theoretical physicist by training, Dijkstra worked as a programmer at the Mathematisch Centrum (Amsterdam) from 1952 to 1962. He was a professor of mathematics at the Eindhoven University of Technology (1962\u20131984) and a research fellow at the Burroughs Corporation (1973\u20131984).One of the most influential members of computing science's founding generation, Dijkstra helped shape the new discipline from both an engineering and a theoretical perspective.[11][12] His fundamental contributions cover diverse areas of computing science, including compiler construction, operating systems, distributed systems, sequential and concurrent programming, programming paradigm and methodology, programming language research, program design, program development, program verification, software engineering principles, graph algorithms, and philosophical foundations of computer programming and computer science. Many of his papers are the source of new research areas. Several concepts and problems that are now standard in computer science were first identified by Dijkstra or bear names coined by him.[13][14]Computer programming in the 1950s to 1960s was not recognized as an academic discipline. In the late 1960s, computer programming was in a state of crisis. He was one of a small group of academics and industrial programmers who advocated a new programming style to improve the quality of programs. Dijkstra, who had a background in mathematics and physics, was one of the driving forces behind the acceptance of computer programming as a scientific discipline.[15][16] He coined the phrase structured programming and during the 1970s this became the new programming orthodoxy.[17][18][19] His ideas about structured programming helped lay the foundations for the birth and development of the professional discipline of software engineering, enabling programmers to organize and manage increasingly complex software projects.[20][21] As Bertrand Meyer (2009) noted, The revolution in views of programming started by Dijkstra's iconoclasm led to a movement known as structured programming, which advocated a systematic, rational approach to program construction. Structured programming is the basis for all that has been done since in programming methodology, including object-oriented programming.[22]The academic study of concurrent computing started in the 1960s, with Dijkstra (1965) credited with being the first paper in this field, identifying and solving the mutual exclusion problem.[23][24] He was also one of the early pioneers of the research on principles of distributed computing. His foundational work on concurrency, semaphores, mutual exclusion, deadlock (deadly embrace), finding shortest paths in graphs, fault-tolerance, self-stabilization, among many other contributions comprises many of the pillars upon which the field of distributed computing is built. Shortly before his death in 2002, he received the ACM PODC Influential-Paper Award in distributed computing for his work on self-stabilization of program computation. This annual award was renamed the Dijkstra Prize (Edsger W. Dijkstra Prize in Distributed Computing) the following year, in his honor.[25][26][27] As the prize, sponsored jointly by the ACM Symposium on Principles of Distributed Computing (PODC) and the EATCS International Symposium on Distributed Computing (DISC), recognizes that No other individual has had a larger influence on research in principles of distributed computing.Biography[edit]Early years[edit]Edsger W. Dijkstra was born in Rotterdam. His father was a chemist who was president of the Dutch Chemical Society; he taught chemistry at a secondary school and was later its superintendent. His mother was a mathematician, but never had a formal job.[28][29]Dijkstra had considered a career in law and had hoped to represent the Netherlands in the United Nations. However, after graduating from school in 1948, at his parents' suggestion he studied mathematics and physics and then theoretical physics at the University of Leiden.[11]In the early 1950s, electronic computers were a novelty. Dijkstra stumbled on his career quite by accident, and through his supervisor, Professor A. Haantjes, he met Adriaan van Wijngaarden, the director of the Computation Department at the Mathematical Center in Amsterdam, who offered Dijkstra a job; he officially became the Netherlands' first programmer in March 1952.[11]For some time Dijkstra remained committed to physics, working on it in Leiden three days out of each week. With increasing exposure to computing, however, his focus began to shift. As he recalled:[30]\nAfter having programmed for some three years, I had a discussion with A. van Wijngaarden, who was then my boss at the Mathematical Center in Amsterdam, a discussion for which I shall remain grateful to him as long as I live. The point was that I was supposed to study theoretical physics at the University of Leiden simultaneously, and as I found the two activities harder and harder to combine, I had to make up my mind, either to stop programming and become a real, respectable theoretical physicist, or to carry my study of physics to a formal completion only, with a minimum of effort, and to become....., yes what? A programmer? But was that a respectable profession? For after all, what was programming? Where was the sound body of knowledge that could support it as an intellectually respectable discipline? I remember quite vividly how I envied my hardware colleagues, who, when asked about their professional competence, could at least point out that they knew everything about vacuum tubes, amplifiers and the rest, whereas I felt that, when faced with that question, I would stand empty-handed. Full of misgivings I knocked on van Wijngaarden's office door, asking him whether I could speak to him for a moment; when I left his office a number of hours later, I was another person. For after having listened to my problems patiently, he agreed that up till that moment there was not much of a programming discipline, but then he went on to explain quietly that automatic computers were here to stay, that we were just at the beginning and could not I be one of the persons called to make programming a respectable discipline in the years to come? This was a turning point in my life and I completed my study of physics formally as quickly as I could.\n\u2014 Edsger Dijkstra, The Humble Programmer (EWD340), Communications of the ACM\nWhen Dijkstra married Maria (Ria) C. Debets in 1957, he was required as a part of the marriage rites to state his profession. He stated that he was a programmer, which was unacceptable to the authorities, there being no such profession at that time in The Netherlands.[31][32]In 1959 he received his PhD from the University of Amsterdam for a thesis entitled 'Communication with an Automatic Computer', devoted to a description of the assembly language designed for the first commercial computer developed in the Netherlands, the X1. His thesis supervisor was van Wijngaarden.[13]Mathematisch Centrum, Amsterdam[edit]From 1952 until 1962 Dijkstra worked at the Mathematisch Centrum in Amsterdam,[13] where he worked closely with Bram Jan Loopstra and Carel S. Scholten, who had been hired to build a computer. Their mode of interaction was disciplined: They would first decide upon the interface between the hardware and the software, by writing a programming manual. Then the hardware designers would have to be faithful to their part of the contract, while Dijkstra, the programmer, would write software for the nonexistent machine. Two of the lessons he learned from this experience were the importance of clear documentation, and that program debugging can be largely avoided through careful design.[11] Dijkstra formulated and solved the shortest path problem for a demonstration at the official inauguration of the ARMAC computer in 1956, but\u2014because of the absence of journals dedicated to automatic computing\u2014did not publish the result until 1959.At the Mathematical Center, Dijkstra and his colleague Jaap Zonneveld (nl) developed a compiler for the programming language ALGOL 60; it had a profound influence on his later thinking on programming as a scientific activity. He and Zonneveld had completed the implementation of the first ALGOL 60 compiler by August 1960, more than a year before a compiler was produced by another group.[11]Eindhoven University of Technology[edit]In 1962 Dijkstra moved to Eindhoven, and later to Nuenen, in the south of the Netherlands, where he became a professor in the Mathematics Department at the Eindhoven University of Technology.[13] The university did not have a separate computer science department and the culture of the mathematics department did not particularly suit him. Dijkstra tried to build a group of computer scientists who could collaborate on solving problems. This was an unusual model of research for the Mathematics Department.[11] In the late 1960s he built the THE operating system (named for the university, then known as Technische Hogeschool Eindhoven), which has influenced the designs of subsequent operating systems through its use of software based paged virtual memory.[33]Burroughs Corporation[edit]Dijkstra joined Burroughs Corporation, a company known at that time for the production of computers based on an innovative hardware architecture, as its Research Fellow in August 1973. His duties consisted of visiting some of the company's research centers a few times a year and carrying on his own research, which he did in the smallest Burroughs research facility, namely, his study on the second floor of his house in Nuenen. In fact, Dijkstra was the only research fellow of Burroughs Corporation and worked for it from home, occasionally travelling to its branches in the United States. As a result, he reduced his appointment at the university to one day a week. That day, Tuesday, soon became known as the day of the famous 'Tuesday Afternoon Club', a seminar during which he discussed with his colleagues scientific articles, looking at all aspects \u2013 notation, organisation, presentation, language, content, etc. Shortly after he moved in 1984 to the University of Texas at Austin (USA), a new 'branch' of the Tuesday Afternoon Club emerged in Austin.[13]The Burroughs years saw him at his most prolific in output of research articles. He wrote nearly 500 documents in the EWD series (described below), most of them technical reports, for private circulation within a select group.[11]The University of Texas at Austin[edit]Dijkstra accepted the Schlumberger Centennial Chair in the Computer Science Department at the University of Texas at Austin in 1984.Last years[edit]Dijkstra worked in Austin until his retirement in November 1999. To mark the occasion and to celebrate his forty-plus years of seminal contributions to computing science, the Department of Computer Sciences organized a symposium, which took place on his 70th birthday in May 2000.[11]Dijkstra and his wife returned from Austin to his original house in Nuenen (Netherlands) where he found that he had only months to live. He said that he wanted to retire in Austin, Texas, but to die in the Netherlands. Dijkstra died on 6 August 2002 after a long struggle with cancer.[34] He and his wife Maria (Ria) Debets were survived by their three children: Marcus, Femke and the computer scientist Rutger M. Dijkstra.Scientific contributions and impacts[edit]As an early theoretical pioneer in many research areas of computing science, Dijkstra helped shape the new discipline from both an engineering and an academic perspective. Many of his papers are the source of new research areas. Many concepts that are now standard in computer science were first identified by Dijkstra and/or bear names coined by him. Several important problems were also first formulated and solved by him. A 1994 survey of over a thousand professors of computer science was conducted to obtain a list of 38 most influential scholarly papers in the field, and Dijkstra is the author of five papers.[35][36][37]During his forty-plus years as a computing scientist, which included positions in both academia and industry, Dijkstra made numerous seminal contributions to many areas of computing science, including compiler construction, operating systems, concurrent programming (concurrent computing), distributed programming (distributed computing), programming paradigm and methodology, programming language research, program design, program development, program verification, software engineering principles, algorithm design, and philosophical foundations of computer programming and computer science. In addition, Dijkstra was intensely interested in teaching computer science, and in the relationships between academic computing science and the software industry.Algorithmic work[edit]Dijkstra's algorithmic work (especially graph algorithms, concurrent algorithms, and distributed algorithms) plays an important role in many areas of computing science. According to Leslie Lamport (2002), Dijkstra started the field of concurrent and distributed algorithms with his 1965 CACM paper Solution of a Problem in Concurrent Programming Control, in which he first stated and solved the mutual exclusion problem. As Lamport explains, that paper is probably why PODC exists (...). It remains to this day the most influential paper in the field. That it did not win a PODC Influential Paper Award reflects an artificial separation between concurrent and distributed algorithms\u2013a separation that has never existed in Dijkstra's work.[38]In 1959 Dijkstra published in a 3-page article 'A note on two problems in connexion with graphs' the algorithm to find the shortest path in a graph between any two given nodes, now called Dijkstra's algorithm. Its impact over the next 40 years is summarised from the article of Mikkel Thorup, 'Undirected Single Source Shortest Paths with Positive Integer Weights in Linear Time' (1999): Since 1959, all theoretical developments in SSSP [Single-Source Shortest Paths] for general directed and undirected graphs have been based on Dijkstra's algorithm. Dijkstra's algorithm is used in SPF, Shortest Path First, which is used in the routing protocols OSPF and IS-IS. Various modifications to Dijkstra's algorithm have been proposed by many authors using heuristics to reduce the run time of shortest path search. One of the most used heuristic algorithms is the A* search algorithm (first described by Peter Hart, Nils Nilsson and Bertram Raphael of Stanford Research Institute in 1968),[39] the main goal is to reduce the run time by reducing the search space. Dijkstra thought about the shortest path problem when working at the Mathematical Center in Amsterdam in 1956 as a programmer to demonstrate capabilities of a new computer called ARMAC. His objective was to choose both a problem as well as an answer (that would be produced by computer) that non-computing people could understand. He designed the shortest path algorithm in about 20 minutes without aid of paper and pen and later implemented it for ARMAC for a slightly simplified transportation map of 64 cities in the Netherlands (so that 6 bits would suffice to represent the city in the algorithm).[40] As he recalled, in an interview published in 2001:[41]\nWhat is the shortest way to travel from Rotterdam to Groningen, in general: from given city to given city. It is the algorithm for the shortest path, which I designed in about twenty minutes. One morning I was shopping in Amsterdam with my young fiance\u0301e, and tired, we sat down on the cafe\u0301 terrace to drink a cup of coffee and I was just thinking about whether I could do this, and I then designed the algorithm for the shortest path. As I said, it was a twenty-minute invention. In fact, it was published in \u201959, three years late. The publication is still readable, it is, in fact, quite nice. One of the reasons that it is so nice was that I designed it without pencil and paper. I learned later that one of the advantages of designing without pencil and paper is that you are almost forced to avoid all avoidable complexities. Eventually that algorithm became, to my great amazement, one of the cornerstones of my fame.\n\u2014 Edsger Dijkstra, in an interview with Philip L. Frana, Communications of the ACM 53 (8), 2001.\nA year later, he came across another problem from hardware engineers working on the institute's next computer: minimize the amount of wire needed to connect the pins on the back panel of the machine. As a solution, he re-discovered the algorithm known as Prim's minimal spanning tree algorithm. The Prim's algorithm was originally developed in 1930 by Czech mathematician Vojte\u030cch Jarni\u0301k[42] and later independently rediscovered and republished by Robert C. Prim in 1957[43] and Dijkstra in 1959.[44] Therefore, it is also sometimes called the DJP algorithm.[45]In 1961 Dijkstra first described the shunting-yard algorithm, a method for parsing mathematical expressions specified in infix notation, in the Mathematisch Centrum report.[46] It can be used to produce output in Reverse Polish notation (RPN) or as an abstract syntax tree (AST). The algorithm was named the shunting yard algorithm because its operation resembles that of a railroad shunting yard. The shunting-yard algorithm is commonly used to implement operator-precedence parsers.In 1962 or 1963 Dijkstra proposed the semaphore mechanism for mutual exclusion algorithm for n processes (a generalization of Dekker's algorithm), which was probably the first published concurrent algorithm and which introduced a new area of algorithmic research. He also identified the deadlock problem and proposed the banker's algorithm that prevents deadlock.In 1974 Dijkstra presented three self-stabilizing algorithms for mutual exclusion on a ring. Dijkstra's work is considered to be the first to introduce and demonstrate the self-stabilization concept.[47]In the mid-1970s Dijkstra (together with other authors) introduced two useful abstractions (mutator and collector) to the study of garbage collection. The mutator abstracts the process that performs the computation, including allocation of a new storage cell. The collector is the process that automatically reclaims garbage. Furthermore, this paper gives a formalization of tri-color marking that is basic to incremental garbage collection.[48][49]In the early 1980s Dijkstra and Carel S. Scholten proposed the Dijkstra\u2013Scholten algorithm for detecting termination in distributed systems.In 1981 Dijkstra developed smoothsort, a comparison-based sorting algorithm and a variation of heapsort.Compiler construction and programming language research[edit]Dijkstra was known to be a fan of ALGOL 60, and worked on the team that implemented the first compiler for that language. He was closely involved in the ALGOL 60 development, realisation and popularisation. As discussed by Peter Naur in the article 'The European side of the last phase of the development of ALGOL 60', in the Proceedings of the First ACM SIGPLAN Conference on History of Programming Languages, January 1978, Dijkstra took part in the period 1958\u20131959 in a number of meetings that culminated in the publication of the report defining the ALGOL 60 language. Dijkstra's name does not appear in the list of 13 authors of the final report. Apparently, he eventually left the committee because he could not agree with the majority opinions. Still, while at the Mathematisch Centrum (Amsterdam), he wrote jointly with Jaap Zonneveld the first ALGOL 60 compiler. Dijkstra and Zonneveld, who collaborated on the compiler, agreed not to shave until the project was completed; while Zonneveld shaved shortly thereafter, Dijkstra kept his beard for the rest of his life.[50]ALGOL was the result of a collaboration of American and European committees. ALGOL 60 (short for ALGOrithmic Language 1960) is a member of the ALGOL family of computer programming languages. It followed on from ALGOL 58 and inspired many languages that followed it. It gave rise to many other programming languages, including BCPL, B, Pascal, Simula and C.[51] Algol 60 was a sophisticatedly designed computer language and it provided a large number of hitherto unknown implementation challenges. As Bjarne Stroustrup notes, one problem with Algol60 was that no one knew how to implement it.[52] A major new challenge in Algol 60 implementation was the run-time allocation and management of data. In 1960 Dijkstra and Zonneveld showed how recursive procedures could be executed using a run-time stack of activation records, and how to efficiently access identifiers from statically enclosing scopes using the so-called 'display'.[53] The ALGOL 60 compiler was one of the first to support recursion[54] employing a novel method to do so. Dijkstra's short book Primer of Algol 60 Programming, originally published in 1962, was the standard reference for the language for several years.Programming paradigm and methodology[edit]Computer programming in the 1950s to 1960s was not recognized as an academic discipline and unlike mature sciences there were no theoretical concepts or coding systems. Programming as a professional activity was poorly understood in those years.In the late 1960s computer programming was in state of crisis. Software crisis is a term used in the early days of computing science for the difficulty of writing useful and efficient computer programs in the required time. The software crisis was due to the rapid increases in computer power and the complexity of the problems that could be tackled. With the increase in the complexity of the software, many software problems arose because existing methods were neither sufficient nor up to the mark. The term software crisis was coined by some attendees at the first NATO Software Engineering Conference in 1968 at Garmisch, Germany.[55][56][57] His 1972 ACM Turing Award Lecture makes reference to this same problem: The major cause of the software crisis is that the machines have become several orders of magnitude more powerful! To put it quite bluntly: as long as there were no machines, programming was no problem at all; when we had a few weak computers, programming became a mild problem, and now we have gigantic computers, programming has become an equally gigantic problem.[30]While Dijkstra had programmed extensively in machine code in the 1950s, he came to the conclusion that in high-level languages frequent use of the GOTO statement was usually symptomatic of poor structure. In 1968 he wrote a private paper A Case against the GO TO Statement,[58] which was then published as a letter in CACM.[59] Editor Niklaus Wirth gave this letter the heading Go To Statement Considered Harmful, which introduced the phrase considered harmful into computing.Dijkstra argued that the programming statement GOTO, found in many high-level programming languages, is a major source of errors, and should therefore be eliminated. This letter caused a huge debate in the programming community. Some went to the length of equating good programming with the elimination of GO TO. Dijkstra refused to mention the debate, or even the GO TO statement, in his article Notes on Structured Programming. The debate has long since died down; programming languages provide alternatives to the GO TO, few programmers today use it liberally, and most never use it at all.[11]Dijkstra's thesis was that departures from linear control flow were clearer if allowed only in disciplined higher-level structures such as the if-then-else statement and the while loop. This methodology was developed into structured programming movement, the title of his 1972 book, coauthored with C.A.R. Hoare and Ole-Johan Dahl. Considered by many as the first significant movement in history of computer programming, structured programming became the new programming orthodoxy during the 1970s.[60][61][62] Bertrand Meyer remarked that, The revolution in views of programming started by Dijkstra's iconoclasm led to a movement known as structured programming, which advocated a systematic, rational approach to program construction. Structured programming is the basis for all that has been done since in programming methodology, including object-oriented programming.[63]Structured programming is often regarded as goto-less programming. But as Bertrand Meyer notes, As the first book on the topic [Structured Programming by Dijkstra, Dahl, and Hoare] shows, structured programming is about much more than control structures and the goto. Its principal message is that programming should be considered a scientific discipline based on mathematical rigor.[64] As a programming paradigm, structured programming \u2013 especially in the 1970s and 1980s \u2013 significantly influenced the birth of many modern programming languages such as Pascal,[65][66] C, Modula-2, and Ada.[67] The Fortran 77 version which incorporates the concepts of structured programming, was released in 1978. The C++ language was a considerably extended and enhanced version of the popular structured programming language C (see also: list of C-based programming languages). Since C++ was developed from a more traditional structured language, it is a 'hybrid language', rather than a pure object-oriented programming language.[68]In his article Structured Programming: Retrospect and Prospect (1986), Harlan Mills writes, Edsger W. Dijkstra's 1969 Structured Programming article precipitated a decade of intense focus on programming techniques that has fundamentally altered human expectations and achievements in software development. Before this decade of intense focus, programming was regarded as a private, puzzle-solving activity of writing computer instructions to work as a program. After this decade, programming could be regarded as a public, mathematics-based activity of restructuring specifications into programs. Before, the challenge was in getting programs to run at all, and then in getting them further debugged to do the right things. After, programs could be expected to both run and do the right things with little or no debugging. Before, it was common wisdom that no sizable program could be error-free. After, many sizable programs have run a year or more with no errors detected. These expectations and achievements are not universal because of the inertia of industrial practices. But they are well-enough established to herald fundamental change in software development.The book Concise Encyclopedia of Computer Science (2004), edited by Edwin D. Reilly, notes that the major contributions of structured programming have been twofold\u2014the elevation of programming technique to something less of an art and more of a science, and the demonstration that carefully structured programs can be creative works of sufficient literary merit to deserve being read by humans and not just by computer.[69]Program design and development (software engineering research)[edit]Dijkstra's ideas about programming methodology (especially the structured programming movement) helped lay the foundations for the birth and development of the professional discipline of software engineering (in particular the software design and development), enabling programmers to organize and manage increasingly complex software projects.[70][71] In the late 1960s Dijkstra discussed the concept of program families. And in the mid 1970s David Parnas and others clarified the idea and showed how to apply it in software engineering principles.The rise of the structured programming movement led to many other structured approaches applied to software design. The techniques of structured analysis and structured design are outgrowths of structured programming concepts and techniques, and of the early ideas about modular design. Principles of modularity were strengthened by Larry Constantine's concepts of coupling (to be minimized between modules) and cohesion (to be maximized within modules), by David Parnas's techniques of information hiding, and by abstract data types.[72] A number of tools and methods employing structured concepts were developed, such as Structured Design, Jackson's Structured Programming, Ross' Structured Analysis and Design Technique (SADT), Yourdon's Structured Method, Structured Systems Analysis and Design Method (SSADM), and James Martin's Information Engineering. The field of software metrics is often considered as a direct influence of the structured programming movement on software engineering in the 1970s.Separation of concerns (SoC), one of the basic principles in software engineering, is a design principle for separating a computer program into distinct sections, such that each section addresses a separate concern. The term separation of concerns was coined by Dijkstra in his 1974 paper On the role of scientific thought.[73]Operating system research[edit]In the 1960s Dijkstra and his colleagues in Eindhoven designed and implemented THE (standing for 'Technische Hogeschool Eindhoven') operating system, which was organised into clearly identified layers.[74] His 1968 article on this subject provided the foundation for subsequent designs of the operating systems. The IEEE Computer Society's David Alan Grier writes, We generally trace the idea of building computer systems in layers back to a 1967 paper that the Dutch computer scientist Edsger Dijkstra gave to a joint IEEE Computer Society/ACM conference. Prior to this paper, engineers had struggled with the problem of how to organize software. If you look at early examples of programs, and you can find many in the electronic library of the Computer Society, you will find that most code of that era is complicated, difficult to read, hard to modify, and challenging to reuse. In his 1967 paper, Dijkstra described how software could be constructed in layers and gave an example of a simple operating system that used five layers. He admitted that this system might not be a realistic test of his ideas but he argued that the larger the project, the more essential the structuring! The idea of using layers to control complexity has become a mainstay of software architecture. We see it in many forms and apply it to many problems. We see it in the hierarchy of classes in object-oriented programming and in the structure of Service-Oriented Architecture (SOA). SOA is a relatively recent application of layering in computer science. It was articulated in 2007 as a means of controlling complexity in business systems, especially distributed systems that make substantial use of the Internet. Like Dijkstra's plan for system development, its layering system is called the SOA Solution Stack or S3. The S3's nine layers are: 1) operational systems, 2) service components, 3) services, 4) business processes, 5) consumer actions, 6) system integration, 7) quality control and assurance, 8) information architecture, and 9) system governance and policies.[75]Dijkstra organized the design of the system in layers in order to reduce the overall complexity of the software. Though the term 'architecture' had not yet been used to describe software design, this was certainly considered the first glimpse of software architecture.[1] It introduced a number of design principles which have become part of the working vocabulary of every professional programmer: levels of abstraction, programming in layers, the semaphore, and cooperating sequential processes. His original paper on the THE operating system was reprinted in the 25th Anniversary issue of Communications of the ACM, in January 1983. By way of introduction, the Editor-in-Chief says, This project initiated a long line of research in multilevel systems architecture \u2014 a line that continues to the present day because hierarchical modularity is a powerful approach to organizing large systems.[11]Concurrent computing and programming[edit]In a one-page paper from 1965 Dijkstra introduced the 'mutual exclusion problem' for n processes and discussed a solution to it. It was probably the first published concurrent algorithm.[7][13] The notion, standard by now, of a 'critical section' was also introduced in this paper. Per Brinch Hansen, a pioneer in the field of concurrent computing, considers Dijkstra's Cooperating Sequential Processes (1965) to be the first classic paper in concurrent programming. As Brinch Hansen notes, 'Dijkstra lays the conceptual foundation for abstract concurrent programming' with that paper.[77]In 1968 Dijkstra published his seminal paper 'Cooperating sequential processes', a 70-page essay that originated the field of concurrent programming. He discussed in it the notion of mutual exclusion (mutex) and the criteria a satisfactory solution should satisfy. He also redressed the historical perspective left out of his 1965 paper by including the first known correct solution to the mutual exclusion problem, for two processes, due to Theodorus Dekker. Dijkstra subsequently generalized Dekker's solution to n processes.[78][79] Further, he proposed the first synchronisation mechanism for concurrent processes,[80] the semaphore with its two operations, P and V. He also identified the 'deadlock problem' (called there 'the problem of the deadly embrace')[81] and proposed an elegant 'Banker's algorithm' that prevents deadlock. The deadlock detection and prevention became perennial research problems in the field of concurrent programming.The dining philosophers problem is an example problem often used in concurrent algorithm design to illustrate synchronization issues and techniques for resolving them. It was originally formulated in 1965 by Dijkstra as a student exam exercise, presented in terms of computers competing for access to tape drive peripherals. Soon after, Tony Hoare gave the problem its present formulation.[82] The sleeping barber problem is also attributed to Dijkstra.In his book Concurrent Programming: Algorithms, Principles, and Foundations[83] Michel Raynal writes, Since the early work of E.W. Dijkstra (1965), who introduced the mutual exclusion problem, the concept of a process, the semaphore object, the notion of a weakest precondition, and guarded commands (among many other contributions), synchronization is no longer a catalog of tricks but a domain of computing science with its own concepts, mechanisms, and techniques whose results can be applied in many domains. This means that process synchronization has to be a major topic of any computer science curriculum.John W. McCormick et al. (2011) notes, The notion of the concurrent program as a means for writing parallel programs without regard for the underlying hardware was first introduced by Edsger Dijkstra (1968). Moti Ben-Ari (1982) elegantly summed up Dijkstra's idea in three sentences: \u2018Concurrent programming is the name given to programming notation and techniques for expressing potential parallelism and solving the resulting synchronization and communication problems. Implementation of parallelism is a topic in computer systems (hardware and software) that is essentially independent of concurrent programming. Concurrent programming is important because it provides an abstract setting in which to study parallelism without getting bogged down in the implementation details.\u2019[84]Distributed computing[edit]Dijkstra was one of the very early pioneers of the research on principles of distributed computing.[85][86] As the citation for the Dijkstra Prize recognizes, no other individual has had a larger influence on research in principles of distributed computing. Some of his papers are even considered to be those that established the field. Dijkstra's 1965 paper, Solution of a Problem in Concurrent Programming Control was the first to present the correct solution to the mutual exclusion problem. Leslie Lamport writes that this work is probably why PODC exists and it started the field of concurrent and distributed algorithms.[87]In particular, his paper Self-stabilizing Systems in Spite of Distributed Control (1974) started the sub-field of self-stabilization. It is also considered as the first scientific examination of fault-tolerant systems.[7] Dijkstra's paper was not widely noticed until Leslie Lamport's invited talk at the ACM Symposium on Principles of Distributed Computing (PODC) in 1983. In his report on Dijkstra's work on self-stabilizing distributed systems, Lamport regard it to be 'a milestone in work on fault tolerance' and 'a very fertile field for research'.[88]Formal specification and verification[edit]From the 1970s, Dijkstra's chief interest was formal verification. In 1976 Dijkstra published a seminal book, A Discipline of Programming, which put forward his method of systematic development of programs together with their correctness proofs. In his exposition he used his 'Guarded Command Language'. The language, with its reliance on non-determinism, the adopted weakest precondition semantics and the proposed development method has had a considerable impact on the field to this day. The refinement calculus, originally proposed by Ralph-Johan Back[89] and developed by Carroll Morgan,[90] is an extension of Dijkstra's weakest precondition calculus, where program statements are modeled as predicate transformers.[91]In 1984, to add further support to this approach to programming, he published jointly with Wim Feijen an introductory textbook for first-year students of computer science. The book, first published in Dutch, was entitled Een methode van programmeren. The English edition appeared in 1988 as A Method of Programming.On the nature of computer science and computer programming[edit]Many of his opinions on computer science and programming have become widespread. For example, the programming phrase two or more, use a for (a rule of thumb when to use a loop) is sometimes attributed to him.[92]He was the first to make the claim that programming is so inherently complex that, in order to manage it successfully, programmers need to harness every trick and abstraction possible.Dijkstra was one of the most famous opponents of the engineering view of computing science. Like Peter Naur and Kristen Nygaard, Dijkstra disliked the very term 'computer science'. Computer science, as Dijkstra pointed out, deserves a better name. He suggests it can be called 'computing science'. Instead of the computer, or computing technology, Dijkstra wanted to emphasize the abstract mechanisms that computing science uses to master complexity. When expressing the abstract nature of computing science, he wrote,\nA confusion of even longer standing came from the fact that the unprepared included the electronic engineers that were supposed to design, build and maintain the machines. The job was actually beyond the electronic technology of the day, and, as a result, the question of how to get and keep the physical equipment more or less in working condition became in the early days the all-overriding concern. As a result, the topic became \u2013 primarily in the USA \u2013 prematurely known as \u2018computer science\u2019 \u2013 which, actually, is like referring to surgery as \u2018knife science\u2019 \u2013 and it was firmly implanted in people's minds that computing science is about machines and their peripheral equipment. Quod non [Latin: Which is not true]. We now know that electronic technology has no more to contribute to computing than the physical equipment. We now know that programmable computer is no more and no less than an extremely handy device for realizing any conceivable mechanism without changing a single wire, and that the core challenge for computing science is hence a conceptual one, viz., what (abstract) mechanisms we can conceive without getting lost in the complexities of our own making.[93]\nIn The Humble Programmer (1972), Dijkstra wrote: We must not forget that it is not our [computing scientists'] business to make programs, it is our business to design classes of computations that will display a desired behaviour.Dijkstra also opposed the inclusion of software engineering under the umbrella of academic computer science. He wrote that, As economics is known as The Miserable Science, software engineering should be known as The Doomed Discipline, doomed because it cannot even approach its goal since its goal is self-contradictory. And software engineering has accepted as its charter How to program if you cannot..[94]Personality and working style[edit]In the world of computing science, Dijkstra is well known as a character. In the preface of his book A Discipline of Programming (1976) he stated the following: For the absence of a bibliography I offer neither explanation nor apology. In fact, most of his articles and books have no references at all.[13] This approach to references was deplored by some researchers. But Dijkstra chose this way of working to preserve his self-reliance.As a university professor for much of his life, Dijkstra saw teaching not just as a required activity but as a serious research endeavor.[11] His approach to teaching was unconventional.[95] His lecturing style has been described as idiosyncratic. When lecturing, the long pauses between sentences have often been attributed to the fact that English is not Dijkstra's first language. However the pauses also served as a way for him to think on his feet and he was regarded as a quick and deep thinker while engaged in the act of lecturing. His courses for students in Austin had little to do with computer science but they dealt with the presentation of mathematical proofs.[13] At the beginning of each semester he would take a photo of each of the students, in order to memorize their names. He never followed a textbook, with the possible exception of his own while it was under preparation. When lecturing, he would write proofs in chalk on a blackboard rather than using overhead foils. He invited the students to suggest ideas, which he then explored, or refused to explore because they violated some of his tenets. He assigned challenging homework problems, and would study his students' solutions thoroughly. He conducted his final examinations orally, over a whole week. Each student was examined in Dijkstra's office or home, and an exam lasted several hours.[11]He was also highly original in his way of assessing people's capacity for a job. When Vladimir Lifschitz came to Austin in 1990 for a job interview, Dijkstra gave him a puzzle. Vladimir solved it and has been working in Austin since then.[13]Despite having invented much of the technology of software, Dijkstra eschewed the use of computers in his own work for many decades. Even after he succumbed to his UT colleagues' encouragement and acquired a Macintosh computer, he used it only for e-mail and for browsing the World Wide Web.[96] Dijkstra never wrote his articles using a computer. He preferred to rely on his typewriter and later on his Montblanc pen.[13] Dijkstra's favorite writing instrument was the Montblanc Meisterstu\u0308ck fountain pen. He repeatedly tried other pens, but none ever displaced the Montblanc.He had no use for word processors, believing that one should be able to write a letter or article without rough drafts, rewriting, or any significant editing. He would work it all out in his head before putting pen to paper, and once mentioned that when he was a physics student he would solve his homework problems in his head while walking the streets of Leiden.[11] Most of Dijkstra's publications were written by him alone. He never had a secretary and took care of all his correspondence alone.[13] When colleagues prepared a Festschrift for his sixtieth birthday, published by Springer-Verlag, he took the trouble to thank each of the 61 contributors separately, in a hand-written letter.[13]Throughout Dijkstra's career, his work was characterized by elegance and economy.[13] A prolific writer (especially as an essayist), Dijkstra authored more than 1,300 papers, many written by hand in his precise script. They were essays and parables; fairy tales and warnings; comprehensive explanation and pedagogical pretext. Most were about mathematics and computer science; others were trip reports that are more revealing about their author than about the people and places visited. It was his habit to copy each paper and circulate it to a small group of colleagues who would copy and forward the papers to another limited group of scientists.[97] His love affair with simplicity came at an early age and under his mother's guidance. He once said he had asked his mother whether trigonometry was a difficult topic. She replied that he must learn all the formulas and that furthermore if he required more than five lines to prove something, he was on the wrong track.[98]Dijkstra was famous for his wit, eloquence, and way with words, such as in his remark, The question of whether Machines Can Think (...) is about as relevant as the question of whether Submarines Can Swim.; his advice to a promising researcher, who asked how to select a topic for research, Do only what only you can do.[11] Dijkstra was also known for his vocal criticism. As an outspoken and critical visionary, he strongly opposed the teaching of BASIC.[99]In many of his more humorous essays, Dijkstra described a fictional company of which he served as chairman. The company was called Mathematics, Inc., a company that he imagined having commercialized the production of mathematical theorems in the same way that software companies had commercialized the production of computer programs. He invented a number of activities and challenges of Mathematics Inc. and documented them in several papers in the EWD series. The imaginary company had produced a proof of the Riemann Hypothesis but then had great difficulties collecting royalties from mathematicians who had proved results assuming the Riemann Hypothesis. The proof itself was a trade secret.[100] Many of the company's proofs were rushed out the door and then much of the company's effort had to be spent on maintenance.[101] A more successful effort was the Standard Proof for Pythagoras' Theorem, that replaced the more than 100 incompatible existing proofs.[102] Dijkstra described Mathematics Inc. as the most exciting and most miserable business ever conceived.[100] EWD 443 (1974) describes his fictional company as having over 75 percent of the world's market share.[103][104]EWD manuscripts[edit]Dijkstra was well known for his habit of carefully composing manuscripts with his fountain pen. The manuscripts are called EWDs, since Dijkstra numbered them with EWD, his initials, as a prefix. According to Dijkstra himself, the EWDs started when he moved from the Mathematical Centre in Amsterdam to the Eindhoven University of Technology (then Technische Hogeschool Eindhoven). After going to Eindhoven, Dijkstra experienced a writer's block for more than a year. Dijkstra distributed photocopies of a new EWD among his colleagues. Many recipients photocopied and forwarded their copies, so the EWDs spread throughout the international computer science community. The topics were computer science and mathematics, and included trip reports, letters, and speeches. These short articles span a period of 40 years. Almost all EWDs appearing after 1972 were hand-written. They are rarely longer than 15 pages and are consecutively numbered. The last one, No. 1318, is from 14 April 2002. Within computer science they are known as the EWD reports, or, simply the EWDs. More than 1300 EWDs have been scanned, with a growing number transcribed to facilitate search, and are available online at the Dijkstra archive of the University of Texas.[105]Personal life[edit]Dijkstra's self-confidence went together with a remarkably modest lifestyle, to the point of being spartan.[13] His and his wife's house in Nuenen was simple, small and unassuming. He did not own a TV, a VCR or a mobile telephone, and did not go to the movies.[13] In contrast, he played the piano well and, while in Austin, liked to go to concerts. An enthusiastic listener of classical music, Dijkstra's favorite composer was Mozart.[11]Death[edit]Dijkstra died on 6 August 2002. According to officials at the University of Texas, the cause of death was cancer.[106]Influence and recognition[edit]In 1972 the Association for Computing Machinery (ACM) acknowledged Dijkstra's seminal contributions to the field by awarding him the distinguished Turing Award. The citation for the award reads:[108]\nEdsger Dijkstra was a principal contributor in the late 1950's to the development of the ALGOL, a high level programming language which has become a model of clarity and mathematical rigor. He is one of the principal exponents of the science and art of programming languages in general, and has greatly contributed to our understanding of their structure, representation, and implementation. His fifteen years of publications extend from theoretical articles on graph theory to basic manuals, expository texts, and philosophical contemplations in the field of programming languages.\nThe introduction given at the awards ceremony is a tribute to Dijkstra:[108]\nThe working vocabulary of programmers everywhere is studded with words originated or forcefully promulgated by E.W. Dijkstra \u2013 display, deadly embrace, semaphore, go-to-less programming, structured programming. But his influence on programming is more pervasive than any glossary can possibly indicate. The precious gift that this Turing Award acknowledges is Dijkstra's style: his approach to programming as a high, intellectual challenge; his eloquent insistence and practical demonstration that programs should be composed correctly, not just debugged into correctness; and his illuminating perception of problems at the foundations of program design. (...) We have come to value good programs in much the same way as we value good literature. And at the center of this movement, creating and reflecting patterns no less beautiful than useful, stands E.W. Dijkstra.\nIn the words of Sir Tony Hoare, FRS, delivered by him at Dijkstra's funeral:[11]\nEdsger is widely recognized as a man who has thought deeply about many deep questions; and among the deepest questions is that of traditional moral philosophy: How is it that a person should live their life? Edsger found his answer to this question early in his life: He decided he would live as an academic scientist, conducting research into a new branch of science, the science of computing. He would lay the foundations that would establish computing as a rigorous scientific discipline; and in his research and in his teaching and in his writing, he would pursue perfection to the exclusion of all other concerns. From these commitments he never deviated, and that is how he has made to his chosen subject of study the greatest contribution that any one person could make in any one lifetime.\nIn March 2003, the following email was sent to the distributed computing community:[109]\nThis is to announce that the award formerly known as the PODC Influential-Paper Award has been renamed the Edsger W. Dijkstra Prize in Distributed Computing after the late Edsger W. Dijkstra, a pioneer in the area of distributed computing. His foundational work on concurrency primitives (such as the semaphore), concurrency problems (such as mutual exclusion and deadlock), reasoning about concurrent systems, and self-stabilization comprises one of the most important supports upon which the field of distributed computing is built. No other individual has had a larger influence on research in principles of distributed computing.\nFormer ACM President Peter J. Denning wrote about Dijkstra:[110]\nEdsger Dijkstra, one of the giants of our field and a passionate believer in the mathematical view of programs and programming (...) Over the previous quarter-century, he had formulated many of the great intellectual challenges of the field as programming\u2014the goto statement, structured programming, concurrent processes, semaphores, deadlocks, recursive programming in Algol, and deriving correct programs.\nAwards and honors[edit]Among Dijkstra's awards and honors are:[96]\nMember of the Royal Netherlands Academy of Arts and Sciences (1971)[111]\nDistinguished Fellow of the British Computer Society (1971)\nThe Association for Computing Machinery's A.M. Turing Award (1972)[112]\nHarry H. Goode Memorial Award from the IEEE Computer Society (1974).[113]\nForeign Honorary Member of the American Academy of Arts and Sciences (1975)\nDoctor of Science Honoris Causa from the Queen's University Belfast (1976)\nComputer Pioneer Charter Recipient from the IEEE Computer Society (1982)\nACM/SIGCSE Award for Outstanding Contributions to Computer Science Education (1989)\nFellow of the Association for Computing Machinery (1994)[114]\nHonorary doctorate from the Athens University of Economics & Business, Greece (2001).\nThe Distinguished Fellowship of the British Computer Society (BCS) is awarded under bylaw 7 of the BCS's Royal Charter. The award was first approved in 1969 and the first election was made in 1971 to Dijkstra.[115]On the occasion of Dijkstra's 60th birthday in 1990, The Department of Computer Science (UTCS) at the University of Texas at Austin organized a two-day seminar in his honor. Speakers came from all over the United States and Europe, and a group of computer scientists contributed research articles which were edited into a book.[11][116]In 2002, the C&C Foundation of Japan recognized Dijkstra for his pioneering contributions to the establishment of the scientific basis for computer software through creative research in basic software theory, algorithm theory, structured programming, and semaphores. Dijkstra was alive to receive notice of the award, but it was accepted by his family in an award ceremony after his death.Shortly before his death in 2002, Dijkstra received the ACM PODC Influential-Paper Award in distributed computing for his work on self-stabilization of program computation. This annual award was renamed the Dijkstra Prize (Edsger W. Dijkstra Prize in Distributed Computing) the following year, in his honor.The Dijkstra Award for Outstanding Academic Achievement in Computer Science (Loyola University Chicago, Department of Computer Science) is named for Edsger W. Dijkstra. Beginning in 2005, this award recognizes the top academic performance by a graduating computer science major. Selection is based on GPA in all major courses and election by department faculty.[117]The Department of Computer Science (UTCS) at the University of Texas at Austin hosted the inaugural Edsger W. Dijkstra Memorial Lecture on 12 October 2010. Tony Hoare, Emeritus Professor at Oxford and Principal Researcher at Microsoft Research, was the speaker for the event. This lecture series was made possible by a generous grant from Schlumberger to honor the memory of Dijkstra.See also[edit]\nDijkstra's algorithm\nDining philosophers problem\nGuarded Command Language\nPredicate transformer semantics\nWeakest precondition calculus\nSemaphore\nSmoothsort\nGo To Statement Considered Harmful\nOn the Cruelty of Really Teaching Computer Science\nList of pioneers in computer science\nList of important publications in computer science\nList of important publications in theoretical computer science\nList of important publications in concurrent, parallel, and distributed computing\nPublications[edit]Books:Selected articles:References[edit]Further reading[edit]\nApt, Krzysztof R. (2002). Edsger Wybe Dijkstra (1930\u20132002): A Portrait of a Genius. Formal Aspects of Computing. 14 (2): 92\u201398. arXiv:cs/0210001 . doi:10.1007/s001650200029. \nBrinch Hansen, Per (2002). The Origin of Concurrent Programming: From Semaphores to Remote Procedure Calls. Springer. ISBN 978-0-387-95401-1. \nBen-Ari, M. (2006). Principles of Concurrent and Distributed Programming (2nd ed.). Addison-Wesley. ISBN 978-0-321-31283-9. \nBroy, M.; Denert, Ernst, eds. (2002). Software Pioneers: Contributions to Software Engineering. Springer. p. 3. ISBN 978-3-540-43081-0. \nDaylight, Edgar G. (2012). The Dawn of Software Engineering: from Turing to Dijkstra. Lonely Scholar. ISBN 9789491386022. \nDaylight, Edgar G. (2011). Dijkstra's Rallying Cry for Generalization: The Advent of the Recursive Procedure, Late 1950s\u2013Early 1960s. The Computer Journal. 54 (11): 1756\u201372. doi:10.1093/comjnl/bxr002. \nDolev, Shlomi (2000). Self-stabilization. MIT Press. ISBN 978-0-262-04178-2. \nFeijen, W.H.J.; Gries, David, eds. (1990). Beauty Is Our Business: A Birthday Salute to Edsger W. Dijkstra. Springer. ISBN 978-0-387-97299-2. \nLaplante, Phillip A. (1996). Great papers in computer science. IEEE Press. ISBN 978-0-7803-1112-1. \nLee, J.A.N. (1991). Frontiers of Computing: A Tribute to Edsger W. Dijkstra on the Occasion of his 60th Birthday (PDF). Ann. Hist. Comp. 13 (1): 91\u201396. \nO\u2019Regan, Gerard (2013). Giants of Computing: A Compendium of Select, Pivotal Pioneers. Springer. ISBN 978-1-4471-5340-5. \nPayette, Sandy (2014). Hopper and Dijkstra: Crisis, Revolution, and the Future of Programming. IEEE Annals of the History of Computing. 36 (4): 64\u201373. doi:10.1109/MAHC.2014.54. \nShasha, Dennis; Lazere, Cathy (1998). Out of their Minds: The Lives and Discoveries of 15 Great Computer Scientists. Springer. ISBN 978-0-387-98269-4. \nExternal links[edit]\nE.W. Dijkstra Archive. Center for American History, University of Texas at Austin. \nDijkstra's Rallying Cry for Generalization. A site devoted to Dijkstra\u2019s works and thoughts, created and maintained by the historian of computing Edgar Graham Daylight. \n", "subtitles": ["Biography", "Scientific contributions and impacts", "Personality and working style", "EWD manuscripts", "Personal life", "Death", "Influence and recognition", "Awards and honors", "See also", "Publications", "References", "Further reading", "External links"], "title": "Edsger W. Dijkstra"},
{"content": "The Wayback Machine is a digital archive of the World Wide Web and other information on the Internet created by the Internet Archive, a nonprofit organization, based in San Francisco, California, United States.History[edit]The Internet Archive launched the Wayback Machine in October 2001.[4][5] It was set up by Brewster Kahle and Bruce Gilliat, and is maintained with content from Alexa Internet.[citation needed] The service enables users to see archived versions of web pages across time, which the archive calls a three dimensional index.[citation needed]Since 1996, the Wayback Machine has been archiving cached pages of websites onto its large cluster of Linux nodes.[citation needed] It revisits sites on occasion (see technical details below) and archives a new version.[6] Sites can also be captured on the fly by visitors who enter the site's URL into a search box.[citation needed] The intent is to capture and archive content that otherwise would be lost whenever a site is changed or closed down.[citation needed] The overall vision of the machine's creators is to archive the entire Internet.[citation needed]Information had been kept on digital tape for five years, with Kahle occasionally allowing researchers and scientists to tap into the clunky database.[7] When the archive reached its fifth anniversary, in 2001, it was unveiled and opened to the public in a ceremony at the University of California, Berkeley.[8]The name Wayback Machine was chosen as a reference to the WABAC machine (pronounced way-back), a time-traveling device used by the characters Mr. Peabody and Sherman in The Rocky and Bullwinkle Show, an animated cartoon.[9][10] In one of the animated cartoon's component segments, Peabody's Improbable History, the characters routinely used the machine to witness, participate in, and, more often than not, alter famous events in history.Technical details[edit]Software has been developed to crawl the web and download all publicly accessible World Wide Web pages, the Gopher hierarchy, the Netnews (Usenet) bulletin board system, and downloadable software.[11] The information collected by these crawlers does not include all the information available on the Internet, since much of the data is restricted by the publisher or stored in databases that are not accessible. To overcome inconsistencies in partially cached websites, Archive-It.org was developed in 2005 by the Internet Archive as a means of allowing institutions and content creators to voluntarily harvest and preserve collections of digital content, and create digital archives.[12]Crawls are contributed from various sources, some imported from third parties and others generated internally by the Archive.[6] For example, crawls are contributed by the Sloan Foundation and Alexa, crawls run by IA on behalf of NARA and the Internet Memory Foundation, mirrors of Common Crawl.[6] The Worldwide Web Crawls have been running since 2010 and capture the global Web.[13][6]The frequency of snapshot captures varies per website.[6] Websites in the Worldwide Web Crawls are included in a crawl list, with the site archived once per crawl.[6] A crawl can take months or even years to complete depending on size.[6] For example, Wide Crawl Number 13 started on January 9, 2015, and completed on July 11, 2016.[14] However, there may be multiple crawls ongoing at any one time, and a site might be included in more than one crawl list, so how often a site is crawled varies widely.[6]Storage capabilities[edit]As of 2009[update], the Wayback Machine contained approximately three petabytes of data and was growing at a rate of 100 terabytes each month;[15] the growth rate reported in 2003 was 12 terabytes/month. The data is stored on PetaBox rack systems manufactured by Capricorn Technologies.[16]In 2009, the Internet Archive migrated its customized storage architecture to Sun Open Storage, and hosts a new data center in a Sun Modular Datacenter on Sun Microsystems' California campus.[17]In 2011 a new, improved version of the Wayback Machine, with an updated interface and fresher index of archived content, was made available for public testing.[18]In March 2011, it was said on the Wayback Machine forum that, the Beta of the new Wayback Machine has a more complete and up-to-date index of all crawled materials into 2010, and will continue to be updated regularly. The index driving the classic Wayback Machine only has a little bit of material past 2008, and no further index updates are planned, as it will be phased out this year.[19]In January 2013, the company announced a ground-breaking milestone of 240 billion URLs.[20]In October 2013, the company announced the Save a Page feature[21] which allows any Internet user to archive the contents of a URL. This became a threat of abuse by the service for hosting malicious binaries.[22][23]As of December 2014[update], the Wayback Machine contained almost nine petabytes of data and was growing at a rate of about 20 terabytes each week.[24]As of July 2016[update], the Wayback Machine reportedly contained around 15 petabytes of data.[25]Growth[edit]Between October 2013 and March 2015 the website's global Alexa rank changed from 162[26] to 208.[27]Website exclusion policy[edit]Historically, Wayback Machine respected the robots exclusion standard (robots.txt) in determining if a website would be crawled or not; or if already crawled, if its archives would be publicly viewable. Website owners had the option to opt-out of Wayback Machine through the use of robots.txt. It applied robots.txt rules retroactively; if a site blocked the Internet Archive, any previously archived pages from the domain were immediately rendered unavailable as well. In addition the Internet Archive stated, Sometimes a website owner will contact us directly and ask us to stop crawling or archiving a site. We comply with these requests.[39] In addition, the website says: The Internet Archive is not interested in preserving or offering access to Web sites or other Internet documents of persons who do not want their materials in the collection.[40]Oakland Archive Policy[edit]Wayback's retroactive exclusion policy is based in part upon Recommendations for Managing Removal Requests and Preserving Archival Integrity published by the School of Information Management and Systems at University of California, Berkeley in 2002, which gives a website owner the right to block access to the site's archives. [41] Wayback has complied with this policy to help avoid expensive litigation.[42]The Wayback retroactive exclusion policy began to relax in 2017, when it stopped honoring robots.txt on U.S. government and military web sites for both crawling and displaying web pages. As of April 2017, Wayback is exploring ignoring robots.txt more broadly, not just for U.S. government websites.[43][44][45][46]Uses[edit]The site is frequently used by journalists and citizens to review dead websites, dated news reports or changes to website contents. Its content has been used to hold politicians accountable and expose battlefield lies.[47]In 2014 an archived social media page of separatist rebel leader in Ukraine Igor Girkin showed him boasting about his troops having shot down a suspected Ukrainian military airplane before it became known that the plane actually was a civilian Malaysian Airlines jet after which he deleted the post and blamed Ukraine's military.[47][48]In 2017 the March for Science originated from a discussion on reddit that indicated someone had visited Archive.org and discovered that all references to climate change had been deleted from the White House website. In response, a user commented, There needs to be a Scientists' March on Washington.[49][50][51]Furthermore, the site is used heavily for verification, providing access to references and content creation by Wikipedia editors.[citation needed]In legal evidence[edit]Civil litigation[edit]Netbula LLC v. Chordiant Software Inc.[edit]In a 2009 case, Netbula, LLC v. Chordiant Software Inc., defendant Chordiant filed a motion to compel Netbula to disable the robots.txt file on its website that was causing the Wayback Machine to retroactively remove access to previous versions of pages it had archived from Netbula's site, pages that Chordiant believed would support its case.[52]Netbula objected to the motion on the ground that defendants were asking to alter Netbula's website and that they should have subpoenaed Internet Archive for the pages directly.[53] An employee of Internet Archive filed a sworn statement supporting Chordiant's motion, however, stating that it could not produce the web pages by any other means without considerable burden, expense and disruption to its operations.[52]Magistrate Judge Howard Lloyd in the Northern District of California, San Jose Division, rejected Netbula's arguments and ordered them to disable the robots.txt blockage temporarily in order to allow Chordiant to retrieve the archived pages that they sought.[52]Telewizja Polska[edit]In an October 2004 case, Telewizja Polska USA, Inc. v. Echostar Satellite, No. 02 C 3293, 65 Fed. R. Evid. Serv. 673 (N.D. Ill. Oct. 15, 2004), a litigant attempted to use the Wayback Machine archives as a source of admissible evidence, perhaps for the first time. Telewizja Polska is the provider of TVP Polonia and EchoStar operates the Dish Network. Prior to the trial proceedings, EchoStar indicated that it intended to offer Wayback Machine snapshots as proof of the past content of Telewizja Polska's website. Telewizja Polska brought a motion in limine to suppress the snapshots on the grounds of hearsay and unauthenticated source, but Magistrate Judge Arlander Keys rejected Telewizja Polska's assertion of hearsay and denied TVP's motion in limine to exclude the evidence at trial.[54][55] At the trial, however, district Court Judge Ronald Guzman, the trial judge, overruled Magistrate Keys' findings,[citation needed] and held that neither the affidavit of the Internet Archive employee nor the underlying pages (i.e., the Telewizja Polska website) were admissible as evidence. Judge Guzman reasoned that the employee's affidavit contained both hearsay and inconclusive supporting statements, and the purported web page printouts were not self-authenticating.[citation needed]Patent law[edit]Provided some additional requirements are met (e.g., providing an authoritative statement of the archivist), the United States patent office and the European Patent Office will accept date stamps from the Internet Archive as evidence of when a given Web page was accessible to the public. These dates are used to determine if a Web page is available as prior art for instance in examining a patent application.[56]Limitations of utility[edit]There are technical limitations to archiving a website, and as a consequence, it is possible for opposing parties in litigation to misuse the results provided by website archives. This problem can be exacerbated by the practice of submitting screen shots of web pages in complaints, answers, or expert witness reports, when the underlying links are not exposed and therefore, can contain errors. For example, archives such as the Wayback Machine do not fill out forms and therefore, do not include the contents of non-RESTful e-commerce databases in their archives.[57]Legal status[edit]In Europe the Wayback Machine could be interpreted as violating copyright laws. Only the content creator can decide where their content is published or duplicated, so the Archive would have to delete pages from its system upon request of the creator.[58] The exclusion policies for the Wayback Machine may be found in the FAQ section of the site.[59]Archived content legal issues[edit]A number of cases have been brought against the Internet Archive specifically for its Wayback Machine archiving efforts.Scientology[edit]In late 2002, the Internet Archive removed various sites that were critical of Scientology from the Wayback Machine.[60] An error message stated that this was in response to a request by the site owner.[61] Later, it was clarified that lawyers from the Church of Scientology had demanded the removal and that the site owners did not want their material removed.[62]Healthcare Advocates, Inc.[edit]In 2003, Harding Earley Follmer & Frailey defended a client from a trademark dispute using the Archive's Wayback Machine. The attorneys were able to demonstrate that the claims made by the plaintiff were invalid, based on the content of their website from several years prior. The plaintiff, Healthcare Advocates, then amended their complaint to include the Internet Archive, accusing the organization of copyright infringement as well as violations of the DMCA and the Computer Fraud and Abuse Act. Healthcare Advocates claimed that, since they had installed a robots.txt file on their website, even if after the initial lawsuit was filed, the Archive should have removed all previous copies of the plaintiff website from the Wayback Machine, however, some material continued to be publicly visible on Wayback.[63] The lawsuit was settled out of court, after Wayback fixed the problem.[64]Suzanne Shell[edit]In December 2005, activist Suzanne Shell filed suit demanding Internet Archive pay her US $100,000 for archiving her website profane-justice.org between 1999 and 2004.[65][66] Internet Archive filed a declaratory judgment action in the United States District Court for the Northern District of California on January 20, 2006, seeking a judicial determination that Internet Archive did not violate Shell's copyright. Shell responded and brought a countersuit against Internet Archive for archiving her site, which she alleges is in violation of her terms of service.[67] On February 13, 2007, a judge for the United States District Court for the District of Colorado dismissed all counterclaims except breach of contract.[66] The Internet Archive did not move to dismiss copyright infringement claims Shell asserted arising out of its copying activities, which would also go forward.[68]On April 25, 2007, Internet Archive and Suzanne Shell jointly announced the settlement of their lawsuit.[65] The Internet Archive said it ...has no interest in including materials in the Wayback Machine of persons who do not wish to have their Web content archived. We recognize that Ms. Shell has a valid and enforceable copyright in her Web site and we regret that the inclusion of her Web site in the Wayback Machine resulted in this litigation. Shell said, I respect the historical value of Internet Archive's goal. I never intended to interfere with that goal nor cause it any harm.[69]Daniel Davydiuk[edit]In 2013\u20132016, a pornographic actor tried to remove archived images of himself from the WayBack Machine's archive, first by sending multiple DMCA requests to the archive, and then by appealing to the Federal Court of Canada.[70][71][72]Censorship and other threats[edit]Archive.org is currently blocked in China.[73][74] After the site enabled the encrypted HTTPS protocol, the Internet Archive was blocked in its entirety in Russia in 2015.[75][76][47][needs update?]Alison Macrina, director of the Library Freedom Project, notes that while librarians deeply value individual privacy, we also strongly oppose censorship.[47]There are known rare cases where online access to content which for nothing has put people in danger was disabled.[47]Other threats include natural disasters,[77] destruction (remote or physical),[citation needed] manipulation of the archive's contents (see also: cyberattack, backup), problematic copyright laws[78] and surveillance of the site's users.[79]Kevin Vaughan suspects that in the long-term of multiple generations next to nothing will survive in a useful way besides if we have continuity in our technological civilization by which a lot of the bare data will remain findable and searchable.[80]Some find the Internet Archive, which describes itself to be built for the long-term,[81] to be working furiously to capture data before it disappears without any long-term infrastructure to speak of.[82]See also[edit]\nCollective memory\n\nNational memory\n\n\nDeep web\nHeritrix\nLibrary Genesis\nThe Memory Hole\nWeb archiving\nWebCite\nReferences[edit]External links[edit]\nOfficial website\nOfficial mirror of the Wayback Machine at the Bibliotheca Alexandrina\nTool to retrieve a backup from the Wayback Machine\nWayback Machine Downloader Online\n", "subtitles": ["History", "Technical details", "Uses", "Legal status", "Archived content legal issues", "Censorship and other threats", "See also", "References", "External links"], "title": "Wayback Machine"},
{"content": "Richard Carl Jeffrey (August 5, 1926 \u2013 November 9, 2002) was an American philosopher, logician, and probability theorist. He is best known for developing and championing the philosophy of radical probabilism and the associated heuristic of probability kinematics, also known as Jeffrey conditioning.Life and work[edit]Born in Boston, Massachusetts, Jeffrey served in the U.S. Navy during World War II. As a graduate student he studied under Rudolf Carnap, and Carl Hempel.[1] He received his M.A. from the University of Chicago in 1952 and his Ph.D. from Princeton in 1957. After holding academic positions at MIT, City College of New York, Stanford University, and the University of Pennsylvania, he joined the faculty of Princeton in 1974 and became a professor emeritus there in 1999. He was also a visiting professor at the University of California, Irvine.[2]As a philosopher, Jeffrey specialized in epistemology and decision theory. He is perhaps best known for defending and developing the Bayesian approach to probability.Jeffrey also wrote, or co-wrote, two widely used and influential logic textbooks: Formal Logic: Its Scope and Limits, a basic introduction to logic, and Computability and Logic, a more advanced text dealing with, among other things, the famous negative results of twentieth century logic such as Go\u0308del's incompleteness theorems and Tarski's indefinability theorem.Jeffrey, who died of lung cancer at the age of 76, was known for his sense of humor, which often came through in his breezy writing style. In the preface of his posthumously published Subjective Probability, he refers to himself as a fond foolish old fart dying of a surfeit of Pall Malls.[3]Radical probabilism[edit]In frequentist statistics, Bayes' theorem provides a useful rule for updating a probability when new frequency data becomes available. In Bayesian statistics, the theorem itself plays a more limited role. Bayes' theorem connects probabilities that are held simultaneously. It does not tell the learner how to update probabilities when new evidence becomes available over time. This subtlety was first pointed out in terms by Ian Hacking in 1967.[4]However, adapting Bayes' theorem, and adopting it as a rule of updating, is a temptation. Suppose that a learner forms probabilities Pold(A&B)=p and Pold(B)=q. If the learner subsequently learns that B is true, nothing in the axioms or probability or the results derived therefrom tells him how to behave. He might be tempted to adopt Bayes' theorem by analogy and set his Pnew(A) = Pold(A | B) = p/q.In fact, that step, Bayes' rule of updating, can be justified, as necessary and sufficient, through a dynamic Dutch book argument that is additional to the arguments used to justify the axioms. This argument was first put forward by David Lewis in the 1970s though he never published it.[5]That works when the new data is certain. C. I. Lewis had argued that If anything is to be probable then something must be certain.[6] There must, on Lewis' account, be some certain facts on which probabilities were conditioned. However, the principle known as Cromwell's rule declares that nothing, apart form a logical law, can ever be certain, if that. Jeffrey famously rejected Lewis' dictum and quipped, It's probabilities all the way down. He called this position radical probabilism.In this case Bayes' rule isn't able to capture a mere subjective change in the probability of some critical fact. The new evidence may not have been anticipated or even be capable of being articulated after the event. It seems reasonable, as a starting position, to adopt the law of total probability and extend it to updating in much the same way as was Bayes' theorem.[7]\nPnew(A) = Pold(A | B)Pnew(B) + Pold(A | not-B)Pnew(not-B)\nAdopting such a rule is sufficient to avoid a Dutch book but not necessary.[8]. Jeffrey advocated this as a rule of updating under radical probabilism and called it probability kinematics. Others have named it Jeffrey conditioning.It is not the only sufficient updating rule for radical probabilism. Others have been advocated including E. T. Jaynes' maximum entropy principle and Brian Skyrms' principle of reflection.Jeffrey conditioning can be generalized from partitions to arbitrary condition events by giving it a frequentist semantics. [9]Selected bibliography[edit]\nFormal Logic: Its Scope and Limits. 3rd ed. McGraw Hill, 1990. ISBN 0-07-032357-7\nFormal Logic: Its Scope and Limits. 4th ed., John P. Burgess (editor), Hackett Publishing, 2006, ISBN 0-87220-813-3; ISBN 978-0-87220-813-1\nThe Logic of Decision. 2nd ed. University of Chicago Press, 1990. ISBN 0-226-39582-0\nProbability and the Art of Judgment. Cambridge University Press, 1992. ISBN 0-521-39770-7\nComputability and Logic (with George Boolos and John P. Burgess). 4th ed. Cambridge University Press, 2002. ISBN 0-521-00758-5\nSubjective Probability: The Real Thing. Cambridge University Press, 2004. ISBN 0-521-53668-5\nReferences[edit]External links[edit]\nHis website at Princeton; includes several manuscripts, including Subjective Probability\nBibliography\nCurriculum Vitae\nPage devoted to the memory of Richard Jeffrey\nStanford Encyclopedia of Philosophy entry on Bayes' Theorem (discusses Jeffrey conditioning)\nTribute, by Brian Skyrms\n[1]\nRichard C. Jeffrey Papers [2](Richard C. Jeffrey Papers, 1934-2002, ASP.2003.02, Archives of Scientific Philosophy, Special Collections Department, University of Pittsburgh)\n", "subtitles": ["Life and work", "Radical probabilism", "Selected bibliography", "References", "External links"], "title": "Richard Jeffrey"},
{"content": "IDEF, initially abbreviation of ICAM Definition, renamed in 1999 as Integration DEFinition,[2] refers to a family of modeling languages in the field of systems and software engineering. They cover a wide range of uses, from functional modeling to data, simulation, object-oriented analysis/design and knowledge acquisition. These definition languages were developed under funding from U.S. Air Force and although still most commonly used by them, as well as other military and United States Department of Defense (DoD) agencies, are in the public domain.The most-widely recognized and used components of the IDEF family are IDEF0, a functional modeling language building on SADT, and IDEF1X, which addresses information models and database design issues.Overview of IDEF methods[edit]IDEF refers to a family of modeling language, which cover a wide range of uses, from functional modeling to data, simulation, object-oriented analysis/design and knowledge acquisition. Eventually the IDEF methods have been defined up to IDEF14:\nIDEF0 : Function modeling[3]\nIDEF1 : Information modeling [4]\nIDEF1X : Data modeling [5]\nIDEF2 : Simulation model design\nIDEF3 : Process description capture [6]\nIDEF4 : Object-oriented design [7]\nIDEF5 : Ontology description capture [8]\nIDEF6 : Design rationale capture [9]\nIDEF7 : Information system auditing\nIDEF8 : User interface modeling\nIDEF9 : Business constraint discovery\nIDEF10 : Implementation architecture modeling\nIDEF11 : Information artifact modeling\nIDEF12 : Organization modeling\nIDEF13 : Three schema mapping design\nIDEF14 : Network design\nIn 1995 only the IDEF0, IDEF1X, IDEF2, IDEF3 and IDEF4 had been developed in full.[10] Some of the other IDEF concepts had some preliminary design. Some of the last efforts were new IDEF developments in 1995 toward establishing reliable methods for business constraint discovery IDEF9, design rationale capture IDEF6, human system, interaction design IDEF8, and network design IDEF14.[1]The methods IDEF7, IDEF10, IDEF11, IDEF 12 and IDEF13 haven't been developed any further than their initial definition.[11]History[edit]IDEF originally stood for ICAM Definition, initiated in the 1970s at the US Air Force Materials Laboratory, Wright-Patterson Air Force Base in Ohio by Dennis E. Wisnosky, Dan L. Shunk and others.[12] and completed in the 1980s. IDEF was a product of the Integrated Computer-Aided Manufacturing (ICAM) initiative of the United States Air Force. The IEEE recast the IDEF abbreviation as Integration DEFinition.[2]The specific projects that produced IDEF were ICAM project priorities 111 and 112 (later renumbered 1102). The subsequent Integrated Information Support System (IISS) project priorities 6201, 6202, and 6203 attempted to create an information processing environment that could be run in heterogeneous physical computing environments. Further development of IDEF occurred under those projects as a result of the experience gained from applications of the new modeling techniques. The intent of the IISS efforts was to create 'generic subsystems' that could be used by a large number of collaborating enterprises, such as U.S. defense contractors and the armed forces of friendly nations.At the time of the ICAM 1102 effort there were numerous, mostly incompatible, data model methods for storing computer data \u2014 sequential (VSAM), hierarchical (IMS), network (Cincom's TOTAL and CODASYL, and Cullinet's IDMS). The relational data model was just emerging as a promising way of thinking about structuring data for easy, efficient, and accurate access. Relational database management systems had not yet emerged as a general standard for data management.The ICAM program office deemed it valuable to create a neutral way of describing the data content of large-scale systems. The emerging academic literature suggested that methods were needed to process data independently of the way it was physically stored. Thus the IDEF1 language was created to allow a neutral description of data structures that could be applied regardless of the storage method or file access method.IDEF1 was developed under ICAM program priority 1102 by Dr Robert R. Brown of the Hughes Aircraft Company, under contract to SofTech, Inc. Dr Brown had previously been responsible for the development of IMS while working at Rockwell International. Rockwell chose not to pursue IMS as a marketable product but IBM, which had served as a support contractor during development, subsequently took over the product and was successful in further developing it for market. Dr Brown credits his Hughes' colleague Mr Timothy Ramey as the inventor of IDEF1 as a viable formalism for modeling information structures. The two Hughes' researchers built on ideas from and interactions with many luminaries in the field at the time. In particular, IDEF1 draws on the following techniques:\nthe evolving natural language information model (ENALIM) technique of Dr G. M. Nijssen (Control Data Corporation) \u2014 this technique is now more widely known as NIAM or the object-role model ORM;\nthe network data structures technique, popularly called the CODASYL approach, of Dr Charles Bachman (Honeywell Information Systems);\nthe hierarchical data management technique, implemented in IBM's IMS data management system, developed by Dr R. R. Brown (Rockwell International);\nthe relational approach to data of Dr E. F. Codd (IBM);\nThe entity-relationship approach (E-R) of Dr Peter Chen (UCLA).\nThe effort to develop IDEF1 resulted in both a new method for information modeling and an example of its use in the form of a reference information model of manufacturing. This latter artifact was developed by D. S. Coleman of the D. Appleton Company (DACOM) acting as a sub-contractor to Hughes and under the direction of Mr Ramey. Personnel at DACOM became quite expert at IDEF1 modeling and subsequently produced a training course and accompanying materials for the IDEF1 modeling technique.Experience with IDEF1 revealed that the translation of information requirements into database designs was more difficult than had originally been anticipated. The most beneficial value of the IDEF1 information modeling technique was its ability to represent data independent of how those data were to be stored and used. It provided data modelers and data analysts with a way to represent data requirements during the requirements-gathering process. This allowed designers to decide which DBMS to use after the nature of the data requirements was understood and thus reduced the misfit between data requirements and the capabilities and limitations of the DBMS. The translation of IDEF1 models to database designs, however, proved to be difficult.The IDEF modeling languages[edit]IDEF0[edit]The IDEF0 functional modeling method is designed to model the decisions, actions, and activities of an organization or system.[13] It was derived from the established graphic modeling language structured analysis and design technique (SADT) developed by Douglas T. Ross and SofTech, Inc.. In its original form, IDEF0 includes both a definition of a graphical modeling language (syntax and semantics) and a description of a comprehensive methodology for developing models.[14] The US Air Force commissioned the SADT developers to develop a function model method for analyzing and communicating the functional perspective of a system. IDEF0 should assist in organizing system analysis and promote effective communication between the analyst and the customer through simplified graphical devices.[13]IDEF1X[edit]To satisfy the data modeling enhancement requirements that were identified in the IISS-6202 project, a sub-contractor, DACOM, obtained a license to the logical database design technique (LDDT) and its supporting software (ADAM). LDDT had been developed in 1982 by Robert G. Brown of The Database Design Group entirely outside the IDEF program and with no knowledge of IDEF1. LDDT combined elements of the relational data model, the E-R model, and generalization in a way specifically intended to support data modeling and the transformation of the data models into database designs. The graphic syntax of LDDT differed from that of IDEF1 and, more importantly, LDDT contained interrelated modeling concepts not present in IDEF1. Mary E. Loomis wrote a concise summary of the syntax and semantics of a substantial subset of LDDT, using terminology compatible with IDEF1 wherever possible. DACOM labeled the result IDEF1X and supplied it to the ICAM program.[15][16]Because the IDEF program was funded by the government, the techniques are in the public domain. In addition to the ADAM software, sold by DACOM under the name Leverage, a number of CASE tools use IDEF1X as their representation technique for data modeling.The IISS projects actually produced working prototypes of an information processing environment that would run in heterogeneous computing environments. Current advancements in such techniques as Java and JDBC are now achieving the goals of ubiquity and versatility across computing environments which was first demonstrated by IISS.IDEF2 and IDEF3[edit]The third IDEF (IDEF2) was originally intended as a user interface modeling method. However, since the Integrated Computer-Aided Manufacturing (ICAM) program needed a simulation modeling tool, the resulting IDEF2 was a method for representing the time varying behavior of resources in a manufacturing system, providing a framework for specification of math model based simulations. It was the intent of the methodology program within ICAM to rectify this situation but limitation of funding did not allow this to happen. As a result, the lack of a method which would support the structuring of descriptions of the user view of a system has been a major shortcoming of the IDEF system. The basic problem from a methodology point of view is the need to distinguish between a description of what a system (existing or proposed) is supposed to do and a representative simulation model that will predict what a system will do. The latter was the focus of IDEF2, the former is the focus of IDEF3.[17]IDEF4[edit]The development of IDEF4 came from the recognition that the modularity, maintainability and code reusability that results from the object-oriented programming paradigm can be realized in traditional data processing applications. The proven ability of the object-oriented programming paradigm to support data level integration in large complex distributed systems is also a major factor in the widespread interest in this technology from the traditional data processing community.[17]IDEF4 was developed as a design tool for software designers who use object-oriented languages such as the Common Lisp Object System, Flavors, Smalltalk, Objective-C, C++, and others. Since effective usage of the object-oriented paradigm requires a different thought process than used with conventional procedural or database languages, standard methodologies such as structure charts, data flow diagrams, and traditional data design models (hierarchical, relational, and network) are not sufficient. IDEF4 seeks to provide the necessary facilities to support the object-oriented design decision making process.[17]IDEF5[edit]IDEF5, or integrated definition for ontology description capture method, is a software engineering method to develop and maintain usable, accurate, domain ontologies.[18] In the field of computer science ontologies are used to capture the concept and objects in a specific domain, along with associated relationships and meanings. In addition, ontology capture helps coordinate projects by standardizing terminology and creates opportunities for information reuse. The IDEF5 Ontology Capture Method has been developed to reliably construct ontologies in a way that closely reflects human understanding of the specific domain.[18]In the IDEF5 method, an ontology is constructed by capturing the content of certain assertions about real-world objects, their properties and their interrelationships, and representing that content in an intuitive and natural form. The IDEF5 method has three main components: A graphical language to support conceptual ontology analysis, a structured text language for detailed ontology characterization, and a systematic procedure that provides guidelines for effective ontology capture.[19]IDEF6[edit]IDEF6, or integrated definition for design rationale capture, is a method to facilitate the acquisition, representation, and manipulation of the design rationale used in the development of enterprise systems. Rationale is the reason, justification, underlying motivation, or excuse that moved the designer to select a particular strategy or design feature. More simply, rationale is interpreted as the answer to the question, \u201cWhy is this design being done in this manner?\u201d Most design methods focus on what the design is (i.e. on the final product, rather than why the design is the way it is).[1]IDEF6 will be a method that possesses the conceptual resources and linguistic capabilities needed\nto represent the nature and structure of the information that constitutes design rationale within a given system, and\nto associate that rationale with design specifications, models, and documentation for the system.\nIDEF6 is applicable to all phases of the information system development process, from initial conceptualization through both preliminary and detailed design activities. To the extent that detailed design decisions for software systems are relegated to the coding phase, the IDEF6 technique should be usable during the software construction process as well.[9]IDEF8[edit]IDEF8, or integrated definition for human-system interaction design, is a method for producing high-quality designs of interactions between users and the systems they operate. Systems are characterized as a collection of objects that perform functions to accomplish a particular goal. The system with which the user interacts can be any system, not necessarily a computer program. Human-system interactions are designed at three levels of specification within the IDEF8 method. The first level defines the philosophy of system operation and produces a set of models and textual descriptions of overall system processes. The second level of design specifies role-centered scenarios of system use. The third level of IDEF8 design is for human-system design detailing. At this level of design, IDEF8 provides a library of metaphors to help users and designers specify the desired behavior in terms of other objects whose behavior is more familiar. Metaphors provide a model of abstract concepts in terms of familiar, concrete objects and experiences.[1]IDEF9[edit]IDEF9, or integrated definition for business constraint discovery, is designed to assist in the discovery and analysis of constraints in a business system. A primary motivation driving the development of IDEF9 was an acknowledgment that the collection of constraints that forge an enterprise system is generally poorly defined. The knowledge of what constraints exist and how those constraints interact is incomplete, disjoint, distributed, and often completely unknown. This situation is not necessarily alarming. Just as living organisms do not need to be aware of the genetic or autonomous constraints that govern certain behaviors, organizations can (and most do) perform well without explicit knowledge of the glue that structures the system. In order to modify business in a predictable manner, however, the knowledge of these constraints is as critical as knowledge of genetics is to the genetic engineer.[1]IDEF14[edit]IDEF14, or integrated definition for network design method, is a method that targets the modeling and design of computer and communication networks. It can be used to model existing (as is) or envisioned (to be) networks. It helps the network designer to investigate potential network designs and to document design rationale. The fundamental goals of the IDEF14 research project developed from a perceived need for good network designs that can be implemented quickly and accurately.[1]See also[edit]\nData modeling\nEntity-relationship model\nModeling language\nRelational model\nStructured analysis and design technique\nReferences[edit] This article incorporates public domain material from the National Institute of Standards and Technology website http://www.nist.gov.Further reading[edit]\nOvidiu S. Noran (2000). Business Modelling: UML vs. IDEF Paper Griffith University\nExternal links[edit]\nIntegrated DEFinition Methods\nData Modeling\nThe IDEF Process Modeling Methodology by Robert P. Hanrahan 1995\n", "subtitles": ["Overview of IDEF methods", "History", "The IDEF modeling languages", "See also", "References", "Further reading", "External links"], "title": "IDEF"},
{"content": "This article describes the features in Haskell.Examples[edit]Factorial[edit]A simple example that is often used to demonstrate the syntax of functional languages is the factorial function for non-negative integers, shown in Haskell:Or in one line:This describes the factorial as a recursive function, with one terminating base case. It is similar to the descriptions of factorials found in mathematics textbooks. Much of Haskell code is similar to standard mathematical notation in facility and syntax.The first line of the factorial function describes the type of this function; while it is optional, it is considered to be good style[1] to include it. It can be read as the function factorial (factorial) has type (::) from integer to integer (Integer -> Integer). That is, it takes an integer as an argument, and returns another integer. The type of a definition is inferred automatically if the programmer didn't supply a type annotation.The second line relies on pattern matching, an important feature of Haskell. Note that parameters of a function are not in parentheses but separated by spaces. When the function's argument is 0 (zero) it will return the integer 1 (one). For all other cases the third line is tried. This is the recursion, and executes the function again until the base case is reached.Using the product function from the Prelude, a number of small functions analogous to C's standard library, and using the Haskell syntax for arithmetic sequences, the factorial function can be expressed in Haskell as follows:Here [1..n] denotes the arithmetic sequence 1, 2, ..., n in list form. Using the Prelude function enumFromTo, the expression [1..n] can be written as enumFromTo 1 n, allowing the factorial function to be expressed aswhich, using the function composition operator (expressed as a dot in Haskell) to compose the product function with the curried enumeration function can be rewritten in point-free style:[2]In the Hugs interpreter, one often needs to define the function and use it on the same line separated by a where or let..in. For example, to test the above examples and see the output 120:orThe GHCi interpreter doesn't have this restriction and function definitions can be entered on one line (with the let syntax without the in part), and referenced later.More complex examples[edit]Calculator[edit]In the Haskell source immediately below, :: can be read as has type; a \u2014> b can be read as is a function from a to b. (Thus the Haskell calc :: String \u2014> [Float] can be read as calc has type of function from Strings to lists of Floats.) In the second line calc = ...  the equals sign can be read as can be; thus multiple lines with calc = ...  can be read as multiple possible values for calc, depending on the circumstance detailed in each line.A simple Reverse Polish notation calculator expressed with the higher-order function foldl whose argument f is defined in a where clause using pattern matching and the type class Read:The empty list is the initial state, and f interprets one word at a time, either as a function name, taking two numbers from the head of the list and pushing the result back in, or parsing the word as a floating-point number and prepending it to the list.Fibonacci sequence[edit]The following definition produces the list of Fibonacci numbers in linear time:The infinite list is produced by corecursion \u2014 the latter values of the list are computed on demand starting from the initial two items 0 and 1. This kind of a definition relies on lazy evaluation, an important feature of Haskell programming. For an example of how the evaluation evolves, the following illustrates the values of fibs and tail fibs after the computation of six items and shows how zipWith (+) has produced four items and proceeds to produce the next item:\nfibs         = 0 : 1 : 1 : 2 : 3 : 5 : ...\n               +   +   +   +   +   +\ntail fibs    = 1 : 1 : 2 : 3 : 5 : ...\n               =   =   =   =   =   =\nzipWith ...  = 1 : 2 : 3 : 5 : 8 : ...\nfibs = 0 : 1 : 1 : 2 : 3 : 5 : 8 : ...\nThe same function, written using GHC's parallel list comprehension syntax (GHC extensions must be enabled using a special command-line flag, here -XParallelListComp, or by starting the source file with {-# LANGUAGE ParallelListComp #-}):or with regular list comprehensions:or directly self-referencing:With stateful generating function:or with unfoldr:or scanl:Using data recursion with Haskell's predefined fixpoint combinator:Factorial[edit]The factorial we saw previously can be written as a sequence of functions:More examples[edit]Hamming numbers[edit]A remarkably concise function that returns the list of Hamming numbers in order:Like the various fibs solutions displayed above, this uses corecursion to produce a list of numbers on demand, starting from the base case of 1 and building new items based on the preceding part of the list. Here the function union is used as an operator by enclosing it in back-quotes. Its case clauses define how it merges two ascending lists into one ascending list without duplicate items, representing sets as ordered lists. Its companion function minus implements set difference:It is possible to generate only the unique multiples, for more efficient operation. Since there are no duplicates, there's no need to remove them:This uses the more efficient function merge which doesn't concern itself with the duplicates (also used in the following next function, mergesort ):Each vertical bar ( | ) starts a guard clause with a guard expression before the = sign and the corresponding definition after it, that is evaluated if the guard is true.Mergesort[edit]Here is a bottom-up merge sort, defined using the higher-order function until:Prime numbers[edit]The mathematical definition of primes can be translated pretty much word for word into Haskell:This finds primes by trial division. Note that it is not optimized for efficiency and has very poor performance. Slightly faster (but still unreasonably slow[3]) is the famous code by David Turner:Much faster is the optimal trial division algorithmor an unbounded sieve of Eratosthenes with postponed sieving in stages,[4]or the combined sieve implementation by Richard Bird,[5]or an even faster tree-like folding variant[6] with nearly optimal (for a list-based code) time complexity and very low space complexity achieved through telescoping multistage recursive production of primes:Working on arrays by segments between consecutive squares of primes, it's (the fastest here)The shortest possible code is probably  nubBy (((>1).).gcd) [2..].  It is quite slow.Syntax[edit]Layout[edit]Haskell allows indentation to be used to indicate the beginning of a new declaration. For example, in a where clause:The two equations for the nested function prod are aligned vertically, which allows the semi-colon separator to be omitted. In Haskell, indentation can be used in several syntactic constructs, including do, let, case, class, and instance.The use of indentation to indicate program structure originates in Landin's ISWIM language, where it was called the off-side rule. This was later adopted by Miranda, and Haskell adopted a similar (but rather more complicated) version of Miranda's off-side rule, which is called layout. Other languages to adopt whitespace-sensitive syntax include Python and F#.The use of layout in Haskell is optional. For example, the function product above can also be written:The explicit open brace after the where keyword indicates that the programmer has opted to use explicit semi-colons to separate declarations, and that the declaration-list will be terminated by an explicit closing brace. One reason for wanting support for explicit delimiters is that it makes automatic generation of Haskell source code easier.Haskell's layout rule has been criticised for its complexity. In particular, the definition states that if the parser encounters a parse error during processing of a layout section, then it should try inserting a close brace (the parse error rule). Implementing this rule in a traditional parsing/lexical-analysis combination requires two-way cooperation between the parser and lexical analyser, whereas in most languages these two phases can be considered independently.Function calls[edit]Applying a function f to a value x is expressed as simply f x.Haskell distinguishes function calls from infix operators syntactically, but not semantically. Function names which are composed of punctuation characters can be used as operators, as can other function names if surrounded with backticks; and operators can be used in prefix notation if surrounded with parentheses.This example shows the ways that functions can be called:Functions which are defined as taking several parameters can always be partially applied. Binary operators can be partially applied using section notation:List comprehensions[edit]See List comprehension#Overview for the Haskell example.Pattern matching[edit]Pattern matching is used to match on the different constructors of algebraic data types. Here are some functions, each using pattern matching on each of the types above:Using the above functions, along with the map function, we can apply them to each element of a list, to see their results:\nAbstract Types\nLists\nTuples[edit]Tuples in haskell can be used to hold a fixed number of elements. They are used to group pieces of data of differing types:Tuples are commonly used in the zip* functions to place adjacent elements in separate lists together in tuples (zip4 to zip7 are provided in the Data.List module):In the GHC compiler, tuples are defined with sizes from 2 elements up to 62 elements.\nRecords\nNamespaces[edit]In the #More_complex_examples section above, calc is used in two senses, showing that there is a Haskell type class namespace and also a namespace for values:\na Haskell type class for calc. The domain and range can be explicitly denoted in a Haskell type class.\na Haskell value, formula, or expression for calc.\nTypeclasses and polymorphism[edit]Algebraic data types[edit]Algebraic data types are used extensively in Haskell. Some examples of these are the built in list, Maybe and Either types:Users of the language can also define their own abstract data types. An example of an ADT used to represent a person's name, sex and age might look like:Type system[edit]\nType classes\nType defaulting\nOverloaded Literals\nHigher Kinded Polymorphism\nMulti-Parameter Type Classes\nFunctional Dependencies\nMonads and input/output[edit]\nOverview of the monad framework\nApplications\n\nMonadic IO\nDo-notation\nReferences\nExceptions\n\n\nST monad[edit]The ST monad allows programmers to write imperative algorithms in Haskell, using mutable variables (STRef's) and mutable arrays (STArrays and STUArrays). The advantage of the ST monad is that it allows programmers to write code that has internal side effects, such as destructively updating mutable variables and arrays, while containing these effects inside the monad. The result of this is that functions written using the ST monad appear completely pure to the rest of the program. This allows programmers to produce imperative code where it may be impractical to write functional code, while still keeping all the safety that pure code provides.Here is an example program (taken from the Haskell wiki page on the ST monad) that takes a list of numbers, and sums them, using a mutable variable:STM monad[edit]The STM monad is an implementation of Software Transactional Memory in Haskell. It is implemented in the GHC compiler, and allows for mutable variables to be modified in transactions.Arrows[edit]\nApplicative Functors\nArrows\nAs Haskell is a pure functional language, functions cannot have side effects. Being non-strict, it also does not have a well-defined evaluation order. This is a challenge for real programs, which among other things need to interact with an environment. Haskell solves this with monadic types that leverage the type system to ensure the proper sequencing of imperative constructs. The typical example is I/O, but monads are useful for many other purposes, including mutable state, concurrency and transactional memory, exception handling, and error propagation.Haskell provides a special syntax for monadic expressions, so that side-effecting programs can be written in a style similar to current imperative programming languages; no knowledge of the mathematics behind monadic I/O is required for this. The following program reads a name from the command line and outputs a greeting message:The do-notation eases working with monads. This do-expression is equivalent to, but (arguably) easier to write and understand than, the de-sugared version employing the monadic operators directly:\nSee also wikibooks:Transwiki:List of hello world programs#Haskell for another example that prints text.\nConcurrency[edit]The Haskell language definition itself does not include either concurrency or parallelism, although GHC supports both.Concurrent Haskell is an extension to Haskell that provides support for threads and synchronization.[7] GHC's implementation of Concurrent Haskell is based on multiplexing lightweight Haskell threads onto a few heavyweight OS threads,[8] so that Concurrent Haskell programs run in parallel on a multiprocessor. The runtime can support millions of simultaneous threads.[9]The GHC implementation employs a dynamic pool of OS threads, allowing a Haskell thread to make a blocking system call without blocking other running Haskell threads.[10] Hence the lightweight Haskell threads have the characteristics of heavyweight OS threads, and the programmer is unaware of the implementation details.Recently, Concurrent Haskell has been extended with support for Software Transactional Memory (STM), which is a concurrency abstraction in which compound operations on shared data are performed atomically, as transactions.[11] GHC's STM implementation is the only STM implementation to date to provide a static compile-time guarantee preventing non-transactional operations from being performed within a transaction. The Haskell STM library also provides two operations not found in other STMs: retry and orElse, which together allow blocking operations to be defined in a modular and composable fashion.References[edit]", "subtitles": ["Examples", "Syntax", "Namespaces", "Typeclasses and polymorphism", "Monads and input/output", "Concurrency", "References"], "title": "Haskell features"},
{"content": "Systems engineering is an interdisciplinary field of engineering and engineering management that focuses on how to design and manage complex systems over their life cycles. At its core, systems engineering utilizes systems thinking principles to organize this body of knowledge. Issues such as requirements engineering, reliability, logistics, coordination of different teams, testing and evaluation, maintainability and many other disciplines necessary for successful system development, design, implementation, and ultimate decommission become more difficult when dealing with large or complex projects. Systems engineering deals with work-processes, optimization methods, and risk management tools in such projects. It overlaps technical and human-centered disciplines such as industrial engineering, mechanical engineering, manufacturing engineering, control engineering, software engineering, electrical engineering, cybernetics, organizational studies and project management. Systems engineering ensures that all likely aspects of a project or system are considered, and integrated into a whole.The systems engineering process is a discovery process that is quite unlike a manufacturing process. A manufacturing process is focused on repetitive activities that achieve high quality outputs with minimum cost and time. The systems engineering process must begin by discovering the real problems that need to be resolved, and identify the most probable or highest impact failures that can occur \u2013 systems engineering involves finding solutions to these problems.History[edit]The term systems engineering can be traced back to Bell Telephone Laboratories in the 1940s.[1] The need to identify and manipulate the properties of a system as a whole, which in complex engineering projects may greatly differ from the sum of the parts' properties, motivated various industries, especially those developing systems for the U.S. Military, to apply the discipline.[2]When it was no longer possible to rely on design evolution to improve upon a system and the existing tools were not sufficient to meet growing demands, new methods began to be developed that addressed the complexity directly.[3] The continuing evolution of systems engineering comprises the development and identification of new methods and modeling techniques. These methods aid in the better comprehension and the design and development control of engineering systems as they grow more complex. Popular tools that are often used in the systems engineering context were developed during these times, including USL, UML, QFD, and IDEF0.In 1990, a professional society for systems engineering, the National Council on Systems Engineering (NCOSE), was founded by representatives from a number of U.S. corporations and organizations. NCOSE was created to address the need for improvements in systems engineering practices and education. As a result of growing involvement from systems engineers outside of the U.S., the name of the organization was changed to the International Council on Systems Engineering (INCOSE) in 1995.[4] Schools in several countries offer graduate programs in systems engineering, and continuing education options are also available for practicing engineers.[5]Concept[edit]Systems engineering signifies only an approach and, more recently, a discipline in engineering. The aim of education in systems engineering is to formalize various approaches simply and in doing so, identify new methods and research opportunities similar to that which occurs in other fields of engineering. As an approach, systems engineering is holistic and interdisciplinary in flavour.Origins and traditional scope[edit]The traditional scope of engineering embraces the conception, design, development, production and operation of physical systems. Systems engineering, as originally conceived, falls within this scope. Systems engineering, in this sense of the term, refers to the distinctive set of concepts, methodologies, organizational structures (and so on) that have been developed to meet the challenges of engineering effective functional systems of unprecedented size and complexity within time, budget, and other constraints. The Apollo program is a leading example of a systems engineering project.Evolution to broader scope[edit]The use of the term systems engineer has evolved over time to embrace a wider, more holistic concept of systems and of engineering processes. This evolution of the definition has been a subject of ongoing controversy,[12] and the term continues to apply to both the narrower and broader scope.Traditional systems engineering was seen as a branch of engineering in the classical sense, that is, as applied only to physical system, such as space craft and aircraft. More recently, systems engineering has evolved to a take on a broader meaning especially when humans were seen as an essential component of a system. Checkland, for example, captures the broader meaning of systems engineering by stating that 'engineering' can be read in its general sense; you can engineer a meeting or a political agreement.[13]:10Consistent with the broader scope of systems engineering, the Systems Engineering Body of Knowledge (SEBoK)[14] has defined three types of systems engineering: (1) Product Systems Engineering (PSE) is the traditional systems engineering focused on the design of physical systems consisting of hardware and software. (2) Enterprise Systems Engineering (ESE) pertains to the view of enterprises, that is, organizations or combinations of organizations, as systems. (3) Service Systems Engineering (SSE) has to do with the engineering of service systems. Checkland[13] defines a service system as a system which is conceived as serving another system. Most civil infrastructure systems are service systems.Holistic view[edit]Systems engineering focuses on analyzing and eliciting customer needs and required functionality early in the development cycle, documenting requirements, then proceeding with design synthesis and system validation while considering the complete problem, the system lifecycle. This includes fully understanding all of the stakeholders involved. Oliver et al. claim that the systems engineering process can be decomposed into\na Systems Engineering Technical Process, and\na Systems Engineering Management Process.\nWithin Oliver's model, the goal of the Management Process is to organize the technical effort in the lifecycle, while the Technical Process includes assessing available information, defining effectiveness measures, to create a behavior model, create a structure model, perform trade-off analysis, and create sequential build & test plan.[15]Depending on their application, although there are several models that are used in the industry, all of them aim to identify the relation between the various stages mentioned above and incorporate feedback. Examples of such models include the Waterfall model and the VEE model.[16]Interdisciplinary field[edit]System development often requires contribution from diverse technical disciplines.[17] By providing a systems (holistic) view of the development effort, systems engineering helps mold all the technical contributors into a unified team effort, forming a structured development process that proceeds from concept to production to operation and, in some cases, to termination and disposal. In an acquisition, the holistic integrative discipline combines contributions and balances tradeoffs among cost, schedule, and performance while maintaining an acceptable level of risk covering the entire life cycle of the item.[18]This perspective is often replicated in educational programs, in that systems engineering courses are taught by faculty from other engineering departments, which helps create an interdisciplinary environment.[19][20]Managing complexity[edit]The need for systems engineering arose with the increase in complexity of systems and projects,[21][22] in turn exponentially increasing the possibility of component friction, and therefore the unreliability of the design. When speaking in this context, complexity incorporates not only engineering systems, but also the logical human organization of data. At the same time, a system can become more complex due to an increase in size as well as with an increase in the amount of data, variables, or the number of fields that are involved in the design. The International Space Station is an example of such a system.The development of smarter control algorithms, microprocessor design, and analysis of environmental systems also come within the purview of systems engineering. Systems engineering encourages the use of tools and methods to better comprehend and manage complexity in systems. Some examples of these tools can be seen here:[23]\nSystem architecture,\nSystem model, Modeling, and Simulation,\nOptimization,\nSystem dynamics,\nSystems analysis,\nStatistical analysis,\nReliability analysis, and\nDecision making\nTaking an interdisciplinary approach to engineering systems is inherently complex since the behavior of and interaction among system components is not always immediately well defined or understood. Defining and characterizing such systems and subsystems and the interactions among them is one of the goals of systems engineering. In doing so, the gap that exists between informal requirements from users, operators, marketing organizations, and technical specifications is successfully bridged.Scope[edit]One way to understand the motivation behind systems engineering is to see it as a method, or practice, to identify and improve common rules that exist within a wide variety of systems.[25] Keeping this in mind, the principles of systems engineering \u2013 holism, emergent behavior, boundary, et al. \u2013 can be applied to any system, complex or otherwise, provided systems thinking is employed at all levels.[26] Besides defense and aerospace, many information and technology based companies, software development firms, and industries in the field of electronics & communications require systems engineers as part of their team.[27]An analysis by the INCOSE Systems Engineering center of excellence (SECOE) indicates that optimal effort spent on systems engineering is about 15-20% of the total project effort.[28] At the same time, studies have shown that systems engineering essentially leads to reduction in costs among other benefits.[28] However, no quantitative survey at a larger scale encompassing a wide variety of industries has been conducted until recently. Such studies are underway to determine the effectiveness and quantify the benefits of systems engineering.[29][30]Systems engineering encourages the use of modeling and simulation to validate assumptions or theories on systems and the interactions within them.[31][32]Use of methods that allow early detection of possible failures, in safety engineering, are integrated into the design process. At the same time, decisions made at the beginning of a project whose consequences are not clearly understood can have enormous implications later in the life of a system, and it is the task of the modern systems engineer to explore these issues and make critical decisions. No method guarantees today's decisions will still be valid when a system goes into service years or decades after first conceived. However, there are techniques that support the process of systems engineering. Examples include soft systems methodology, Jay Wright Forrester's System dynamics method, and the Unified Modeling Language (UML)\u2014all currently being explored, evaluated, and developed to support the engineering decision process.Education[edit]Education in systems engineering is often seen as an extension to the regular engineering courses,[33] reflecting the industry attitude that engineering students need a foundational background in one of the traditional engineering disciplines (e.g., aerospace engineering, civil engineering, electrical engineering, mechanical engineering, manufacturing engineering, industrial engineering)\u2014plus practical, real-world experience to be effective as systems engineers. Undergraduate university programs in systems engineering are rare. Typically, systems engineering is offered at the graduate level in combination with interdisciplinary study.INCOSE maintains a continuously updated Directory of Systems Engineering Academic Programs worldwide.[5] As of 2009, there are about 80 institutions in United States that offer 165 undergraduate and graduate programs in systems engineering. Education in systems engineering can be taken as Systems-centric or Domain-centric.\nSystems-centric programs treat systems engineering as a separate discipline and most of the courses are taught focusing on systems engineering principles and practice.\nDomain-centric programs offer systems engineering as an option that can be exercised with another major field in engineering.\nBoth of these patterns strive to educate the systems engineer who is able to oversee interdisciplinary projects with the depth required of a core-engineer.[34]Systems engineering topics[edit]Systems engineering tools are strategies, procedures, and techniques that aid in performing systems engineering on a project or product. The purpose of these tools vary from database management, graphical browsing, simulation, and reasoning, to document production, neutral import/export and more.[35]System[edit]There are many definitions of what a system is in the field of systems engineering. Below are a few authoritative definitions:\nANSI/EIA-632-1999: An aggregation of end products and enabling products to achieve a given purpose.[36]\nDAU Systems Engineering Fundamentals: an integrated composite of people, products, and processes that provide a capability to satisfy a stated need or objective.\nIEEE Std 1220-1998: A set or arrangement of elements and processes that are related and whose behavior satisfies customer/operational needs and provides for life cycle sustainment of the products.[37]\nISO/IEC 15288:2008: A combination of interacting elements organized to achieve one or more stated purposes.[38]\nNASA Systems Engineering Handbook: (1) The combination of elements that function together to produce the capability to meet a need. The elements include all hardware, software, equipment, facilities, personnel, processes, and procedures needed for this purpose. (2) The end product (which performs operational functions) and enabling products (which provide life-cycle support services to the operational end products) that make up a system.[39]\nINCOSE Systems Engineering Handbook: homogeneous entity that exhibits predefined behavior in the real world and is composed of heterogeneous parts that do not individually exhibit that behavior and an integrated configuration of components and/or subsystems.[40]\nINCOSE: A system is a construct or collection of different elements that together produce results not obtainable by the elements alone. The elements, or parts, can include people, hardware, software, facilities, policies, and documents; that is, all things required to produce systems-level results. The results include system level qualities, properties, characteristics, functions, behavior and performance. The value added by the system as a whole, beyond that contributed independently by the parts, is primarily created by the relationship among the parts; that is, how they are interconnected.[41]\nThe systems engineering process[edit]Depending on their application, tools are used for various stages of the systems engineering process:[24]\n\n\n\n\n\nUsing models[edit]Models play important and diverse roles in systems engineering. A model can be defined in several ways, including:[42]\nAn abstraction of reality designed to answer specific questions about the real world\nAn imitation, analogue, or representation of a real world process or structure; or\nA conceptual, mathematical, or physical tool to assist a decision maker.\nTogether, these definitions are broad enough to encompass physical engineering models used in the verification of a system design, as well as schematic models like a functional flow block diagram and mathematical (i.e., quantitative) models used in the trade study process. This section focuses on the last.[42]The main reason for using mathematical models and diagrams in trade studies is to provide estimates of system effectiveness, performance or technical attributes, and cost from a set of known or estimable quantities. Typically, a collection of separate models is needed to provide all of these outcome variables. The heart of any mathematical model is a set of meaningful quantitative relationships among its inputs and outputs. These relationships can be as simple as adding up constituent quantities to obtain a total, or as complex as a set of differential equations describing the trajectory of a spacecraft in a gravitational field. Ideally, the relationships express causality, not just correlation.[42] Furthermore, key to successful systems engineering activities are also the methods with which these models are efficiently and effectively managed and used to simulate the systems. However, diverse domains often present recurring problems of modeling and simulation for systems engineering, and new advancements are aiming to crossfertilize methods among distinct scientific and engineering communities, under the title of 'Modeling & Simulation-based Systems Engineering'.[43]Modeling formalisms and graphical representations[edit]Initially, when the primary purpose of a systems engineer is to comprehend a complex problem, graphic representations of a system are used to communicate a system's functional and data requirements.[44] Common graphical representations include:\nFunctional flow block diagram (FFBD)\nModel-based design\nData Flow Diagram (DFD)\nN2 Chart\nIDEF0 Diagram\nUse case diagram\nSequence diagram\nBlock diagram\nSignal-flow graph\nUSL Function Maps and Type Maps.\nEnterprise Architecture frameworks\nModel-based systems engineering\nA graphical representation relates the various subsystems or parts of a system through functions, data, or interfaces. Any or each of the above methods are used in an industry based on its requirements. For instance, the N2 chart may be used where interfaces between systems is important. Part of the design phase is to create structural and behavioral models of the system.Once the requirements are understood, it is now the responsibility of a systems engineer to refine them, and to determine, along with other engineers, the best technology for a job. At this point starting with a trade study, systems engineering encourages the use of weighted choices to determine the best option. A decision matrix, or Pugh method, is one way (QFD is another) to make this choice while considering all criteria that are important. The trade study in turn informs the design, which again affects graphic representations of the system (without changing the requirements). In an SE process, this stage represents the iterative step that is carried out until a feasible solution is found. A decision matrix is often populated using techniques such as statistical analysis, reliability analysis, system dynamics (feedback control), and optimization methods.Other tools[edit]Systems Modeling Language (SysML), a modeling language used for systems engineering applications, supports the specification, analysis, design, verification and validation of a broad range of complex systems.[45]Lifecycle Modeling Language (LML), is an open-standard modeling language designed for systems engineering that supports the full lifecycle: conceptual, utilization, support and retirement stages.[46]Related fields and sub-fields[edit]Many related fields may be considered tightly coupled to systems engineering. These areas have contributed to the development of systems engineering as a distinct entity.\nCognitive Systems Engineering \nCognitive systems engineering (CSE) is a specific approach to the description and analysis of human-machine systems or sociotechnical systems.[47] The three main themes of CSE are how humans cope with complexity, how work is accomplished by the use of artifacts, and how human-machine systems and socio-technical systems can be described as joint cognitive systems. CSE has since its beginning become a recognized scientific discipline, sometimes also referred to as cognitive engineering. The concept of a Joint Cognitive System (JCS) has in particular become widely used as a way of understanding how complex socio-technical systems can be described with varying degrees of resolution. The more than 20 years of experience with CSE has been described extensively.[48][49]\nConfiguration management\nLike systems engineering, configuration management as practiced in the defense and aerospace industry is a broad systems-level practice. The field parallels the taskings of systems engineering; where systems engineering deals with requirements development, allocation to development items and verification, configuration management deals with requirements capture, traceability to the development item, and audit of development item to ensure that it has achieved the desired functionality that systems engineering and/or Test and Verification Engineering have proven out through objective testing.\nControl engineering\nControl engineering and its design and implementation of control systems, used extensively in nearly every industry, is a large sub-field of systems engineering. The cruise control on an automobile and the guidance system for a ballistic missile are two examples. Control systems theory is an active field of applied mathematics involving the investigation of solution spaces and the development of new methods for the analysis of the control process.\nIndustrial engineering\nIndustrial engineering is a branch of engineering that concerns the development, improvement, implementation and evaluation of integrated systems of people, money, knowledge, information, equipment, energy, material and process. Industrial engineering draws upon the principles and methods of engineering analysis and synthesis, as well as mathematical, physical and social sciences together with the principles and methods of engineering analysis and design to specify, predict, and evaluate results obtained from such systems.\nInterface design\nInterface design and its specification are concerned with assuring that the pieces of a system connect and inter-operate with other parts of the system and with external systems as necessary. Interface design also includes assuring that system interfaces be able to accept new features, including mechanical, electrical and logical interfaces, including reserved wires, plug-space, command codes and bits in communication protocols. This is known as extensibility. Human-Computer Interaction (HCI) or Human-Machine Interface (HMI) is another aspect of interface design, and is a critical aspect of modern systems engineering. Systems engineering principles are applied in the design of network protocols for local-area networks and wide-area networks.\nMechatronic engineering\nMechatronic engineering, like systems engineering, is a multidisciplinary field of engineering that uses dynamical systems modeling to express tangible constructs. In that regard it is almost indistinguishable from Systems Engineering, but what sets it apart is the focus on smaller details rather than larger generalizations and relationships. As such, both fields are distinguished by the scope of their projects rather than the methodology of their practice.\nOperations research\nOperations research supports systems engineering. The tools of operations research are used in systems analysis, decision making, and trade studies. Several schools teach SE courses within the operations research or industrial engineering department,[25] highlighting the role systems engineering plays in complex projects. Operations research, briefly, is concerned with the optimization of a process under multiple constraints.[50]\nPerformance engineering\nPerformance engineering is the discipline of ensuring a system meets customer expectations for performance throughout its life. Performance is usually defined as the speed with which a certain operation is executed, or the capability of executing a number of such operations in a unit of time. Performance may be degraded when an operations queue to execute is throttled by limited system capacity. For example, the performance of a packet-switched network is characterized by the end-to-end packet transit delay, or the number of packets switched in an hour. The design of high-performance systems uses analytical or simulation modeling, whereas the delivery of high-performance implementation involves thorough performance testing. Performance engineering relies heavily on statistics, queueing theory and probability theory for its tools and processes.\nProgram management and project management.\nProgram management (or programme management) has many similarities with systems engineering, but has broader-based origins than the engineering ones of systems engineering. Project management is also closely related to both program management and systems engineering.\nProposal engineering\nProposal engineering is the application of scientific and mathematical principles to design, construct, and operate a cost-effective proposal development system. Basically, proposal engineering uses the systems engineering process to create a cost effective proposal and increase the odds of a successful proposal.\nReliability engineering\nReliability engineering is the discipline of ensuring a system meets customer expectations for reliability throughout its life; i.e., it does not fail more frequently than expected. Next to prediction of failure, it is just as much about prevention of failure. Reliability engineering applies to all aspects of the system. It is closely associated with maintainability, availability (dependability or RAMS preferred by some), and logistics engineering. Reliability engineering is always a critical component of safety engineering, as in failure modes and effects analysis (FMEA) and hazard fault tree analysis, and of security engineering.\nRisk Management\nRisk Management, the practice of assessing and dealing with risk is one of the interdisciplinary parts of Systems Engineering. In development, acquisition, or operational activities, the inclusion of risk in tradeoff with cost, schedule, and performance features, involves the iterative complex configuration management of traceability and evaluation to the scheduling and requirements management across domains and for the system lifecycle that requires the interdisciplinary technical approach of systems engineering. Systems Engineering has Risk Management define, tailor, implement, and monitor a structured process for risk management which is integrated to the overall effort.[51]\nSafety engineering\nThe techniques of safety engineering may be applied by non-specialist engineers in designing complex systems to minimize the probability of safety-critical failures. The System Safety Engineering function helps to identify safety hazards in emerging designs, and may assist with techniques to mitigate the effects of (potentially) hazardous conditions that cannot be designed out of systems.\nScheduling\nScheduling is one of the systems engineering support tools as a practice and item in assessing interdisciplinary concerns under configuration management. In particular the direct relationship of resources, performance features, and risk to duration of a task or the dependency links among tasks and impacts across the system lifecycle are systems engineering concerns.\nSecurity engineering\nSecurity engineering can be viewed as an interdisciplinary field that integrates the community of practice for control systems design, reliability, safety and systems engineering. It may involve such sub-specialties as authentication of system users, system targets and others: people, objects and processes.\nSoftware engineering\nFrom its beginnings, software engineering has helped shape modern systems engineering practice. The techniques used in the handling of the complexities of large software-intensive systems have had a major effect on the shaping and reshaping of the tools, methods and processes of Software Engineering.\nSee also[edit]\nArcadia (engineering)\nControl engineering\nDesign review (U.S. government)\nEnterprise systems engineering\nIndustrial Engineering\nInterdisciplinarity\nList of production topics\nList of systems engineers\nList of types of systems engineering\nManagement cybernetics\nOperations management\nStructured systems analysis and design method\nSystem of systems engineering (SoSE)\nSystems Architecture\nSystems Thinking (e.g. Theory of Constraints, Value Stream Mapping)\nSystem information modelling\nReferences[edit]Further reading[edit]\nDennis M. Buede and William D. Miller, The Engineering Design of Systems: Models and Methods, Third Edition, John Wiley and Sons, 2016.\nHarold Chestnut, Systems Engineering Methods. Wiley, 1967.\nDaniele Gianni, Andrea D'Ambrogio, and Andreas Tolk (editors), Modeling and Simulation-Based Systems Engineering Handbook, CRC Press, 2014 at CRC\nHarry H. Goode, Robert E. Machol System Engineering: An Introduction to the Design of Large-scale Systems, McGraw-Hill, 1957.\nDerek Hitchins (1997) World Class Systems Engineering at hitchins.net.\nMalakooti, B. (2013). Operations and Production Systems with Multiple Objectives. John Wiley & Sons.ISBN 978-1-118-58537-5\nMITRE, The MITRE Systems Engineering Guide(pdf)\nNASA (2007) Systems Engineering Handbook, NASA/SP-2007-6105 Rev1, December 2007.\nNASA (2013) NASA Systems Engineering Processes and Requirements NPR 7123.1B, April 2013 NASA Procedural Requirements\nDavid W. Oliver, Timothy P. Kelliher & James G. Keegan, Jr. Engineering Complex Systems with Models and Objects. McGraw-Hill, 1997.\nSimon Ramo, Robin K. St.Clair, The Systems Approach: Fresh Solutions to Complex Problems Through Combining Science and Practical Common Sense, Anaheim, CA: KNI, Inc, 1998.\nAndrew P. Sage, Systems Engineering. Wiley IEEE, 1992. ISBN 0-471-53639-3.\nAndrew P. Sage, Stephen R. Olson, Modeling and Simulation in Systems Engineering, 2001.\nSEBOK.org, Systems Engineering Body of Knowledge (SEBoK)\nDale Shermon, Systems Cost Engineering, Gower publishing, 2009\nRobert Shishko et al. (2005) NASA Systems Engineering Handbook. NASA Center for AeroSpace Information, 2005.\nRichard Stevens, Peter Brook, Ken Jackson & Stuart Arnold. Systems Engineering: Coping with Complexity. Prentice Hall, 1998.\nUS Air Force, SMC Systems Engineering Primer & Handbook, 2004\nUS DoD Systems Management College (2001) Systems Engineering Fundamentals. Defense Acquisition University Press, 2001\nUS DoD Guide for Integrating Systems Engineering into DoD Acquisition Contracts, 2006\nUS DoD MIL-STD-499 System Engineering Management\nExternal links[edit]\nICSEng homepage.\nINCOSE homepage.\nINCOSE UK homepage\nPPI SE Goldmine homepage\nSystems Engineering Body of Knowledge\nSystems Engineering Tools List of systems engineering tools\nDoD Systems Engineering ODASD/SE site with guidance including reference materials\nNDIA Systems Engineering Division\n\nAssociations\n\nINCOSE\nInstitute of Industrial and Systems Engineers\n", "subtitles": ["History", "Concept", "Education", "Systems engineering topics", "Related fields and sub-fields", "See also", "References", "Further reading", "External links"], "title": "Systems engineering"},
{"content": "A view model or viewpoints framework in systems engineering, software engineering, and enterprise engineering is a framework which defines a coherent set of views to be used in the construction of a system architecture, software architecture, or enterprise architecture. A view is a representation of a whole system from the perspective of a related set of concerns.[1][2]Since the early 1990s there have been a number of efforts to prescribe approaches for describing and analyzing system architectures. These recent efforts define a set of views (or viewpoints). They are sometimes referred to as architecture frameworks or enterprise architecture frameworks, but are not usually called view models.Usually a view is a work product that presents specific architecture data for a given system. However, the same term is sometimes used to refer to a view definition, including the particular viewpoint and the corresponding guidance that defines each concrete view. The term view model is related to view definitions.Overview[edit]The purpose of views and viewpoints is to enable humans to comprehend very complex systems, to organize the elements of the problem and the solution around domains of expertise and to separate concerns. In the engineering of physically intensive systems, viewpoints often correspond to capabilities and responsibilities within the engineering organization.[3]Most complex system specifications are so extensive that no single individual can fully comprehend all aspects of the specifications. Furthermore, we all have different interests in a given system and different reasons for examining the system's specifications. A business executive will ask different questions of a system make-up than would a system implementer. The concept of viewpoints framework, therefore, is to provide separate viewpoints into the specification of a given complex system in order to facilitate communication with the stakeholders. Each viewpoint satisfies an audience with interest in a particular set of aspects of the system. Each viewpoint may use a specific viewpoint language that optimizes the vocabulary and presentation for the audience of that viewpoint. Viewpoint modeling has become an effective approach for dealing with the inherent complexity of large distributed systems.Architecture description practices, as described in IEEE Std 1471-2000, utilize multiple views to address several areas of concerns, each one focusing on a specific aspect of the system. Examples of architecture frameworks using multiple views include Kruchten's 4+1 view model, the Zachman Framework, TOGAF, DoDAF, RM-ODP, and Hamdaqa's 5+1 view model.[4]History[edit]In the 1970s, methods began to appear in software engineering for modeling with multiple views. Douglas T. Ross and K.E. Schoman in 1977 introduce the constructs context, viewpoint, and vantage point to organize the modeling process in systems requirements definition.[5] According to Ross and Schoman, a viewpoint makes clear what aspects are considered relevant to achieving ... the overall purpose [of the model] and determines How do we look at [a subject being modelled]?As examples of viewpoints, the paper offers: Technical, Operational and Economic viewpoints. In 1992, Anthony Finkelstein and others published a very important paper on viewpoints.[6] In that work: A viewpoint can be thought of as a combination of the idea of an \u201cactor\u201d, \u201cknowledge source\u201d, \u201crole\u201d or \u201cagent\u201d in the development process and the idea of a \u201cview\u201d or \u201cperspective\u201d which an actor maintains. An important idea in this paper was to distinguish a representation style, the scheme and notation by which the viewpoint expresses what it can see and a specification, the statements expressed in the viewpoint's style describing particular domains. Subsequent work, such as IEEE 1471, preserved this distinction by utilizing two separate terms: viewpoint and view, respectively.Since the early 1990s there have been a number of efforts to codify approaches for describing and analyzing system architectures. These are often terms architecture frameworks or sometimes viewpoint sets. Many of these have been funded by the United States Department of Defense, but some have sprung from international or national efforts in ISO or the IEEE. Among these, the IEEE Recommended Practice for Architectural Description of Software-Intensive Systems (IEEE Std 1471-2000) established useful definitions of view, viewpoint, stakeholder and concern and guidelines for documenting a system architecture through the use of multiple views by applying viewpoints to address stakeholder concerns.[7] The advantage of multiple views is that hidden requirements and stakeholder disagreements can be discovered more readily. However, studies show that in practice, the added complexity of reconciling multiple views can undermine this advantage.[8]IEEE 1471 (now ISO/IEC/IEEE 42010:2011, Systems and software engineering \u2014 Architecture description) prescribes the contents of architecture descriptions and describes their creation and use under a number of scenarios, including precedented and unprecedented design, evolutionary design, and capture of design of existing systems. In all of these scenarios the overall process is the same: identify stakeholders, elicit concerns, identify a set of viewpoints to be used, and then apply these viewpoint specifications to develop the set of views relevant to the system of interest. Rather than define a particular set of viewpoints, the standard provides uniform mechanisms and requirements for architects and organizations to define their own viewpoints. In 1996 the ISO Reference Model for Open Distributed Processing (RM-ODP) was published to provide a useful framework for describing the architecture and design of large-scale distributed systems.View model topics[edit]View[edit]A view of a system is a representation of the system from the perspective of a viewpoint. This viewpoint on a system involves a perspective focusing on specific concerns regarding the system, which suppresses details to provide a simplified model having only those elements related to the concerns of the viewpoint. For example, a security viewpoint focuses on security concerns and a security viewpoint model contains those elements that are related to security from a more general model of a system.[9]A view allows a user to examine a portion of a particular interest area. For example, an Information View may present all functions, organizations, technology, etc. that use a particular piece of information, while the Organizational View may present all functions, technology, and information of concern to a particular organization. In the Zachman Framework views comprise a group of work products whose development requires a particular analytical and technical expertise because they focus on either the \u201cwhat,\u201d \u201chow,\u201d \u201cwho,\u201d \u201cwhere,\u201d \u201cwhen,\u201d or \u201cwhy\u201d of the enterprise. For example, Functional View work products answer the question \u201chow is the mission carried out?\u201d They are most easily developed by experts in functional decomposition using process and activity modeling. They show the enterprise from the point of view of functions. They also may show organizational and information components, but only as they relate to functions.[10]Viewpoints[edit]In systems engineering, a viewpoint is a partitioning or restriction of concerns in a system. Adoption of a viewpoint is usable so that issues in those aspects can be addressed separately. A good selection of viewpoints also partitions the design of the system into specific areas of expertise.[3]Viewpoints provide the conventions, rules, and languages for constructing, presenting and analysing views. In ISO/IEC 42010:2007 (IEEE-Std-1471-2000) a viewpoint is a specification for an individual view. A view is a representation of a whole system from the perspective of a viewpoint. A view may consist of one or more architectural models.[11] Each such architectural model is developed using the methods established by its associated architectural system, as well as for the system as a whole. [7]Modeling perspectives[edit]Modeling perspectives is a set of different ways to represent pre-selected aspects of a system. Each perspective has a different focus, conceptualization, dedication and visualization of what the model is representing.In information systems, the traditional way to divide modeling perspectives is to distinguish the structural, functional and behavioral/processual perspectives. This together with rule, object, communication and actor and role perspectives is one way of classifying modeling approaches [12]Viewpoint model[edit]In any given viewpoint, it is possible to make a model of the system that contains only the objects that are visible from that viewpoint, but also captures all of the objects, relationships and constraints that are present in the system and relevant to that viewpoint. Such a model is said to be a viewpoint model, or a view of the system from that viewpoint.[3]A given view is a specification for the system at a particular level of abstraction from a given viewpoint. Different levels of abstraction contain different levels of detail. Higher-level views allow the engineer to fashion and comprehend the whole design and identify and resolve problems in the large. Lower-level views allow the engineer to concentrate on a part of the design and develop the detailed specifications.[3]In the system itself, however, all of the specifications appearing in the various viewpoint models must be addressed in the realized components of the system. And the specifications for any given component may be drawn from many different viewpoints. On the other hand, the specifications induced by the distribution of functions over specific components and component interactions will typically reflect a different partitioning of concerns than that reflected in the original viewpoints. Thus additional viewpoints, addressing the concerns of the individual components and the bottom-up synthesis of the system, may also be useful.[3]Architecture description[edit]An architecture description is a representation of a system architecture, at any time, in terms of its component parts, how those parts function, the rules and constraints under which those parts function, and how those parts relate to each other and to the environment. In an architecture description the architecture data is shared across several views and products.At the data layer are the architecture data elements and their defining attributes and relationships. At the presentation layer are the products and views that support a visual means to communicate and understand the purpose of the architecture, what it describes, and the various architectural analyses performed. Products provide a way for visualizing architecture data as graphical, tabular, or textual representations. Views provide the ability to visualize architecture data that stem across products, logically organizing the data for a specific or holistic perspective of the architecture.Types of system view models[edit]Three-schema approach[edit]The Three-schema approach for data modeling, introduced in 1977, can be considered one of the first view models. It is an approach to building information systems and systems information management, that promotes the conceptual model as the key to achieving data integration.[14] The Three schema approach defines three schemas and views:\nExternal schema for user views\nConceptual schema integrates external schemata\nInternal schema that defines physical storage structures\nAt the center, the conceptual schema defines the ontology of the concepts as the users think of them and talk about them. The physical schema describes the internal formats of the data stored in the database, and the external schema defines the view of the data presented to the application programs.[15] The framework attempted to permit multiple data models to be used for external schemata.[16]Over the years, the skill and interest in building information systems has grown tremendously. However, for the most part, the traditional approach to building systems has only focused on defining data from two distinct views, the user view and the computer view. From the user view, which will be referred to as the \u201cexternal schema,\u201d the definition of data is in the context of reports and screens designed to aid individuals in doing their specific jobs. The required structure of data from a usage view changes with the business environment and the individual preferences of the user. From the computer view, which will be referred to as the \u201cinternal schema,\u201d data is defined in terms of file structures for storage and retrieval. The required structure of data for computer storage depends upon the specific computer technology employed and the need for efficient processing of data.[17]4+1 view model of architecture[edit]4+1 is a view model designed by Philippe Kruchten in 1995 for describing the architecture of software-intensive systems, based on the use of multiple, concurrent views.[18] The views are used to describe the system in the viewpoint of different stakeholders, such as end-users, developers and project managers. The four views of the model are logical, development, process and physical view:The four views of the model are concerned with :\nLogical view: is concerned with the functionality that the system provides to end-users.\nDevelopment view: illustrates a system from a programmers perspective and is concerned with software management.\nProcess view: deals with the dynamic aspect of the system, explains the system processes and how they communicate, and focuses on the runtime behavior of the system.\nPhysical view: depicts the system from a system engineer's point of view. It is concerned with the topology of software components on the physical layer, as well as communication between these components.\nIn addition selected use cases or scenarios are utilized to illustrate the architecture. Hence the model contains 4+1 views.[18]Types of enterprise architecture views[edit]Enterprise architecture framework defines how to organize the structure and views associated with an enterprise architecture. Because the discipline of Enterprise Architecture and Engineering is so broad, and because enterprises can be large and complex, the models associated with the discipline also tend to be large and complex. To manage this scale and complexity, an Architecture Framework provides tools and methods that can bring the task into focus and allow valuable artifacts to be produced when they are most needed.Architecture Frameworks are commonly used in Information technology and Information system governance. An organization may wish to mandate that certain models be produced before a system design can be approved. Similarly, they may wish to specify certain views be used in the documentation of procured systems - the U.S. Department of Defense stipulates that specific DoDAF views be provided by equipment suppliers for capital project above a certain value.Zachman Framework[edit]The Zachman Framework, originally conceived by John Zachman at IBM in 1987, is a framework for enterprise architecture, which provides a formal and highly structured way of viewing and defining an enterprise.The Framework is used for organizing architectural artifacts in a way that takes into account both who the artifact targets (for example, business owner and builder) and what particular issue (for example, data and functionality) is being addressed. These artifacts may include design documents, specifications, and models.[20]The Zachman Framework is often referenced as a standard approach for expressing the basic elements of enterprise architecture. The Zachman Framework has been recognized by the U.S. Federal Government as having ... received worldwide acceptance as an integrated framework for managing change in enterprises and the systems that support them.[21]RM-ODP views[edit]The International Organization for Standardization (ISO) Reference Model for Open Distributed Processing (RM-ODP) [22] specifies a set of viewpoints for partitioning the design of a distributed software/hardware system. Since most integration problems arise in the design of such systems or in very analogous situations, these viewpoints may prove useful in separating integration concerns. The RMODP viewpoints are:[3]\nthe enterprise viewpoint, which is concerned with the purpose and behaviors of the system as it relates to the business objective and the business processes of the organization\nthe information viewpoint, which is concerned with the nature of the information handled by the system and constraints on the use and interpretation of that information\nthe computational viewpoint, which is concerned with the functional decomposition of the system into a set of components that exhibit specific behaviors and interact at interfaces\nthe engineering viewpoint, which is concerned with the mechanisms and functions required to support the interactions of the computational components\nthe technology viewpoint, which is concerned with the explicit choice of technologies for the implementation of the system, and particularly for the communications among the components\nRMODP further defines a requirement for a design to contain specifications of consistency between viewpoints, including:[3]\nthe use of enterprise objects and processes in defining information units\nthe use of enterprise objects and behaviors in specifying the behaviors of computational components, and use of the information units in defining computational interfaces\nthe association of engineering choices with computational interfaces and behavior requirements\nthe satisfaction of information, computational and engineering requirements in the chosen technologies\nDoDAF views[edit]The Department of Defense Architecture Framework (DoDAF) defines a standard way to organize an enterprise architecture (EA) or systems architecture into complementary and consistent views. It is especially suited to large systems with complex integration and interoperability challenges, and is apparently unique in its use of operational views detailing the external customer's operating domain in which the developing system will operate.The DoDAF defines a set of products that act as mechanisms for visualizing, understanding, and assimilating the broad scope and complexities of an architecture description through graphic, tabular, or textual means. These products are organized under four views:\nOverarching All View (AV),\nOperational View (OV),\nSystems View (SV), and the\nTechnical Standards View (TV).\nEach view depicts certain perspectives of an architecture as described below. Only a subset of the full DoDAF viewset is usually created for each system development. The figure represents the information that links the operational view, systems and services view, and technical standards view. The three views and their interrelationships driven \u2013 by common architecture data elements \u2013 provide the basis for deriving measures such as interoperability or performance, and for measuring the impact of the values of these metrics on operational mission and task effectiveness.[23]Federal Enterprise Architecture views[edit]In the US Federal Enterprise Architecture enterprise, segment, and solution architecture provide different business perspectives by varying the level of detail and addressing related but distinct concerns. Just as enterprises are themselves hierarchically organized, so are the different views provided by each type of architecture. The Federal Enterprise Architecture Practice Guidance (2006) has defined three types of architecture:[24]\nEnterprise architecture,\nSegment architecture, and\nSolution architecture.\nBy definition, Enterprise Architecture (EA) is fundamentally concerned with identifying common or shared assets \u2013 whether they are strategies, business processes, investments, data, systems, or technologies. EA is driven by strategy; it helps an agency identify whether its resources are properly aligned to the agency mission and strategic goals and objectives. From an investment perspective, EA is used to drive decisions about the IT investment portfolio as a whole. Consequently, the primary stakeholders of the EA are the senior managers and executives tasked with ensuring the agency fulfills its mission as effectively and efficiently as possible.[24]By contrast, segment architecture defines a simple roadmap for a core mission area, business service, or enterprise service. Segment architecture is driven by business management and delivers products that improve the delivery of services to citizens and agency staff. From an investment perspective, segment architecture drives decisions for a business case or group of business cases supporting a core mission area or common or shared service. The primary stakeholders for segment architecture are business owners and managers. Segment architecture is related to EA through three principles: structure, reuse, and alignment. First, segment architecture inherits the framework used by the EA, although it may be extended and specialized to meet the specific needs of a core mission area or common or shared service. Second, segment architecture reuses important assets defined at the enterprise level including: data; common business processes and investments; and applications and technologies. Third, segment architecture aligns with elements defined at the enterprise level, such as business strategies, mandates, standards, and performance measures.[24]Nominal set of views[edit]In search of Framework for Modeling Space Systems Architectures Peter Shames and Joseph Skipper (2006) defined a nominal set of views,[7] Derived from CCSDS RASDS, RM-ODP, ISO 10746 and compliant with IEEE 1471.This set of views, as described below, is a listing of possible modeling viewpoints. Not all of these views may be used for any one project and other views may be defined as necessary. Note that for some analyses elements from multiple viewpoints may be combined into a new view, possibly using a layered representation.In a latter presentation this nominal set of views was presented as an Extended RASDS Semantic Information Model Derivation.[25] Hereby RASDS stands for Reference Architecture for Space Data Systems. see second image.\nEnterprise Viewpoint[7]\n\nOrganization view \u2013 Includes organizational elements and their structures and relationships. May include agreements, contracts, policies and organizational interactions.\nRequirements view \u2013 Describes the requirements, goals, and objectives that drive the system. Says what the system must be able to do.\nScenario view \u2013 Describes the way that the system is intended to be used, see scenario planning. Includes user views and descriptions of how the system is expected to behave.\n\nInformation viewpoint[7]\n\nMetamodel view \u2013 An abstract view that defines information model elements and their structures and relationships. Defines the classes of data that are created and managed by the system and the data architecture.\nInformation view \u2013 Describes the actual data and information as it is realized and manipulated within the system. Data elements are defined by the metamodel view and they are referred to by functional objects in other views.\n\nFunctional viewpoint[7]\n\nFunctional Dataflow view \u2013 An abstract view that describes the functional elements in the system, their interactions, behavior, provided services, constraints and data flows among them. Defines which functions the system is capable of performing, regardless of how these functions are actually implemented.\nFunctional Control view \u2013 Describes the control flows and interactions among functional elements within the system. Includes overall system control interactions, interactions between control elements and sensor / effector elements and management interactions.\n\nPhysical viewpoint[7]\n\nData System view \u2013 Describes instruments, computers, and data storage components, their data system attributes and the communications connectors (busses, networks, point to point links) that are used in the system.\nTelecomm view \u2013 Describes the telecomm components (antenna, transceiver), their attributes and their connectors (RF or optical links).\nNavigation view \u2013 Describes the motion of the major elements within the system (trajectory, path, orbit), including their interaction with external elements and forces that are outside of the control of the system, but that must be modeled with it to understand system behavior (planets, asteroids, solar pressure, gravity)\nStructural view \u2013 Describes the structural components in the system (s/c bus, struts, panels, articulation), their physical attributes and connectors, along with the relevant structural aspects of other components (mass, stiffness, attachment)\nThermal view \u2013 Describes the active and passive thermal components in the system (radiators, coolers, vents) and their connectors (physical and free space radiation) and attributes, along with the thermal properties of other components (i.e. antenna as sun shade)\nPower view \u2013 Describes the active and passive power components in the system (solar panels, batteries, RTGs) within the system and their connectors, along with the power properties of other components (data system and propulsion elements as power sinks and structural panels as grounding plane)\nPropulsion view \u2013 Describes the active and passive propulsion components in the system (thrusters, gyros, motors, wheels) within the system and their connectors, along with the propulsive properties of other components\n\nEngineering viewpoint[7]\n\nAllocation view \u2013 Describes the allocation of functional objects to engineered physical and computational components within the system, permits analysis of performance and used to verify satisfaction of requirements\nSoftware view - Describes the software engineering aspects of the system, software design and implementation of functionality within software components, select languages and libraries to be used, define APIs, do the engineering of abstract functional objects into tangible software elements. Some functional elements, described using a software language, may actually be implemented as hardware (FPGA, ASIC)\nHardware views \u2013 Describes the hardware engineering aspects of the system, hardware design, selection and implementation of all of the physical components to be assembled into the system. There may be many of these views, each specific to a different engineering discipline.\nCommunications Protocol view \u2013 Describes the end to end design of the communications protocols and related data transport and data management services, shows the protocol stacks as they are implemented on each of the physical components of the system.\nRisk view \u2013 Describes the risks associated with the system design, processes, and technologies, assigns additional risk assessment attributes to other elements described in the architecture\nControl Engineering view - Analyzes system from the perspective of its controllability, allocation of elements into system under control and control system\nIntegration and Test view \u2013 Looks at the system from the perspective of what must be done to assemble, integrate and test system and sub-systems, and assemblies. Includes verification of proper functionality, driven by scenarios, in satisfaction of requirements.\nIV&V view \u2013 independent validation and verification of functionality and proper operation of the system in satisfaction of requirements. Does system as designed and developed meet goals and objectives.\n\nTechnology viewpoint[7]\n\nStandards view \u2013 Defines the standards to be adopted during design of the system (e.g. communication protocols, radiation tolerance, soldering). These are essentially constraints on the design and implementation processes.\nInfrastructure view \u2013 Defines the infrastructure elements that are to support the engineering, design, and fabrication process. May include data system elements (design repositories, frameworks, tools, networks) and hardware elements (chip fabrication, thermal vacuum facility, machine shop, RF testing lab)\nTechnology Development & Assessment view \u2013 Includes description of technology development programs designed to produce algorithms or components that may be included in a system development project. Includes evaluation of properties of selected hardware and software components to determine if they are at a sufficient state of maturity to be adopted for the mission being designed.\nIn contrast to the previous listed view models, this nominal set of views lists a whole range of views, possible to develop powerful and extensible approaches for describing a general class of software intensive system architectures.[7]See also[edit]\nEnterprise Architecture framework\nOrganizational architecture\nSoftware development methodology\nTreasury Enterprise Architecture Framework\nTOGAF\nZachman Framework\nOntology (information science)\nKnowledge Acquisition\nReferences[edit]\nAttribution\n This article incorporates public domain material from the National Institute of Standards and Technology website http://www.nist.gov.External links[edit]\n Media related to View models at Wikimedia Commons\n", "subtitles": ["Overview", "History", "View model topics", "Types of system view models", "Types of enterprise architecture views", "See also", "References", "External links"], "title": "View model"},
{"content": "Risk management is the identification, evaluation, and prioritization of risks (defined in ISO 31000 as the effect of uncertainty on objectives) followed by coordinated and economical application of resources to minimize, monitor, and control the probability or impact of unfortunate events[1] or to maximize the realization of opportunities. Risk management\u2019s objective is to assure uncertainty does not deflect the endeavor from the business goals.[2]Risks can come from various sources including uncertainty in financial markets, threats from project failures (at any phase in design, development, production, or sustainment life-cycles), legal liabilities, credit risk, accidents, natural causes and disasters, deliberate attack from an adversary, or events of uncertain or unpredictable root-cause. There are two types of events i.e. negative events can be classified as risks while positive events are classified as opportunities. Several risk management standards have been developed including the Project Management Institute, the National Institute of Standards and Technology, actuarial societies, and ISO standards.[3][4] Methods, definitions and goals vary widely according to whether the risk management method is in the context of project management, security, engineering, industrial processes, financial portfolios, actuarial assessments, or public health and safety.Strategies to manage threats (uncertainties with negative consequences) typically include avoiding the threat, reducing the negative effect or probability of the threat, transferring all or part of the threat to another party, and even retaining some or all of the potential or actual consequences of a particular threat, and the opposites for opportunities (uncertain future states with benefits).Certain aspects of many of the risk management standards have come under criticism for having no measurable improvement on risk; whereas the confidence in estimates and decisions seem to increase.[1] For example, one study found that one in six IT projects were black swans with gigantic overruns (cost overruns averaged 200%, and schedule overruns 70%).[5]Introduction[edit]A widely used vocabulary for risk management is defined by ISO Guide 73:2009, Risk management. Vocabulary.[3]In ideal risk management, a prioritization process is followed whereby the risks with the greatest loss (or impact) and the greatest probability of occurring are handled first, and risks with lower probability of occurrence and lower loss are handled in descending order. In practice the process of assessing overall risk can be difficult, and balancing resources used to mitigate between risks with a high probability of occurrence but lower loss versus a risk with high loss but lower probability of occurrence can often be mishandled.Intangible risk management identifies a new type of a risk that has a 100% probability of occurring but is ignored by the organization due to a lack of identification ability. For example, when deficient knowledge is applied to a situation, a knowledge risk materializes. Relationship risk appears when ineffective collaboration occurs. Process-engagement risk may be an issue when ineffective operational procedures are applied. These risks directly reduce the productivity of knowledge workers, decrease cost-effectiveness, profitability, service, quality, reputation, brand value, and earnings quality. Intangible risk management allows risk management to create immediate value from the identification and reduction of risks that reduce productivity.Risk management also faces difficulties in allocating resources. This is the idea of opportunity cost. Resources spent on risk management could have been spent on more profitable activities. Again, ideal risk management minimizes spending (or manpower or other resources) and also minimizes the negative effects of risks.According to the definition to the risk, the risk is the possibility that an event will occur and adversely affect the achievement of an objective. Therefore, risk itself has the uncertainty. Risk management such as COSO ERM, can help managers have a good control for their risk. Each company may have different internal control components, which leads to different outcomes. For example, the framework for ERM components includes Internal Environment, Objective Setting, Event Identification, Risk Assessment, Risk Response, Control Activities, Information and Communication, and Monitoring.Method[edit]For the most part, these methods consist of the following elements, performed, more or less, in the following order.\nidentify, characterize threats\nassess the vulnerability of critical assets to specific threats\ndetermine the risk (i.e. the expected likelihood and consequences of specific types of attacks on specific assets)\nidentify ways to reduce those risks\nprioritize risk reduction measures\nPrinciples[edit]The International Organization for Standardization (ISO) identifies the following principles of risk management:[6]Risk management should:\ncreate value \u2013 resources expended to mitigate risk should be less than the consequence of inaction\nbe an integral part of organizational processes\nbe part of decision making process\nexplicitly address uncertainty and assumptions\nbe a systematic and structured process\nbe based on the best available information\nbe tailorable\ntake human factors into account\nbe transparent and inclusive\nbe dynamic, iterative and responsive to change\nbe capable of continual improvement and enhancement\nbe continually or periodically re-assessed\nProcess[edit]According to the standard ISO 31000 Risk management \u2013 Principles and guidelines on implementation,[4] the process of risk management consists of several steps as follows:Establishing the context[edit]This involves:\nidentification of risk in a selected domain of interest\nplanning the remainder of the process\nmapping out the following:\n\nthe social scope of risk management\nthe identity and objectives of stakeholders\nthe basis upon which risks will be evaluated, constraints.\n\n\ndefining a framework for the activity and an agenda for identification\ndeveloping an analysis of risks involved in the process\nmitigation or solution of risks using available technological, human and organizational resources\nIdentification[edit]After establishing the context, the next step in the process of managing risk is to identify potential risks. Risks are about events that, when triggered, cause problems or benefits. Hence, risk identification can start with the source of our problems and those of our competitors (benefit), or with the problem itself.\nSource analysis[citation needed] \u2013 Risk sources may be internal or external to the system that is the target of risk management (use mitigation instead of management since by its own definition risk deals with factors of decision-making that cannot be managed).\nExamples of risk sources are: stakeholders of a project, employees of a company or the weather over an airport.\nProblem analysis[citation needed] \u2013 Risks are related to identified threats. For example: the threat of losing money, the threat of abuse of confidential information or the threat of human errors, accidents and casualties. The threats may exist with various entities, most important with shareholders, customers and legislative bodies such as the government.\nWhen either source or problem is known, the events that a source may trigger or the events that can lead to a problem can be investigated. For example: stakeholders withdrawing during a project may endanger funding of the project; confidential information may be stolen by employees even within a closed network; lightning striking an aircraft during takeoff may make all people on board immediate casualties.The chosen method of identifying risks may depend on culture, industry practice and compliance. The identification methods are formed by templates or the development of templates for identifying source, problem or event. Common risk identification methods are:\nObjectives-based risk identification[citation needed] \u2013 Organizations and project teams have objectives. Any event that may endanger achieving an objective partly or completely is identified as risk.\nScenario-based risk identification \u2013 In scenario analysis different scenarios are created. The scenarios may be the alternative ways to achieve an objective, or an analysis of the interaction of forces in, for example, a market or battle. Any event that triggers an undesired scenario alternative is identified as risk \u2013 see Futures Studies for methodology used by Futurists.\nTaxonomy-based risk identification \u2013 The taxonomy in taxonomy-based risk identification is a breakdown of possible risk sources. Based on the taxonomy and knowledge of best practices, a questionnaire is compiled. The answers to the questions reveal risks.[7]\nCommon-risk checking[citation needed] \u2013 In several industries, lists with known risks are available. Each risk in the list can be checked for application to a particular situation.[8]\nRisk charting [9] \u2013 This method combines the above approaches by listing resources at risk, threats to those resources, modifying factors which may increase or decrease the risk and consequences it is wished to avoid. Creating a matrix under these headings enables a variety of approaches. One can begin with resources and consider the threats they are exposed to and the consequences of each. Alternatively one can start with the threats and examine which resources they would affect, or one can begin with the consequences and determine which combination of threats and resources would be involved to bring them about.\nAssessment[edit]Once risks have been identified, they must then be assessed as to their potential severity of impact (generally a negative impact, such as damage or loss) and to the probability of occurrence. These quantities can be either simple to measure, in the case of the value of a lost building, or impossible to know for sure in the case of an unlikely event, the probability of occurrence of which is unknown. Therefore, in the assessment process it is critical to make the best educated decisions in order to properly prioritize the implementation of the risk management plan.Even a short-term positive improvement can have long-term negative impacts. Take the turnpike example. A highway is widened to allow more traffic. More traffic capacity leads to greater development in the areas surrounding the improved traffic capacity. Over time, traffic thereby increases to fill available capacity. Turnpikes thereby need to be expanded in a seemingly endless cycles. There are many other engineering examples where expanded capacity (to do any function) is soon filled by increased demand. Since expansion comes at a cost, the resulting growth could become unsustainable without forecasting and management.The fundamental difficulty in risk assessment is determining the rate of occurrence since statistical information is not available on all kinds of past incidents and is particularly scanty in the case of catastrophic events, simply because of their infrequency. Furthermore, evaluating the severity of the consequences (impact) is often quite difficult for intangible assets. Asset valuation is another question that needs to be addressed. Thus, best educated opinions and available statistics are the primary sources of information. Nevertheless, risk assessment should produce such information for senior executives of the organization that the primary risks are easy to understand and that the risk management decisions may be prioritized within overall company goals. Thus, there have been several theories and attempts to quantify risks. Numerous different risk formulae exist, but perhaps the most widely accepted formula for risk quantification is: Rate (or probability) of occurrence multiplied by the impact of the event equals risk magnitude.[vague]Risk options[edit]Risk mitigation measures are usually formulated according to one or more of the following major risk options, which are:\nDesign a new business process with adequate built-in risk control and containment measures from the start.\nPeriodically re-assess risks that are accepted in ongoing processes as a normal feature of business operations and modify mitigation measures.\nTransfer risks to an external agency (e.g. an insurance company)\nAvoid risks altogether (e.g. by closing down a particular high-risk business area)\nLater research[10] has shown that the financial benefits of risk management are less dependent on the formula used but are more dependent on the frequency and how risk assessment is performed.In business it is imperative to be able to present the findings of risk assessments in financial, market, or schedule terms. Robert Courtney Jr. (IBM, 1970) proposed a formula for presenting risks in financial terms. The Courtney formula was accepted as the official risk analysis method for the US governmental agencies. The formula proposes calculation of ALE (annualized loss expectancy) and compares the expected loss value to the security control implementation costs (cost-benefit analysis).Potential risk treatments[edit]Once risks have been identified and assessed, all techniques to manage the risk fall into one or more of these four major categories:[11]\nAvoidance (eliminate, withdraw from or not become involved)\nReduction (optimize \u2013 mitigate)\nSharing (transfer \u2013 outsource or insure)\nRetention (accept and budget)\nIdeal use of these risk control strategies may not be possible. Some of them may involve trade-offs that are not acceptable to the organization or person making the risk management decisions. Another source, from the US Department of Defense (see link), Defense Acquisition University, calls these categories ACAT, for Avoid, Control, Accept, or Transfer. This use of the ACAT acronym is reminiscent of another ACAT (for Acquisition Category) used in US Defense industry procurements, in which Risk Management figures prominently in decision making and planning.Risk avoidance[edit]This includes not performing an activity that could carry risk. An example would be not buying a property or business in order to not take on the legal liability that comes with it. Another would be not flying in order not to take the risk that the airplane were to be hijacked. Avoidance may seem the answer to all risks, but avoiding risks also means losing out on the potential gain that accepting (retaining) the risk may have allowed. Not entering a business to avoid the risk of loss also avoids the possibility of earning profits. Increasing risk regulation in hospitals has led to avoidance of treating higher risk conditions, in favor of patients presenting with lower risk.[12]Risk reduction[edit]Risk reduction or optimization involves reducing the severity of the loss or the likelihood of the loss from occurring. For example, sprinklers are designed to put out a fire to reduce the risk of loss by fire. This method may cause a greater loss by water damage and therefore may not be suitable. Halon fire suppression systems may mitigate that risk, but the cost may be prohibitive as a strategy.Acknowledging that risks can be positive or negative, optimizing risks means finding a balance between negative risk and the benefit of the operation or activity; and between risk reduction and effort applied. By an offshore drilling contractor effectively applying Health, Safety and Environment (HSE) management in its organization, it can optimize risk to achieve levels of residual risk that are tolerable.[13]Modern software development methodologies reduce risk by developing and delivering software incrementally. Early methodologies suffered from the fact that they only delivered software in the final phase of development; any problems encountered in earlier phases meant costly rework and often jeopardized the whole project. By developing in iterations, software projects can limit effort wasted to a single iteration.Outsourcing could be an example of risk reduction if the outsourcer can demonstrate higher capability at managing or reducing risks.[14] For example, a company may outsource only its software development, the manufacturing of hard goods, or customer support needs to another company, while handling the business management itself. This way, the company can concentrate more on business development without having to worry as much about the manufacturing process, managing the development team, or finding a physical location for a call center.Risk sharing[edit]Briefly defined as sharing with another party the burden of loss or the benefit of gain, from a risk, and the measures to reduce a risk.The term of 'risk transfer' is often used in place of risk sharing in the mistaken belief that you can transfer a risk to a third party through insurance or outsourcing. In practice if the insurance company or contractor go bankrupt or end up in court, the original risk is likely to still revert to the first party. As such in the terminology of practitioners and scholars alike, the purchase of an insurance contract is often described as a transfer of risk. However, technically speaking, the buyer of the contract generally retains legal responsibility for the losses transferred, meaning that insurance may be described more accurately as a post-event compensatory mechanism. For example, a personal injuries insurance policy does not transfer the risk of a car accident to the insurance company. The risk still lies with the policy holder namely the person who has been in the accident. The insurance policy simply provides that if an accident (the event) occurs involving the policy holder then some compensation may be payable to the policy holder that is commensurate with the suffering/damage.Some ways of managing risk fall into multiple categories. Risk retention pools are technically retaining the risk for the group, but spreading it over the whole group involves transfer among individual members of the group. This is different from traditional insurance, in that no premium is exchanged between members of the group up front, but instead losses are assessed to all members of the group.Risk retention[edit]Involves accepting the loss, or benefit of gain, from a risk when it occurs. True self-insurance falls in this category. Risk retention is a viable strategy for small risks where the cost of insuring against the risk would be greater over time than the total losses sustained. All risks that are not avoided or transferred are retained by default. This includes risks that are so large or catastrophic that either they cannot be insured against or the premiums would be infeasible. War is an example since most property and risks are not insured against war, so the loss attributed by war is retained by the insured. Also any amounts of potential loss (risk) over the amount insured is retained risk. This may also be acceptable if the chance of a very large loss is small or if the cost to insure for greater coverage amounts is so great that it would hinder the goals of the organization too much.Risk management plan[edit]Select appropriate controls or countermeasures to measure each risk. Risk mitigation needs to be approved by the appropriate level of management. For instance, a risk concerning the image of the organization should have top management decision behind it whereas IT management would have the authority to decide on computer virus risks.The risk management plan should propose applicable and effective security controls for managing the risks. For example, an observed high risk of computer viruses could be mitigated by acquiring and implementing antivirus software. A good risk management plan should contain a schedule for control implementation and responsible persons for those actions.According to ISO/IEC 27001, the stage immediately after completion of the risk assessment phase consists of preparing a Risk Treatment Plan, which should document the decisions about how each of the identified risks should be handled. Mitigation of risks often means selection of security controls, which should be documented in a Statement of Applicability, which identifies which particular control objectives and controls from the standard have been selected, and why.Implementation[edit]Implementation follows all of the planned methods for mitigating the effect of the risks. Purchase insurance policies for the risks that it has been decided to transferred to an insurer, avoid all risks that can be avoided without sacrificing the entity's goals, reduce others, and retain the rest.Review and evaluation of the plan[edit]Initial risk management plans will never be perfect. Practice, experience, and actual loss results will necessitate changes in the plan and contribute information to allow possible different decisions to be made in dealing with the risks being faced.Risk analysis results and management plans should be updated periodically. There are two primary reasons for this:\nto evaluate whether the previously selected security controls are still applicable and effective\nto evaluate the possible risk level changes in the business environment. For example, information risks are a good example of rapidly changing business environment.\nLimitations[edit]Prioritizing the risk management processes too highly could keep an organization from ever completing a project or even getting started. This is especially true if other work is suspended until the risk management process is considered complete.It is also important to keep in mind the distinction between risk and uncertainty. Risk can be measured by impacts \u00d7 probability.If risks are improperly assessed and prioritized, time can be wasted in dealing with risk of losses that are not likely to occur. Spending too much time assessing and managing unlikely risks can divert resources that could be used more profitably. Unlikely events do occur but if the risk is unlikely enough to occur it may be better to simply retain the risk and deal with the result if the loss does in fact occur. Qualitative risk assessment is subjective and lacks consistency. The primary justification for a formal risk assessment process is legal and bureaucratic.Areas[edit]As applied to corporate finance, risk management is the technique for measuring, monitoring and controlling the financial or operational risk on a firm's balance sheet, a traditional measure is the value at risk (VaR), but there also other measures like profit at risk (PaR) or margin at risk. The Basel II framework breaks risks into market risk (price risk), credit risk and operational risk and also specifies methods for calculating capital requirements for each of these components.In Information Technology, Risk management includes Incident Handling, an action plan for dealing with intrusions, cyber-theft, denial of service, fire, floods, and other security-related events. According to the SANS Institute,[15] it is a six step process: Preparation, Identification, Containment, Eradication, Recovery, and Lessons Learned.Enterprise[edit]In enterprise risk management, a risk is defined as a possible event or circumstance that can have negative influences on the enterprise in question. Its impact can be on the very existence, the resources (human and capital), the products and services, or the customers of the enterprise, as well as external impacts on society, markets, or the environment. In a financial institution, enterprise risk management is normally thought of as the combination of credit risk, interest rate risk or asset liability management, liquidity risk, market risk, and operational risk.In the more general case, every probable risk can have a pre-formulated plan to deal with its possible consequences (to ensure contingency if the risk becomes a liability).From the information above and the average cost per employee over time, or cost accrual ratio, a project manager can estimate:\nthe cost associated with the risk if it arises, estimated by multiplying employee costs per unit time by the estimated time lost (cost impact, C where C = cost accrual ratio * S)\n\n\n\n\n\n\nThis article uses abbreviations that may be confusing or ambiguous. There might be a discussion about this on the talk page. Please improve this article if you can. (September 2016)\n\n\n\n.\nthe probable increase in time associated with a risk (schedule variance due to risk, Rs where Rs = P * S):\n\nSorting on this value puts the highest risks to the schedule first. This is intended to cause the greatest risks to the project to be attempted first so that risk is minimized as quickly as possible.\nThis is slightly misleading as schedule variances with a large P and small S and vice versa are not equivalent. (The risk of the RMS Titanic sinking vs. the passengers' meals being served at slightly the wrong time).\n\n\nthe probable increase in cost associated with a risk (cost variance due to risk, Rc where Rc = P*C = P*CAR*S = P*S*CAR)\n\nsorting on this value puts the highest risks to the budget first.\nsee concerns about schedule variance as this is a function of it, as illustrated in the equation above.\n\n\nRisk in a project or process can be due either to Special Cause Variation or Common Cause Variation and requires appropriate treatment. That is to re-iterate the concern about extremal cases not being equivalent in the list immediately above.Medical device[edit]For medical devices, risk management is a process for identifying, evaluating and mitigating risks associated with harm to people and damage to property or the environment. Risk management is an integral part of medical device design and development, production processes and evaluation of field experience, and is applicable to all types of medical devices. The evidence of its application is required by most regulatory bodies such as the US FDA. The management of risks for medical devices is described by the International Organization for Standardization (ISO) in ISO 14971:2007, Medical Devices\u2014The application of risk management to medical devices, a product safety standard. The standard provides a process framework and associated requirements for management responsibilities, risk analysis and evaluation, risk controls and lifecycle risk management.The European version of the risk management standard was updated in 2009 and again in 2012 to refer to the Medical Devices Directive (MDD) and Active Implantable Medical Device Directive (AIMDD) revision in 2007, as well as the In Vitro Medical Device Directive (IVDD). The requirements of EN 14971:2012 are nearly identical to ISO 14971:2007. The differences include three (informative) Z Annexes that refer to the new MDD, AIMDD, and IVDD. These annexes indicate content deviations that include the requirement for risks to be reduced as far as possible, and the requirement that risks be mitigated by design and not by labeling on the medical device (i.e., labeling can no longer be used to mitigate risk).Typical risk analysis and evaluation techniques adopted by the medical device industry include hazard analysis, fault tree analysis (FTA), failure mode and effects analysis (FMEA), hazard and operability study (HAZOP), and risk traceability analysis for ensuring risk controls are implemented and effective (i.e. tracking risks identified to product requirements, design specifications, verification and validation results etc.). FTA analysis requires diagramming software. FMEA analysis can be done using a spreadsheet program. There are also integrated medical device risk management solutions.Through a draft guidance, the FDA has introduced another method named Safety Assurance Case for medical device safety assurance analysis. The safety assurance case is structured argument reasoning about systems appropriate for scientists and engineers, supported by a body of evidence, that provides a compelling, comprehensible and valid case that a system is safe for a given application in a given environment. With the guidance, a safety assurance case is expected for safety critical devices (e.g. infusion devices) as part of the pre-market clearance submission, e.g. 510(k). In 2013, the FDA introduced another draft guidance expecting medical device manufacturers to submit cybersecurity risk analysis information.Project management[edit]Project risk management must be considered at the different phases of acquisition. In the beginning of a project, the advancement of technical developments, or threats presented by a competitor's projects, may cause a risk or threat assessment and subsequent evaluation of alternatives (see Analysis of Alternatives). Once a decision is made, and the project begun, more familiar project management applications can be used:[16][17][18]\nPlanning how risk will be managed in the particular project. Plans should include risk management tasks, responsibilities, activities and budget.\nAssigning a risk officer \u2013 a team member other than a project manager who is responsible for foreseeing potential project problems. Typical characteristic of risk officer is a healthy skepticism.\nMaintaining live project risk database. Each risk should have the following attributes: opening date, title, short description, probability and importance. Optionally a risk may have an assigned person responsible for its resolution and a date by which the risk must be resolved.\nCreating anonymous risk reporting channel. Each team member should have the possibility to report risks that he/she foresees in the project.\nPreparing mitigation plans for risks that are chosen to be mitigated. The purpose of the mitigation plan is to describe how this particular risk will be handled \u2013 what, when, by whom and how will it be done to avoid it or minimize consequences if it becomes a liability.\nSummarizing planned and faced risks, effectiveness of mitigation activities, and effort spent for the risk management.\nMegaprojects (infrastructure)[edit]Megaprojects (sometimes also called major programs) are large-scale investment projects, typically costing more than $1 billion per project. Megaprojects include major bridges, tunnels, highways, railways, airports, seaports, power plants, dams, wastewater projects, coastal flood protection schemes, oil and natural gas extraction projects, public buildings, information technology systems, aerospace projects, and defense systems. Megaprojects have been shown to be particularly risky in terms of finance, safety, and social and environmental impacts.[20] Risk management is therefore particularly pertinent for megaprojects and special methods and special education have been developed for such risk management.[21]Natural disasters[edit]It is important to assess risk in regard to natural disasters like floods, earthquakes, and so on. Outcomes of natural disaster risk assessment are valuable when considering future repair costs, business interruption losses and other downtime, effects on the environment, insurance costs, and the proposed costs of reducing the risk.[22][23] The Sendai Framework for Disaster Risk Reduction is a 2015 international accord that has set goals and targets for disaster risk reduction in response to natural disasters.[24] There are regular International Disaster and Risk Conferences in Davos to deal with integral risk management.Information technology[edit]IT risk is a risk related to information technology. This is a relatively new term due to an increasing awareness that information security is simply one facet of a multitude of risks that are relevant to IT and the real world processes it supports.ISACA's Risk IT framework ties IT risk to enterprise risk management.Petroleum and natural gas[edit]For the offshore oil and gas industry, operational risk management is regulated by the safety case regime in many countries. Hazard identification and risk assessment tools and techniques are described in the international standard ISO 17776:2000, and organisations such as the IADC (International Association of Drilling Contractors) publish guidelines for Health, Safety and Environment (HSE) Case development which are based on the ISO standard. Further, diagrammatic representations of hazardous events are often expected by governmental regulators as part of risk management in safety case submissions; these are known as bow-tie diagrams (see Network theory in risk assessment). The technique is also used by organisations and regulators in mining, aviation, health, defence, industrial and finance.Pharmaceutical sector[edit]The principles and tools for quality risk management are increasingly being applied to different aspects of pharmaceutical quality systems. These aspects include development, manufacturing, distribution, inspection, and submission/review processes throughout the lifecycle of drug substances, drug products, biological and biotechnological products (including the use of raw materials, solvents, excipients, packaging and labeling materials in drug products, biological and biotechnological products). Risk management is also applied to the assessment of microbiological contamination in relation to pharmaceutical products and cleanroom manufacturing environments.[25]Risk communication[edit]Risk communication is a complex cross-disciplinary academic field related to core values of the targeted audiences.[26][27] Problems for risk communicators involve how to reach the intended audience, how to make the risk comprehensible and relatable to other risks, how to pay appropriate respect to the audience's values related to the risk, how to predict the audience's response to the communication, etc. A main goal of risk communication is to improve collective and individual decision making. Risk communication is somewhat related to crisis communication. Some experts coincide that risk is not only enrooted in the communication process but also it cannot be dissociated from the use of language. Though each culture develops its own fears and risks, these construes apply only by the hosting culture.See also[edit]\nBusiness continuity\nDisaster risk reduction\nEnvironmental Risk Management Authority (NZ)\nEvent chain methodology\nInternational Institute of Risk & Safety Management\nLoss-control consultant\nNational Safety Council (USA)\nOptimism bias\nRisk appetite\nRoy's safety-first criterion\nPrecautionary principle\nRepresentative heuristic\nReference class forecasting\nBNP Paribas#152 million risk management affair\nReferences[edit]", "subtitles": ["Introduction", "Process", "Risk options", "Limitations", "Areas", "Risk communication", "See also", "References"], "title": "Risk management"},
{"content": "Information science is a field primarily concerned with the analysis, collection, classification, manipulation, storage, retrieval, movement, dissemination, and protection of information.[1] Practitioners within and outside the field study application and usage of knowledge in organizations along with the interaction between people, organizations, and any existing information systems with the aim of creating, replacing, improving, or understanding information systems. Historically, information science is associated with computer science, library science, and telecommunications.[2] However, information science also incorporates aspects of diverse fields such as archival science, cognitive science, commerce, law, museology, management, mathematics, philosophy, public policy, and social sciences.Information science should not be confused with information theory. Information theory is the study of the types of communications we use, such as verbal, signal transmission, encoding, and others.[3] Information science deals with all the processes and techniques pertaining to the information life cycle, including capture, generation, packaging, dissemination, transformation, refining, repackaging, usage, storage, communication, protection, presentation etc. in any possible manner.Foundations[edit]by AhmadScope and approach[edit]Information science focuses on understanding problems from the perspective of the stakeholders involved and then applying information and other technologies as needed. In other words, it tackles systemic problems first rather than individual pieces of technology within that system. In this respect, one can see information science as a response to technological determinism, the belief that technology develops by its own laws, that it realizes its own potential, limited only by the material resources available and the creativity of its developers. It must therefore be regarded as an autonomous system controlling and ultimately permeating all other subsystems of society.[4]Many universities have entire colleges, departments or schools devoted to the study of information science, while numerous information-science scholars work in disciplines such as communication, computer science, law, library science, and sociology. Several institutions have formed an I-School Caucus (see List of I-Schools), but numerous others besides these also have comprehensive information foci.Within information science, current issues as of 2013[update] include:\nhuman\u2013computer interaction\ngroupware\nthe semantic web\nvalue-sensitive design\niterative design processes\nthe ways people generate, use and find information\nDefinitions of information science[edit]The first known usage of the word Information Science was in 1955.[5] An early definition of Information science (going back to 1968, the year when the American Documentation Institute renamed itself as the American Society for Information Science and Technology) states:\nInformation science is that discipline that investigates the properties and behavior of information, the forces governing the flow of information, and the means of processing information for optimum accessibility and usability. It is concerned with that body of knowledge relating to the origination, collection, organization, storage, retrieval, interpretation, transmission, transformation, and utilization of information. This includes the investigation of information representations in both natural and artificial systems, the use of codes for efficient message transmission, and the study of information processing devices and techniques such as computers and their programming systems. It is an interdisciplinary science derived from and related to such fields as mathematics, logic, linguistics, psychology, computer technology, operations research, the graphic arts, communications, library science, management, and other similar fields. It has both a pure science component, which inquires into the subject without regard to its application, and an applied science component, which develops services and products. (Borko, 1968, p.3).[6]\nSome authors use informatics as a synonym for information science. This is especially true when related to the concept developed by A. I. Mikhailov and other Soviet authors in the mid-1960s. The Mikhailov school saw informatics as a discipline related to the study of scientific information.[7] Informatics is difficult to precisely define because of the rapidly evolving and interdisciplinary nature of the field. Definitions reliant on the nature of the tools used for deriving meaningful information from data are emerging in Informatics academic programs.[8]Regional differences and international terminology complicate the problem. Some people[which?] note that much of what is called Informatics today was once called Information Science \u2013 at least in fields such as Medical Informatics. For example, when library scientists began also to use the phrase Information Science to refer to their work, the term informatics emerged:\nin the United States as a response by computer scientists to distinguish their work from that of library science\nin Britain as a term for a science of information that studies natural, as well as artificial or engineered, information-processing systems\nAnother term discussed as a synonym for information studies is information systems. Brian Campbell Vickery's Information Systems (1973) places information systems within IS. Ellis, Allen, & Wilson (1999), on the other hand, provide a bibliometric investigation describing the relation between two different fields: information science and information systems.Philosophy of information[edit]Philosophy of information (PI) studies conceptual issues arising at the intersection of computer science, information technology, and philosophy. It includes the investigation of the conceptual nature and basic principles of information, including its dynamics, utilisation and sciences, as well as the elaboration and application of information-theoretic and computational methodologies to its philosophical problems.[9]Ontology[edit]In computer science and information science, an ontology formally represents knowledge as a set of concepts within a domain, and the relationships between those concepts. It can be used to reason about the entities within that domain and may be used to describe the domain.More specifically, an ontology is a model for describing the world that consists of a set of types, properties, and relationship types. Exactly what is provided around these varies, but they are the essentials of an ontology. There is also generally an expectation that there be a close resemblance between the real world and the features of the model in an ontology.[10]In theory, an ontology is a formal, explicit specification of a shared conceptualisation.[11] An ontology renders shared vocabulary and taxonomy which models a domain with the definition of objects and/or concepts and their properties and relations.[12]Ontologies are the structural frameworks for organizing information and are used in artificial intelligence, the Semantic Web, systems engineering, software engineering, biomedical informatics, library science, enterprise bookmarking, and information architecture as a form of knowledge representation about the world or some part of it. The creation of domain ontologies is also fundamental to the definition and use of an enterprise architecture framework.Careers[edit]Information scientist[edit]An information scientist is an individual, usually with a relevant subject degree or high level of subject knowledge, providing focused information to scientific and technical research staff in industry, a role quite distinct from and complementary to that of a librarian. The title also applies to an individual carrying out research in information science.Systems Analyst[edit]Main article: Systems analystA systems analyst works on creating, designing, and improving information systems for a specific need. Oftentimes a systems analyst works with a business to evaluate and implement organizational processes and techniques for accessing information in order to improve efficiency and productivity within the business.Information professional[edit]An information professional is an individual who preserves, organizes, and disseminates information. Information professionals are skilled in the organization and retrieval of recorded knowledge. Traditionally, their work has been with print materials, but these skills are being increasingly used with electronic, visual, audio, and digital materials. Information professionals work in a variety of public, private, non-profit, and academic institutions. Information professionals can also be found within organisational and industrial contexts. Performing roles that include system design and development and system analysis.History[edit]Early beginnings[edit]Information science, in studying the collection, classification, manipulation, storage, retrieval and dissemination of information has origins in the common stock of human knowledge. Information analysis has been carried out by scholars at least as early as the time of the Abyssinian Empire with the emergence of cultural depositories, what is today known as libraries and archives.[13] Institutionally, information science emerged in the 19th century along with many other social science disciplines. As a science, however, it finds its institutional roots in the history of science, beginning with publication of the first issues of Philosophical Transactions, generally considered the first scientific journal, in 1665 by the Royal Society (London).The institutionalization of science occurred throughout the 18th century. In 1731, Benjamin Franklin established the Library Company of Philadelphia, the first library owned by a group of public citizens, which quickly expanded beyond the realm of books and became a center of scientific experiment, and which hosted public exhibitions of scientific experiments.[14] Benjamin Franklin invested a town in Massachusetts with a collection of books that the town voted to make available to all free of charge, forming the first Public Library.[15] Academie de Chirurgia (Paris) published Memoires pour les Chirurgiens, generally considered to be the first medical journal, in 1736. The American Philosophical Society, patterned on the Royal Society (London), was founded in Philadelphia in 1743. As numerous other scientific journals and societies were founded, Alois Senefelder developed the concept of lithography for use in mass printing work in Germany in 1796.19th century[edit]By the 19th century the first signs of information science emerged as separate and distinct from other sciences and social sciences but in conjunction with communication and computation. In 1801, Joseph Marie Jacquard invented a punched card system to control operations of the cloth weaving loom in France. It was the first use of memory storage of patterns system.[16] As chemistry journals emerged throughout the 1820s and 1830s,[17] Charles Babbage developed his difference engine, the first step towards the modern computer, in 1822 and his analytical engine\u201d by 1834. By 1843 Richard Hoe developed the rotary press, and in 1844 Samuel Morse sent the first public telegraph message. By 1848 William F. Poole begins the Index to Periodical Literature, the first general periodical literature index in the US.In 1854 George Boole published An Investigation into Laws of Thought..., which lays the foundations for Boolean algebra, which is later used in information retrieval.[18] In 1860 a congress was held at Karlsruhe Technische Hochschule to discuss the feasibility of establishing a systematic and rational nomenclature for chemistry. The congress did not reach any conclusive results, but several key participants returned home with Stanislao Cannizzaro's outline (1858), which ultimately convinces them of the validity of his scheme for calculating atomic weights.[19]By 1865, the Smithsonian Institution began a catalog of current scientific papers, which became the International Catalogue of Scientific Papers in 1902.[20] The following year the Royal Society began publication of its Catalogue of Papers in London. In 1868, Christopher Sholes, Carlos Glidden, and S. W. Soule produced the first practical typewriter. By 1872 Lord Kelvin devised an analogue computer to predict the tides, and by 1875 Frank Stephen Baldwin was granted the first US patent for a practical calculating machine that performs four arithmetic functions.[17] Alexander Graham Bell and Thomas Edison invented the telephone and phonograph in 1876 and 1877 respectively, and the American Library Association was founded in Philadelphia. In 1879 Index Medicus was first issued by the Library of the Surgeon General, U.S. Army, with John Shaw Billings as librarian, and later the library issues Index Catalogue, which achieved an international reputation as the most complete catalog of medical literature.[21]European documentation[edit]The discipline of documentation science, which marks the earliest theoretical foundations of modern information science, emerged in the late part of the 19th century in Europe together with several more scientific indexes whose purpose was to organize scholarly literature. Many information science historians cite Paul Otlet and Henri La Fontaine as the fathers of information science with the founding of the International Institute of Bibliography (IIB) in 1895.[22] A second generation of European Documentalists emerged after the Second World War, most notably Suzanne Briet. However, information science as a term is not popularly used in academia until sometime in the latter part of the 20th century.[23]Documentalists emphasized the utilitarian integration of technology and technique toward specific social goals. According to Ronald Day, As an organized system of techniques and technologies, documentation was understood as a player in the historical development of global organization in modernity \u2013 indeed, a major player inasmuch as that organization was dependent on the organization and transmission of information.[24] Otlet and Lafontaine (who won the Nobel Prize in 1913) not only envisioned later technical innovations but also projected a global vision for information and information technologies that speaks directly to postwar visions of a global information society. Otlet and Lafontaine established numerous organizations dedicated to standardization, bibliography, international associations, and consequently, international cooperation. These organizations were fundamental for ensuring international production in commerce, information, communication and modern economic development, and they later found their global form in such institutions as the League of Nations and the United Nations. Otlet designed the Universal Decimal Classification, based on Melville Dewey\u2019s decimal classification system.[25]Although he lived decades before computers and networks emerged, what he discussed prefigured what ultimately became the World Wide Web. His vision of a great network of knowledge focused on documents and included the notions of hyperlinks, search engines, remote access, and social networks.Otlet not only imagined that all the world's knowledge should be interlinked and made available remotely to anyone, but he also proceeded to build a structured document collection. This collection involved standardized paper sheets and cards filed in custom-designed cabinets according to a hierarchical index (which culled information worldwide from diverse sources) and a commercial information retrieval service (which answered written requests by copying relevant information from index cards). Users of this service were even warned if their query was likely to produce more than 50 results per search.[25] By 1937 documentation had formally been institutionalized, as evidenced by the founding of the American Documentation Institute (ADI), later called the American Society for Information Science and Technology.Transition to modern information science[edit]With the 1950s came increasing awareness of the potential of automatic devices for literature searching and information storage and retrieval. As these concepts grew in magnitude and potential, so did the variety of information science interests. By the 1960s and 70s, there was a move from batch processing to online modes, from mainframe to mini and microcomputers. Additionally, traditional boundaries among disciplines began to fade and many information science scholars joined with library programs. They further made themselves multidisciplinary by incorporating disciplines in the sciences, humanities and social sciences, as well as other professional programs, such as law and medicine in their curriculum. By the 1980s, large databases, such as Grateful Med at the National Library of Medicine, and user-oriented services such as Dialog and Compuserve, were for the first time accessible by individuals from their personal computers. The 1980s also saw the emergence of numerous special interest groups to respond to the changes. By the end of the decade, special interest groups were available involving non-print media, social sciences, energy and the environment, and community information systems. Today, information science largely examines technical bases, social consequences, and theoretical understanding of online databases, widespread use of databases in government, industry, and education, and the development of the Internet and World Wide Web.[26]Information dissemination in the 21st century[edit]Changing definition[edit]Dissemination has historically been interpreted as unilateral communication of information. With the advent of the internet, and the explosion in popularity of online communities, social media has changed the information landscape in many respects, and creates both new modes of communication and new types of information,[27] changing the interpretation of the definition of dissemination. The nature of social networks allows for faster diffusion of information than through organizational sources.[28] The internet has changed the way we view, use, create, and store information, now it is time to re-evaluate the way we share and spread it.Impact of social media on people and industry[edit]Social media networks provide an open information environment for the mass of people who have limited time or access to traditional outlets of information diffusion,[28] this is an increasingly mobile and social world [that] demands...new types of information skills.[27] Social media integration as an access point is a very useful and mutually beneficial tool for users and providers. All major news providers have visibility and an access point through networks such as Facebook and Twitter maximizing their breadth of audience. Through social media people are directed to, or provided with, information by people they know. The ability to share, like, and comment on...content[29] increases the reach farther and wider than traditional methods. People like to interact with information, they enjoy including the people they know in their circle of knowledge. Sharing through social media has become so influential that publishers must play nice if they desire to succeed. Although, it is often mutually beneficial for publishers and Facebook to share, promote and uncover new content[29] to improve both user base experiences. The impact of popular opinion can spread in unimaginable ways. Social media allows interaction through simple to learn and access tools; The Wall Street Journal offers an app through Facebook, and The Washington Post goes a step further and offers an independent social app that was downloaded by 19.5 million users in 6 months,[29] proving how interested people are in the new way of being provided information.Social media's power to facilitate topics[edit]The connections and networks sustained through social media help information providers learn what is important to people. The connections people have throughout the world enable the exchange of information at an unprecedented rate. It is for this reason that these networks have been realized for the potential they provide. Most news media monitor Twitter for breaking news,[28] as well as news anchors frequently request the audience to tweet pictures of events.[29] The users and viewers of the shared information have earned opinion-making and agenda-setting power[28] This channel has been recognized for the usefulness of providing targeted information based on public demand.Research vectors and applications[edit]The following areas are some of those that information science investigates and develops.Information access[edit]Information access is an area of research at the intersection of Informatics, Information Science, Information Security, Language Technology, Computer Science, and Library Science. The objectives of information access research are to automate the processing of large and unwieldy amounts of information and to simplify users' access to it. What about assigning privileges and restricting access to unauthorized users? The extent of access should be defined in the level of clearance granted for the information. Applicable technologies include information retrieval, text mining, text editing, machine translation, and text categorisation. In discussion, information access is often defined as concerning the insurance of free and closed or public access to information and is brought up in discussions on copyright, patent law, and public domain. Public libraries need resources to provide knowledge of information assurance.Information architecture[edit]Information architecture (IA) is the art and science of organizing and labelling websites, intranets, online communities and software to support usability.[30] It is an emerging discipline and community of practice focused on bringing together principles of design and architecture to the digital landscape.[31] Typically it involves a model or concept of information which is used and applied to activities that require explicit details of complex information systems. These activities include library systems and database development.Information management[edit]Information management (IM) is the collection and management of information from one or more sources and the distribution of that information to one or more audiences. This sometimes involves those who have a stake in, or a right to that information. Management means the organization of and control over the structure, processing and delivery of information. Throughout the 1970s this was largely limited to files, file maintenance, and the life cycle management of paper-based files, other media and records. With the proliferation of information technology starting in the 1970s, the job of information management took on a new light and also began to include the field of data maintenance.Information retrieval[edit]Information retrieval (IR) is the area of study concerned with searching for documents, for information within documents, and for metadata about documents, as well as that of searching structured storage, relational databases, and the World Wide Web. Automated information retrieval systems are used to reduce what has been called information overload. Many universities and public libraries use IR systems to provide access to books, journals and other documents. Web search engines are the most visible IR applications.An information retrieval process begins when a user enters a query into the system. Queries are formal statements of information needs, for example search strings in web search engines. In information retrieval a query does not uniquely identify a single object in the collection. Instead, several objects may match the query, perhaps with different degrees of relevancy.An object is an entity that is represented by information in a database. User queries are matched against the database information. Depending on the application the data objects may be, for example, text documents, images,[32] audio,[33] mind maps[34] or videos. Often the documents themselves are not kept or stored directly in the IR system, but are instead represented in the system by document surrogates or metadata.Most IR systems compute a numeric score on how well each object in the database match the query, and rank the objects according to this value. The top ranking objects are then shown to the user. The process may then be iterated if the user wishes to refine the query.[35]Information seeking[edit]Information seeking is the process or activity of attempting to obtain information in both human and technological contexts. Information seeking is related to, but different from, information retrieval (IR).Much library and information science (LIS) research has focused on the information-seeking practices of practitioners within various fields of professional work. Studies have been carried out into the information-seeking behaviors of librarians,[36] academics,[37] medical professionals,[38] engineers[39] and lawyers[40] (among others). Much of this research has drawn on the work done by Leckie, Pettigrew (now Fisher) and Sylvain, who in 1996 conducted an extensive review of the LIS literature (as well as the literature of other academic fields) on professionals' information seeking. The authors proposed an analytic model of professionals' information seeking behaviour, intended to be generalizable across the professions, thus providing a platform for future research in the area. The model was intended to prompt new insights... and give rise to more refined and applicable theories of information seeking (1996, p. 188). The model has been adapted by Wilkinson (2001) who proposes a model of the information seeking of lawyers.Information society[edit]An information society is a society where the creation, distribution, diffusion, uses, integration and manipulation of information is a significant economic, political, and cultural activity. The aim of an information society is to gain competitive advantage internationally, through using IT in a creative and productive way. The knowledge economy is its economic counterpart, whereby wealth is created through the economic exploitation of understanding. People who have the means to partake in this form of society are sometimes called digital citizens.Basically, an information society is the means of getting information from one place to another (Wark, 1997, p. 22). As technology has become more advanced over time so too has the way we have adapted in sharing this information with each other.Information society theory discusses the role of information and information technology in society, the question of which key concepts should be used for characterizing contemporary society, and how to define such concepts. It has become a specific branch of contemporary sociology.Knowledge representation and reasoning[edit]Knowledge representation (KR) is an area of artificial intelligence research aimed at representing knowledge in symbols to facilitate inferencing from those knowledge elements, creating new elements of knowledge. The KR can be made to be independent of the underlying knowledge model or knowledge base system (KBS) such as a semantic network.[41]Knowledge Representation (KR) research involves analysis of how to reason accurately and effectively and how best to use a set of symbols to represent a set of facts within a knowledge domain. A symbol vocabulary and a system of logic are combined to enable inferences about elements in the KR to create new KR sentences. Logic is used to supply formal semantics of how reasoning functions should be applied to the symbols in the KR system. Logic is also used to define how operators can process and reshape the knowledge. Examples of operators and operations include, negation, conjunction, adverbs, adjectives, quantifiers and modal operators. The logic is interpretation theory. These elements\u2014symbols, operators, and interpretation theory\u2014are what give sequences of symbols meaning within a KR.See also[edit]\nComputer and information science\nLibrary and information science\nOutline of information science\nOutline of information technology\nOutline of library science\nJournal of the Association for Information Science and Technology (JASIST)\nReferences[edit]Buckland, Michael (2011). What kind of science can information science be? Journal of the American Society for Information Science and Technology, published as early view October 2011.Ellis, D., Allen, D. and Wilson, T. 1999. Information Science and Information Systems: Conjunct Subjects Disjunct Disciplines. JASIS 50(12):1095\u20131107 (see also: https://web.archive.org/web/20120425073115/http://www.cais-acsi.ca/proceedings/2000/monarch_2000.pdf )Vickery; B. C. (1973). Information Systems. London: Butterworth.Further reading[edit]\nKhosrow-Pour, Mehdi (2005-03-22). Encyclopedia of Information Science and Technology. Idea Group Reference. ISBN 1-59140-553-X. \nExternal links[edit]\nKnowledge Map of Information Science\nJournal of Information Science\nDigital Library of Information Science and Technology open access archive for the Information Sciences\nCurrent Information Science Research at U.S. Geological Survey\nIntroduction to Information Science\nThe Nitecki Trilogy\nInformation science at the University of California at Berkeley in the 1960s: a memoir of student days\nChronology of Information Science and Technology\nLIBRES \u2013 Library and Information Science Research Electronic Journal -\nCurtin University of Technology, Perth, Western Australia\nShared decision-making\n", "subtitles": ["Foundations", "Careers", "History", "Information dissemination in the 21st century", "Research vectors and applications", "See also", "References", "Further reading", "External links"], "title": "Information science"},
{"content": "BCPL (Basic Combined Programming Language; or 'Before C Programming Language' (a common humorous backronym)[3] ) is a procedural, imperative, and structured computer programming language. Originally intended for writing compilers for other languages, BCPL is no longer in common use. However, its influence is still felt because a stripped down and syntactically changed version of BCPL, called B, was the language on which the C programming language was based. BCPL introduced several features of modern programming languages, including using curly braces to delimit code blocks; compilation via virtual machine byte code; and the world's first 'hello world' demonstrator program.Design[edit]\nBCPL was designed so that small and simple compilers could be written for it; reputedly some compilers could be run in 16 kilobytes. Further, the original compiler, itself written in BCPL, was easily portable. BCPL was thus a popular choice for bootstrapping a system.[citation needed] A major reason for the compiler's portability lay in its structure. It was split into two parts: the front end parsed the source and generated O-code for a virtual machine, and the back end took the O-code and translated it into the code for the target machine. Only 1/5 of the compiler's code needed to be rewritten to support a new machine, a task that usually took between 2 and 5 man-months. This approach became common practice later, e.g., Pascal or Java, but the original BCPL compiler was the first to define a virtual machine for this purpose.[citation needed]The language is unusual in having only one data type: a word, a fixed number of bits, usually chosen to align with the architecture's machine word and of adequate capacity to represent any valid storage address. For many machines of the time, this data type was a 16-bit word. This choice later proved to be a significant problem when BCPL was used on machines in which the smallest addressable item was not a word but a byte, or on machines with larger word sizes such as 32-bit or 64-bit.[citation needed]The interpretation of any value was determined by the operators used to process the values. (For example, + added two values together treating them as integers; ! indirected through a value, effectively treating it as a pointer.) In order for this to work, the implementation provided no type checking. The Hungarian notation was developed to help programmers avoid inadvertent type errors.[citation needed]The mismatch between BCPL's word orientation and byte-oriented hardware was addressed in several ways. One was providing standard library routines for packing and unpacking words into byte strings. Later, two language features were added: the bit-field selection operator and the infix byte indirection operator (denoted by the '%' character).[citation needed]BCPL handles bindings spanning separate compilation units in a unique way. There are no user-declarable global variables; instead there is a global vector, which is similar to blank common in Fortran. All data shared between different compilation units comprises scalars and pointers to vectors stored in a pre-arranged place in the global vector. Thus the header files (files included during compilation using the GET directive) become the primary means of synchronizing global data between compilation units, containing GLOBAL directives that present lists of symbolic names, each paired with a number that associates the name with the corresponding numerically addressed word in the global vector. As well as variables, the global vector also contains bindings for external procedures. This makes dynamic loading of compilation units very simple to achieve. Instead of relying on the link loader of the underlying implementation, effectively BCPL gives the programmer control of the linking process.[citation needed]The global vector also made it very simple to replace or augment standard library routines. A program could save the pointer from the global vector to the original routine and replace it with a pointer to an alternative version. The alternative might call the original as part of its processing. This could be used as a quick ad hoc debugging aid.[citation needed]BCPL was the first brace programming language, and the braces survived the syntactical changes and have become a common means of denoting program source code statements. In practice, on limited keyboards of the day, source programs often used the sequences $( and $) in place of the symbols { and }. The single-line // comments of BCPL, which were not adopted by C, reappeared in C++ and later in C99.The book BCPL: The language and its compiler describes the philosophy of BCPL as follows:\nThe philosophy of BCPL is not one of the tyrant who thinks he knows best and lays down the law on what is and what is not allowed; rather, BCPL acts more as a servant offering his services to the best of his ability without complaint, even when confronted with apparent nonsense. The programmer is always assumed to know what he is doing and is not hemmed in by petty restrictions. [4]\nExamples[edit]Print factorials:\nGET LIBHDR\n\nLET START() = VALOF $(\n        FOR I = 1 TO 5 DO\n                WRITEF(%N! = %I4*N, I, FACT(I))\n        RESULTIS 0\n$)\n\nAND FACT(N) = N = 0 -> 1, N * FACT(N - 1)\nCount solutions to the N queens problem:\nGET LIBHDR\n\nGLOBAL $(\n        COUNT: 200\n        ALL: 201\n$)\n\nLET TRY(LD, ROW, RD) BE\n        TEST ROW = ALL THEN\n                COUNT := COUNT + 1\n        ELSE $(\n                LET POSS = ALL & ~(LD | ROW | RD)\n                UNTIL POSS = 0 DO $(\n                        LET P = POSS & -POSS\n                        POSS := POSS - P\n                        TRY(LD + P << 1, ROW + P, RD + P >> 1)\n                $)\n        $)\n\nLET START() = VALOF $(\n        ALL := 1\n        FOR I = 1 TO 12 DO $(\n                COUNT := 0\n                TRY(0, 0, 0)\n                WRITEF(%I2-QUEENS PROBLEM HAS %I5 SOLUTIONS*N, I, COUNT)\n                ALL := 2 * ALL + 1\n        $)\n        RESULTIS 0\n$)\nHistory[edit]BCPL was first implemented by Martin Richards of the University of Cambridge in 1967.[1] BCPL was a response to difficulties with its predecessor, Cambridge Programming Language, later renamed Combined Programming Language (CPL), which was designed during the early 1960s. Richards created BCPL by removing those features of the full language which make compilation difficult. The first compiler implementation, for the IBM 7094 under Compatible Time-Sharing System (CTSS), was written while Richards was visiting Project MAC at the Massachusetts Institute of Technology (MIT) in the spring of 1967. The language was first described in a paper presented to the 1969 Spring Joint Computer Conference.[citation needed]BCPL has been rumored to have originally stood for Bootstrap Cambridge Programming Language, but CPL was never created since development stopped at BCPL, and the acronym was later reinterpreted for the BCPL book.[clarification needed][citation needed]BCPL is the language in which the original hello world program was written.[5] The first MUD was also written in BCPL (MUD1).Several operating systems were written partially or wholly in BCPL (for example, TRIPOS and the earliest versions of AmigaDOS). BCPL was also the initial language used in the seminal Xerox PARC Alto project, the first modern personal computer; among other projects, the Bravo document preparation system was written in BCPL.An early compiler, bootstrapped in 1969, by starting with a paper tape of the O-code of Martin Richards's Atlas 2 compiler, targeted the ICT 1900 series. The two machines had different word-lengths (48 vs 24 bits), different character encodings, and different packed string representations\u2014and the successful bootstrapping increased confidence in the practicality of the method.By late 1970, implementations existed for the Honeywell 635 and Honeywell 645, the IBM 360, the PDP-10, the TX-2, the CDC 6400, the UNIVAC 1108, the PDP-9, the KDF 9 and the Atlas 2. In 1974 a dialect of BCPL was implemented at BBN without using the intermediate O-code. The initial implementation was a cross-compiler hosted on BBN's Tenex PDP-10s, and directly targeted the PDP-11s used in BBN's implementation of the second generation IMPs used in the Arpanet.There was also a version produced for the BBC Micro in the mid-1980s, by Richards Computer Products, a company started by John Richards, the brother of Dr. Martin Richards.[6] The BBC Domesday Project made use of the language. Versions of BCPL for the Amstrad CPC and Amstrad PCW computers were also released in 1986 by UK software house Arnor Ltd. MacBCPL was released for the Apple Macintosh in 1985 by Topexpress Ltd, of Kensington, England.Both the design and philosophy of BCPL strongly influenced B, which in turn influenced C.[7] Programmers at the time debated whether an eventual successor to C would be called D (the next letter in the alphabet) or P (the next letter in the parent language name). The language most accepted as being C's successor is C++; ++ is the operator in the C language for incrementing a variable by 1.[8], although a D programming language also exists.In 1979, implementations of BCPL existed for at least 25 architectures; the language gradually fell out of favour as C became popular on non-Unix systems.Martin Richards maintains a modern Linux version of BCPL on his website, last updated in 2011. He continues to program in it, including for his research on musical automated score following.References[edit]Further reading[edit]\nMartin Richards, The BCPL Reference Manual (Memorandum M-352, Project MAC, Cambridge, MA, USA, July, 1967)\nMartin Richards, BCPL - a tool for compiler writing and systems programming (Proceedings of the Spring Joint Computer Conference, Vol 34, pp 557\u2013566, 1969)\nMartin Richards, Arthur Evans, Robert F. Mabee, The BCPL Reference Manual (MAC TR-141, Project MAC, Cambridge, MA, USA, 1974)\nMartin Richards, C. Whitby-Strevens, BCPL, the language and its compiler (Cambridge University Press, 1980) ISBN 0-521-28681-6\nExternal links[edit]\nMartin Richards' BCPL distribution\nMartin Richards' BCPL Reference Manual, 1967 by Dennis M. Ritchie\nBCPL entry in the Jargon File\nNordier & Associates' x86 port\nArnorBCPL manual\nRitchie's The Development of the C Language has commentary about BCPL's influence on C\n", "subtitles": ["Design", "Examples", "History", "References", "Further reading", "External links"], "title": "BCPL"},
{"content": "The syntax of JavaScript is the set of rules that define a correctly structured JavaScript program.The examples below make use of the log function of the console object present in most browsers for standard text output.The JavaScript standard library lacks an official standard text output function. Given that JavaScript is mainly used for client-side scripting within modern Web browsers, and that almost all Web browsers provide the alert function, alert can also be used, but is not commonly used.Origins[edit]Brendan Eich summarized the ancestry of the syntax in the first paragraph of the JavaScript 1.1 specification[1] as follows:\nJavaScript borrows most of its syntax from Java, but also inherits from Awk and Perl, with some indirect influence from Self in its object prototype system.\nBasics[edit]Case sensitivity[edit]JavaScript is case sensitive. It is common to start the name of a constructor with a capitalised letter, and the name of a function or variable with a lower-case letter.Example:Whitespace and semicolons[edit]Spaces, tabs and newlines used outside of string constants are called whitespace. Unlike C, whitespace in JavaScript source can directly impact semantics. Because of a technique called automatic semicolon insertion (ASI), some statements that are well formed when a newline is parsed will be considered complete (as if a semicolon were inserted just prior to the newline). Some authorities advise supplying statement-terminating semicolons explicitly, because it may lessen unintended effects of the automatic semicolon insertion.[2]There are two issues: five tokens can either begin a statement or be the extension of a complete statement; and five restricted productions, where line breaks are not allowed in certain positions, potentially yielding incorrect parsing.[3]The five problematic tokens are the open parenthesis (, open bracket [, slash /, plus +, and minus -. Of these, open parenthesis is common in the immediately-invoked function expression pattern, and open bracket occurs sometimes, while others are quite rare. The example given in the spec is:[3]with the suggestion that the preceding statement be terminated with a semicolon.Some suggest instead the use of leading semicolons on lines starting with '(' or '[', so the line is not accidentally joined with the previous one. This is known as a defensive semicolon, and is particularly recommended, because code may otherwise become ambiguous when it is rearranged.[3][4] For example:Initial semicolons are also sometimes used at the start of JavaScript libraries, in case they are appended to another library that omits a trailing semicolon, as this can result in ambiguity of the initial statement.The five restricted productions are return, throw, break, continue, and post-increment/decrement. In all cases, inserting semicolons does not fix the problem, but makes the parsed syntax clear, making the error easier to detect. return and throw take an optional value, while break and continue take an optional label. In all cases, the advice is to keep the value or label on the same line as the statement. This most often shows up in the return statement, where one might return a large object literal, which might be accidentally placed starting on a new line. For post-increment/decrement, there is potential ambiguity with pre-increment/decrement, and again it is recommended to simply keep these on the same line.Comments[edit]Comment syntax is the same as in C++ and many other languages.Variables[edit]Variables in standard JavaScript have no type attached, and any value can be stored in any variable. Starting with ES6, the version of the language finalised in 2015, variables can be declared with let (for a block level variable), var (for a function level variable) or const (for an immutable one). However, while the object assigned to a const cannot be changed, its properties can. Before ES6, variables were declared only with a var statement. An identifier must start with a letter, underscore (_), or dollar sign ($); subsequent characters can also be digits (0-9). Because JavaScript is case sensitive, letters include the characters A through Z (uppercase) and the characters a through z (lowercase).Starting with JavaScript 1.5, ISO 8859-1 or Unicode letters (or \\uXXXX Unicode escape sequences) can be used in identifiers.[5] In certain JavaScript implementations, the at sign (@) can be used in an identifier, but this is contrary to the specifications and not supported in newer implementations.Scoping and hoisting[edit]Variables are lexically scoped at function level (not block level as in C), and this does not depend on order (forward declaration is not necessary): if a variable is declared inside a function (at any point, in any block), then inside the function, the name will resolve to that variable. This is equivalent in block scoping to variables being forward declared at the top of the function, and is referred to as hoisting.[6]However, the variable value is undefined until it is initialized, and forward reference is not possible. Thus a var x = 1 statement in the middle of the function is equivalent to a var x declaration statement at the top of the function, and an x = 1 assignment statement at that point in the middle of the function \u2013 only the declaration is hoisted, not the assignment.Function statements, whose effect is to declare a variable of type Function and assign a value to it, are similar to variable statements, but in addition to hoisting the declaration, they also hoist the assignment \u2013 as if the entire statement appeared at the top of the containing function \u2013 and thus forward reference is also possible: the location of a function statement within an enclosing function is irrelevant.Block scoping can be produced by wrapping the entire block in a function and then executing it \u2014 this is known as the immediately-invoked function expression pattern. Or by declaring the variable using the let keyword.Declaration and assignment[edit]Variables declared outside any function are global. If a variable is declared in a higher scope, it can be accessed by child functions.When JavaScript tries to resolve an identifier, it looks in the local function scope. If this identifier is not found, it looks in the outer function that declared the local one, and so on along the scope chain until it reaches the global scope where global variables reside. If it is still not found, JavaScript will raise a ReferenceError exception.When assigning an identifier, JavaScript goes through exactly the same process to retrieve this identifier, except that if it is not found in the global scope, it will create the variable as a property of the global object.[7] As a consequence, a variable never declared will be global, if assigned. Declaring a variable (with the keyword var) in the global code (i.e. outside of any function body), assigning a never declared identifier or adding a property to the global object (usually window) will also create a new global variable.Note that JavaScript's strict mode forbids the assignment of an undeclared variable, which avoids global namespace pollution. Also const cannot be declared without initialization.Examples[edit]Here are examples of variable declarations and scope:Primitive data types[edit]The JavaScript language provides six primitive data types:\nUndefined\nNull\nNumber\nString\nBoolean\nSymbol\nSome of the primitive data types also provide a set of named values that represent the extents of the type boundaries. These named values are described within the appropriate sections below.Undefined[edit]The value of undefined is assigned to all uninitialized variables, and is also returned when checking for object properties that do not exist. In a Boolean context, the undefined value is considered a false value.Note: undefined is considered a genuine primitive type. Unless explicitly converted, the undefined value may behave unexpectedly in comparison to other types that evaluate to false in a logical context.Note: There is no built-in language literal for undefined. Thus (x == undefined) is not a foolproof way to check whether a variable is undefined, because in versions before ECMAScript 5, it is legal for someone to write var undefined = I'm defined now;. A more robust approach is to compare using (typeof x === 'undefined').Functions like this won't work as expected:Here, calling isUndefined(my_var) raises a ReferenceError if my_var is an unknown identifier, whereas typeof my_var === 'undefined' doesn't.Null[edit]Unlike undefined, null is often set to indicate that something has been declared, but has been defined to be empty. In a Boolean context, the value of null is considered a false value in JavaScript.Note: Null is a true primitive-type within the JavaScript language, of which null (note case) is the single value. As such, when performing checks that enforce type checking, the null value will not equal other false types. Surprisingly, null is considered an object by typeof.Number[edit]Numbers are represented in binary as IEEE-754 doubles, which provides an accuracy of nearly 16 significant digits. Because they are floating point numbers, they do not always exactly represent real numbers, including fractions.This becomes an issue when comparing or formatting numbers. For example:As a result, a routine such as the toFixed() method should be used to round numbers whenever they are formatted for output.Numbers may be specified in any of these notations:The extents +\u221e, \u2212\u221e and NaN (Not a Number) of the number type may be obtained by two program expressions:These three special values correspond and behave as the IEEE-754 describes them.The Number constructor, or a unary + or -, may be used to perform explicit numeric conversion:When used as a constructor, a numeric wrapper object is created (though it is of little use):However, it's impossible to use equality operators (== and ===) to determine whether a value is NaN:String[edit]A string in JavaScript is a sequence of characters. In JavaScript, strings can be created directly (as literals) by placing the series of characters between double () or single (') quotes. Such strings must be written on a single line, but may include escaped newline characters (such as \n). The JavaScript standard allows the backquote character (`, a.k.a. grave accent or backtick) to quote multiline literal strings, but this is supported only on certain browsers as of 2016: Firefox and Chrome, but not Internet Explorer 11.[citation needed]Individual characters within a string can be accessed using the charAt method (provided by String.prototype). This is the preferred way when accessing individual characters within a string, because it also works in non-modern browsers:In modern browsers, individual characters within a string can be accessed (as strings with only a single character) through the same notation as arrays:However, JavaScript strings are immutable:Applying the equality operator (==) to two strings returns true, if the strings have the same contents, which means: of the same length and containing the same sequence of characters (case is significant for alphabets). Thus:Quotes of the same type cannot be nested unless they are escaped.It is possible to create a string using the String constructor:These objects have a valueOf method returning the primitive string wrapped within them:Equality between two String objects does not behave as with string primitives:Boolean[edit]JavaScript provides a Boolean data type with true and false literals. The typeof operator returns the string boolean for these primitive types. When used in a logical context, 0, -0, null, NaN, undefined, and the empty string () evaluate as false due to automatic type coercion. All other values (the complement of the previous list) evaluate as true, including the strings 0, false and any object. Automatic type coercion by the equality comparison operators (== and !=) can be avoided by using the type checked comparison operators (=== and !==).When type conversion is required, JavaScript converts Boolean, Number, String, or Object operands as follows:[8]\nNumber and String\nThe string is converted to a number value. JavaScript attempts to convert the string numeric literal to a Number type value. First, a mathematical value is derived from the string numeric literal. Next, this value is rounded to nearest Number type value.\nBoolean\nIf one of the operands is a Boolean, the Boolean operand is converted to 1 if it is true, or to 0 if it is false.\nObject\nIf an object is compared with a number or string, JavaScript attempts to return the default value for the object. An object is converted to a primitive String or Number value, using the .valueOf() or .toString() methods of the object. If this fails, a runtime error is generated.\nDouglas Crockford advocates the terms truthy and falsy to describe how values of various types behave when evaluated in a logical context, especially in regard to edge cases.[9] The binary logical operators returned a Boolean value in early versions of JavaScript, but now they return one of the operands instead. The left\u2013operand is returned, if it can be evaluated as : false, in the case of conjunction: (a && b), or true, in the case of disjunction: (a || b); otherwise the right\u2013operand is returned. Automatic type coercion by the comparison operators may differ for cases of mixed Boolean and number-compatible operands (including strings that can be evaluated as a number, or objects that can be evaluated as such a string), because the Boolean operand will be compared as a numeric value. This may be unexpected. An expression can be explicitly cast to a Boolean primitive by doubling the logical negation operator: (!!), using the Boolean() function, or using the conditional operator: (c ? t : f).The new operator can be used to create an object wrapper for a Boolean primitive. However, the typeof operator does not return boolean for the object wrapper, it returns object. Because all objects evaluate as true, a method such as .valueOf(), or .toString(), must be used to retrieve the wrapped value. For explicit coercion to the Boolean type, Mozilla recommends that the Boolean() function (without new) be used in preference to the Boolean object.Symbol[edit]New in ECMAScript6. A Symbol is a unique and immutable identifier.Example:The Symbol wrapper also provides access to a variable free iterator.Native objects[edit]The JavaScript language provides a handful of native objects. JavaScript native objects are considered part of the JavaScript specification. JavaScript environment notwithstanding, this set of objects should always be available.Array[edit]An Array is a JavaScript object prototyped from the Array constructor specifically designed to store data values indexed by integer keys. Arrays, unlike the basic Object type, are prototyped with methods and properties to aid the programmer in routine tasks (for example, join, slice, and push).As in the C family, arrays use a zero-based indexing scheme: A value that is inserted into an empty array by means of the push method occupies the 0th index of the array.Arrays have a length property that is guaranteed to always be larger than the largest integer index used in the array. It is automatically updated, if one creates a property with an even larger index. Writing a smaller number to the length property will remove larger indices.Elements of Arrays may be accessed using normal object property access notation:The above two are equivalent. It's not possible to use the dot-notation or strings with alternative representations of the number:Declaration of an array can use either an Array literal or the Array constructor:Arrays are implemented so that only the defined elements use memory; they are sparse arrays. Setting myArray[10] = 'someThing' and myArray[57] = 'somethingOther' only uses space for these two elements, just like any other object. The length of the array will still be reported as 58.One can use the object declaration literal to create objects that behave much like associative arrays in other languages:One can use the object and array declaration literals to quickly create arrays that are associative, multidimensional, or both. (Technically, JavaScript does not support multidimensional arrays, but one can mimic them with arrays-of-arrays.)Date[edit]A Date object stores a signed millisecond count with zero representing 1970-01-01 00:00:00 UT and a range of \u00b1108 days. There are several ways of providing arguments to the Date constructor. Note that months are zero-based.Methods to extract fields are provided, as well as a useful toString:Error[edit]Custom error messages can be created using the Error class:These can be caught by try...catch...finally blocks as described in the section on exception handling.Math[edit]The Math object contains various math-related constants (for example, \u03c0) and functions (for example, cosine). (Note that the Math object has no constructor, unlike Array or Date. All its methods are static, that is class methods.) All the trigonometric functions use angles expressed in radians, not degrees or grads.Regular expression[edit]Character classes[edit]Character matching[edit]Repeaters[edit]Anchors[edit]Subexpression[edit]Flags[edit]Advanced methods[edit]Capturing groups[edit]Function[edit]Every function in JavaScript is an instance of the Function constructor:The add function above may also be defined using a function expression:There exists a shorthand for assigning a function expression to a variable, and is as follows:OrA function instance has properties and methods.Operators[edit]The '+' operator is overloaded: it is used for string concatenation and arithmetic addition. This may cause problems when inadvertently mixing strings and numbers. As a unary operator, it can convert a numeric string to a number.Similarly, the '*' operator is overloaded: it can convert a string into a number.Arithmetic[edit]JavaScript supports the following binary arithmetic operators:JavaScript supports the following unary arithmetic operators:The modulo operator displays the remainder after division by the modulus. If negative numbers are involved, the returned value depends on the operand.To always return a non-negative number, re-add the modulus and apply the modulo operator again:Assignment[edit]Assignment of primitive typesAssignment of object typesDestructuring assignment[edit]In Mozilla's JavaScript, since version 1.7, destructuring assignment allows the assignment of parts of data structures to several variables at once. The left hand side of an assignment is a pattern that resembles an arbitrarily nested object/array literal containing l-lvalues at its leaves that are to receive the substructures of the assigned value.Spread/rest operator[edit]The ECMAScript 2015 standard introduces the ... operator, for the related concepts of spread syntax[10] and rest parameters[11]Spread syntax provides another way to destructure arrays. It indicates that the elements in a specified array should be used as the parameters in a function call or the items in an array literal.In other words, ... transforms [...foo] into [foo[0], foo[1], foo[2]], and this.bar(...foo); into this.bar(foo[0], foo[1], foo[2]);.When ... is used in a function declaration, it indicates a rest parameter. The rest parameter must be the final named parameter in the function's parameter list. It will be assigned an Array containing any arguments passed to the function in excess of the other named parameters. In other words, it gets the rest of the arguments passed to the function (hence the name).Rest parameters are similar to Javascript's arguments object, which is an array-like object that contains all of the parameters (named and unnamed) in the current function call. Unlike arguments, however, rest parameters are true Array objects, so methods such as .slice() and .sort() can be used on them directly.The ... operator can only be used with Array objects. (However, there is a proposal to extend it to Objects in a future ECMAScript standard.[12])Comparison[edit]Variables referencing objects are equal or identical only if they reference the same object:See also String.Logical[edit]JavaScript provides four logical operators:\nunary negation (NOT = !a)\nbinary disjunction (OR = a || b) and conjunction (AND = a && b)\nternary conditional (c ? t : f)\nIn the context of a logical operation, any expression evaluates to true except the following:\nStrings: , '',\nNumbers: 0, -0, NaN,\nSpecial: null, undefined,\nBoolean: false.\nThe Boolean function can be used to explicitly convert to a primitive of type Boolean:The NOT operator evaluates its operand as a Boolean and returns the negation. Using the operator twice in a row, as a double negative, explicitly converts an expression to a primitive of type Boolean:The ternary operator can also be used for explicit conversion:Expressions that use features such as post\u2013incrementation (i++) have an anticipated side effect. JavaScript provides short-circuit evaluation of expressions; the right operand is only executed if the left operand does not suffice to determine the value of the expression.In early versions of JavaScript and JScript, the binary logical operators returned a Boolean value (like most C-derived programming languages). However, all contemporary implementations return one of their operands instead:Programmers who are more familiar with the behavior in C might find this feature surprising, but it allows for a more concise expression of patterns like null coalescing:Bitwise[edit]JavaScript supports the following binary bitwise operators:Examples:JavaScript supports the following unary bitwise operator:Bitwise Assignment[edit]JavaScript supports the following binary assignment operatorsExamples:String[edit]Examples:Control structures[edit]Compound statements[edit]A pair of curly brackets { } and an enclosed sequence of statements constitute a compound statement, which can be used wherever a statement can be used.If ... else[edit]Conditional operator[edit]The conditional operator creates an expression that evaluates as one of two expressions depending on a condition. This is similar to the if statement that selects one of two statements to execute depending on a condition. I.e., the conditional operator is to expressions what if is to statements.is the same as:Unlike the if statement, the conditional operator cannot omit its else-branch.Switch statement[edit]The syntax of the JavaScript switch statement is as follows:\nbreak; is optional; however, it is usually needed, since otherwise code execution will continue to the body of the next case block.\nAdd a break statement to the end of the last case as a precautionary measure, in case additional cases are added later.\nString literal values can also be used for the case values.\nExpressions can be used instead of values.\nCase default: is optional.\nBraces are required.\nFor loop[edit]The syntax of the JavaScript for loop is as follows:orFor ... in loop[edit]The syntax of the JavaScript for ... in loop is as follows:\nIterates through all enumerable properties of an object.\nIterates through all used indices of array including all user-defined properties of array object, if any. Thus it may be better to use a traditional for loop with a numeric index when iterating over arrays.\nThere are differences between the various Web browsers with regard to which properties will be reflected with the for...in loop statement. In theory, this is controlled by an internal state property defined by the ECMAscript standard called DontEnum, but in practice, each browser returns a slightly different set of properties during introspection. It is useful to test for a given property using if (some_object.hasOwnProperty(property_name)) { ... }. Thus, adding a method to the array prototype with Array.prototype.newMethod = function() {...} may cause for ... in loops to loop over the method's name.\nWhile loop[edit]The syntax of the JavaScript while loop is as follows:Do ... while loop[edit]The syntax of the JavaScript do ... while loop is as follows:With[edit]The with statement adds all of the given object's properties and methods into the following block's scope, letting them be referenced as if they were local variables.\nNote the absence of document. before each getElementById() invocation.\nThe semantics are similar to the with statement of Pascal.Because the availability of with statements hinders program performance and is believed to reduce code clarity (since any given variable could actually be a property from an enclosing with), this statement is not allowed in strict mode.Labels[edit]JavaScript supports nested labels in most implementations. Loops or blocks can be labelled for the break statement, and loops for continue. Although goto is a reserved word,[13] goto is not implemented in JavaScript.Functions[edit]A function is a block with a (possibly empty) parameter list that is normally given a name. A function may use local variables. If you exit the function without a return statement, the value undefined is returned.Functions are first class objects and may be assigned to other variables.The number of arguments given when calling a function may not necessarily correspond to the number of arguments in the function definition; a named argument in the definition that does not have a matching argument in the call will have the value undefined (that can be implicitly cast to false). Within the function, the arguments may also be accessed through the arguments object; this provides access to all arguments using indices (e.g. arguments[0], arguments[1], ... arguments[n]), including those beyond the number of named arguments. (While the arguments list has a .length property, it is not an instance of Array; it does not have methods such as .slice(), .sort(), etc.)Primitive values (number, boolean, string) are passed by value. For objects, it is the reference to the object that is passed.Functions can be declared inside other functions, and access the outer function's local variables. Furthermore, they implement full closures by remembering the outer function's local variables even after the outer function has exited.Objects[edit]For convenience, types are normally subdivided into primitives and objects. Objects are entities that have an identity (they are only equal to themselves) and that map property names to values (slots in prototype-based programming terminology). Objects may be thought of as associative arrays or hashes, and are often implemented using these data structures. However, objects have additional features, such as a prototype chain[clarification needed], which ordinary associative arrays do not have.JavaScript has several kinds of built-in objects, namely Array, Boolean, Date, Function, Math, Number, Object, RegExp and String. Other objects are host objects, defined not by the language, but by the runtime environment. For example, in a browser, typical host objects belong to the DOM (window, form, links, etc.).Creating objects[edit]Objects can be created using a constructor or an object literal. The constructor can use either a built-in Object function or a custom function. It is a convention that constructor functions are given a name that starts with a capital letter:Object literals and array literals allow one to easily create flexible data structures:This is the basis for JSON, which is a simple notation that uses JavaScript-like syntax for data exchange.Methods[edit]A method is simply a function that has been assigned to a property name of an object. Unlike many object-oriented languages, there is no distinction between a function definition and a method definition in object-related JavaScript. Rather, the distinction occurs during function calling; a function can be called as a method.When called as a method, the standard local variable this is just automatically set to the object instance to the left of the .. (There are also call and apply methods that can set this explicitly\u2014some packages such as jQuery do unusual things with this.)In the example below, Foo is being used as a constructor. There is nothing special about a constructor - it is just a plain function that initialises an object. When used with the new keyword, as is the norm, this is set to a newly created blank object.Note that in the example below, Foo is simply assigning values to slots, some of which are functions. Thus it can assign different functions to different instances. There is no prototyping in this example.Constructors[edit]Constructor functions simply assign values to slots of a newly created object. The values may be data or other functions.Example: Manipulating an object:The constructor itself is referenced in the object's prototype's constructor slot. So,Functions are objects themselves, which can be used to produce an effect similar to static properties (using C++/Java terminology) as shown below. (The function object also has a special prototype property, as discussed in the Inheritance section below.)Object deletion is rarely used as the scripting engine will garbage collect objects that are no longer being referenced.Inheritance[edit]JavaScript supports inheritance hierarchies through prototyping in the manner of Self.In the following example, the Derived class inherits from the Base class. When d is created as Derived, the reference to the base instance of Base is copied to d.base.Derive does not contain a value for aBaseFunction, so it is retrieved from aBaseFunction when aBaseFunction is accessed. This is made clear by changing the value of base.aBaseFunction, which is reflected in the value of d.aBaseFunction.Some implementations allow the prototype to be accessed or set explicitly using the __proto__ slot as shown below.The following shows clearly how references to prototypes are copied on instance creation, but that changes to a prototype can affect all instances that refer to it.In practice many variations of these themes are used, and it can be both powerful and confusing.Exception handling[edit]JavaScript includes a try ... catch ... finally exception handling statement to handle run-time errors.The try ... catch ... finally statement catches exceptions resulting from an error or a throw statement. Its syntax is as follows:Initially, the statements within the try block execute. If an exception is thrown, the script's control flow immediately transfers to the statements in the catch block, with the exception available as the error argument. Otherwise the catch block is skipped. The catch block can throw(errorValue), if it does not want to handle a specific error.In any case the statements in the finally block are always executed. This can be used to free resources, although memory is automatically garbage collected.Either the catch or the finally clause may be omitted. The catch argument is required.The Mozilla implementation allows for multiple catch statements, as an extension to the ECMAScript standard. They follow a syntax similar to that used in Java:In a browser, the onerror event is more commonly used to trap exceptions.Native functions and methods[edit](Not related to Web browsers.)eval (expression)[edit]Evaluates expression string parameter, which can include assignment statements. Variables local to functions can be referenced by the expression.See also[edit]\nComparison of JavaScript-based source code editors\nJavaScript\nReferences[edit]Further reading[edit]\nDanny Goodman: JavaScript Bible, Wiley, John & Sons, ISBN 0-7645-3342-8.\nDavid Flanagan, Paula Ferguson: JavaScript: The Definitive Guide, O'Reilly & Associates, ISBN 0-596-10199-6.\nThomas A. Powell, Fritz Schneider: JavaScript: The Complete Reference, McGraw-Hill Companies, ISBN 0-07-219127-9.\nAxel Rauschmayer: Speaking JavaScript: An In-Depth Guide for Programmers, 460 pages, O'Reilly Media, February 25, 2014, ISBN 978-1449365035. (free online edition)\nEmily Vander Veer: JavaScript For Dummies, 4th Edition, Wiley, ISBN 0-7645-7659-3.\nExternal links[edit]\nA re-introduction to JavaScript - Mozilla Developer Center\nComparison Operators in JavaScript\nECMAScript standard references: ECMA-262\nInteractive JavaScript Lessons - example-based\nJavaScript on About.com: lessons and explanation\nJavaScript Training\nMozilla Developer Center Core References for JavaScript versions 1.5, 1.4, 1.3 and 1.2\nMozilla JavaScript Language Documentation\n", "subtitles": ["Origins", "Basics", "Variables", "Primitive data types", "Native objects", "Operators", "Control structures", "Functions", "Objects", "Exception handling", "Native functions and methods", "See also", "References", "Further reading", "External links"], "title": "JavaScript syntax"},
{"content": "Coding conventions are a set of guidelines for a specific programming language that recommend programming style, practices, and methods for each aspect of a program written in that language. These conventions usually cover file organization, indentation, comments, declarations, statements, white space, naming conventions, programming practices, programming principles, programming rules of thumb, architectural best practices, etc. These are guidelines for software structural quality. Software programmers are highly recommended to follow these guidelines to help improve the readability of their source code and make software maintenance easier. Coding conventions are only applicable to the human maintainers and peer reviewers of a software project. Conventions may be formalized in a documented set of rules that an entire team or company follows, or may be as informal as the habitual coding practices of an individual. Coding conventions are not enforced by compilers.Software maintenance[edit]Reducing the cost of software maintenance is the most often cited reason for following coding conventions. In their introduction to code conventions for the Java programming language, Sun Microsystems provides the following rationale:[1]\nCode conventions are important to programmers for a number of reasons:\n\n40%\u201380% of the lifetime cost of a piece of software goes to maintenance.[2]\nHardly any software is maintained for its whole life by the original author.\nCode conventions improve the readability of the software, allowing engineers to understand new code more quickly and thoroughly.\nIf you ship your source code as a product, you need to make sure it is as well packaged and clean as any other product you create.\n\nSoftware engineering[edit]Software engineering is the process by which the project is specified and designed. It is absolutely fundamental to the success of projects, particularly if they are large projects. The software engineering process is what runs the coding process to successful completion. Good software engineering can make the difference between a successful project - both in financial and engineering terms - and a project that is, at worst, dead on delivery. Good software engineering will minimise downstream costs and maximise the marketing success of the project.Project specifications[edit]The following documents need to be produced:\nThe project brief. This is what kicks off the project. It is basically a brief description of the project and does not form a part of the formal document chain.\nThe requirements specification. This specifies what the project is to do. It is the fundamental part of the document chain. All other documents relate to it.\nThe project design. This is the formal design document of the project. It specifies the modules and the components, what their interfaces are and how they are connected. The software engineer, in carrying out this task, is looking at all the various ways to design the project and is choosing the best ways. He/She is taking into consideration all aspects including technical, quality, managerial, logistical and commercial. This includes time and cost of development, maintenance, support and usage of - both upfront and downstream. Part of this job is the architectural design but it goes a lot farther than that.\nThe test specification. This specifies all the tests that are to be performed and what results are to be checked for. Often tests are run within automated test harnesses and the tests specified either within code files or script files.\nThe test results.\nThe project specifications all the way down to the test results form what is called a document chain. Each document has a 1:1 relationship to the previous document. And ultimately the test specification has a 1:1 relationship to the requirements specification. The document chain is bidirectional - specifications going down, results coming back up.These methods are called formal methods.Quality[edit]Software peer review frequently involves reading source code. This type of peer review is primarily a defect detection activity. By definition, only the original author of a piece of code has read the source file before the code is submitted for review. Code that is written using consistent guidelines is easier for other reviewers to understand and assimilate, improving the efficacy of the defect detection process.Even for the original author, consistently coded software eases maintainability. There is no guarantee that an individual will remember the precise rationale for why a particular piece of code was written in a certain way long after the code was originally written. Coding conventions can help. Consistent use of whitespace improves readability and reduces the time it takes to understand the software.Coding standards[edit]Where coding conventions have been specifically designed to produce high-quality code, and have then been formally adopted, they then become coding standards. Specific styles, irrespective of whether they are commonly adopted, do not automatically produce good quality code. It is only if they are designed to produce good quality code that they actually result in good quality code being produced, i.e., they must be very logical in every aspect of their design - every aspect justified and resulting in quality code being produced.Reduction of complexity[edit]Complexity is a factor going against security.[3]The management of complexity includes the following basic principle: during the project development the question if this project been implemented with the least amount of code necessary has to be asked. If it has not, then unnecessary work has been undertaken and unnecessary cost - both upfront and downstream - has been incurred.Complexity is managed both at the design stage - how the project is architectured - and at the development stage - what coding is used. If the coding is kept basic and simple then the complexity will be minimised. Very often this is keeping the coding as 'physical' as possible - coding in a manner that is very direct and not highly abstract. This produces optimal code that is easy to read and follow.The more complex the code is the more likely it is to be buggy, the more difficult the bugs are to find and the more likely there are to be hidden bugs.Refactoring[edit]Refactoring refers to a software maintenance activity where source code is modified to improve readability or improve its structure. Software is often refactored to bring it into conformance with a team's stated coding standards after its initial release. Any change that does not alter the behavior of the software can be considered refactoring. Common refactoring activities are changing variable names, renaming methods, moving methods or whole classes and breaking large methods (or functions) into smaller ones.Agile software development methodologies plan for regular (or even continuous) refactoring making it an integral part of the team software development process.[4]Task automation[edit]Coding conventions allow to have simple scripts or programs whose job is to process source code for some purpose other than compiling it into an executable. It is common practice to count the software size (Source lines of code) to track current project progress or establish a baseline for future project estimates.Consistent coding standards can, in turn, make the measurements more consistent. Special tags within source code comments are often used to process documentation, two notable examples are javadoc and doxygen. The tools specify the use of a set of tags, but their use within a project is determined by convention.Coding conventions simplify writing new software whose job is to process existing software. Use of static code analysis has grown consistently since the 1950s. Some of the growth of this class of development tools stems from increased maturity and sophistication of the practitioners themselves (and the modern focus on safety and security), but also from the nature of the languages themselves.Language factors[edit]All software practitioners must grapple with the problem of organizing and managing a large number of sometimes complex instructions. For all but the smallest software projects, source code (instructions) are partitioned into separate files and frequently among many directories. It was natural for programmers to collect closely related functions (behaviors) in the same file and to collect related files into directories. As software development shifted from purely procedural programming (such as found in FORTRAN) towards more object-oriented constructs (such as found in C++), it became the practice to write the code for a single (public) class in a single file (the 'one class per file' convention).[5][6] Java has gone one step further - the Java compiler returns an error if it finds more than one public class per file.A convention in one language may be a requirement in another. Language conventions also affect individual source files. Each compiler (or interpreter) used to process source code is unique. The rules a compiler applies to the source creates implicit standards. For example, Python code is much more consistently indented than, say Perl, because whitespace (indentation) is actually significant to the interpreter. Python does not use the brace syntax Perl uses to delimit functions. Changes in indentation serve as the delimiters.[7][8] Tcl, which uses a brace syntax similar to Perl or C/C++ to delimit functions, does not allow the following, which seems fairly reasonable to a C programmer:The reason is that in Tcl, curly braces are not used only to delimit functions as in C or Java. More generally, curly braces are used to group words together into a single argument.[9][10] In Tcl, the word while takes two arguments, a condition and an action. In the example above, while is missing its second argument, its action (because the Tcl also uses the newline character to delimit the end of a command).Common conventions[edit]There are a large number of coding conventions; see Coding Style for numerous examples and discussion. Common coding conventions may cover the following areas:\nComment conventions\nIndent style conventions\nLine length conventions\nNaming conventions\nProgramming practices\nProgramming principles\nProgramming style conventions\nCoding standards include the CERT C Coding Standard, MISRA C, High Integrity C++.See also[edit]\nComparison of programming languages (syntax)\nHungarian Notation\nIndent style\nList of tools for static code analysis\nProgramming style\nSoftware metrics\nReferences[edit]External links[edit]Coding conventions for languages[edit]\nActionScript: Flex SDK coding conventions and best practices\nAda: Ada 95 Quality and Style Guide: Guidelines for Professional Programmers\nAda: Guide for the use of the Ada programming language in high integrity systems[permanent dead link] (ISO/IEC TR 15942:2000)\nAda: NASA Flight Software Branch \u2014 Ada Coding Standard\nAda: European Space Agency's Ada Coding Standard[permanent dead link] (BSSC(98)3)\nC: CERT C Coding Standard CERT C Coding Standard (SEI)\nC: Embedded C Coding Standard (Barr Group)\nC: Firmware Development Standard (Jack Ganssle)\nC++: Quantum Leaps C/C++ Coding Standard\nC++: C++ Programming/Programming Languages/C++/Code/Style Conventions\nC++: GeoSoft's C++ Programming Style Guidelines\nC++: Google's C++ Style Guide\nC++: High Integrity C++\nC#: C# Coding Conventions (C# Programming Guide)\nC#: Design Guidelines for Developing Class Libraries\nC#: Brad Abrams\nC#: Philips Healthcare\nD: The D Style\nDart: The Dart Style Guide\nErlang: Erlang Programming Rules and Conventions\nFlex: Code conventions for the Flex SDK\nJava: Ambysoft's Coding Standards for Java\nJava: Code Conventions for the Java Programming Language (Not actively maintained. Latest version: 1999-APR-20.)\nJava: GeoSoft's Java Programming Style Guidelines\nJava: Java Coding Standards at Curlie (based on DMOZ)\nJava: SoftwareMonkey's coding conventions for Java and other brace-syntax languages\nJavaScript: Code Conventions for the JavaScript Programming Language\nLisp: Riastradh's Lisp Style Rules\nMATLAB: Neurobat Coding Conventions for MATLAB\nMono: Programming style for Mono\nObject Pascal: Object Pascal Style Guide\nPerl: Perl Style Guide\nPHP::PEAR: PHP::PEAR Coding Standards\nPHP::FIG: PHP Framework Interop Group\nPython: Style Guide for Python Code\nRuby: The Unofficial Ruby Usage Guide\nRuby: GitHub Ruby style guide\nCoding conventions for projects[edit]\nApache Developers' C Language Style Guide\nDrupal PHP Coding Standards\nZend Framework Coding Standards\nGNU Coding Standards\nStyle guides for Google-originated open-source projects\nLinux Kernel Coding Style (or Documentation/CodingStyle in the Linux Kernel source tree)\nMozilla Coding Style Guide\nRoad Intranet's C++ Guidelines\nThe NetBSD source code style guide (formerly known as the BSD Kernel Normal Form)\nOpenBSD Kernel source file style guide (KNF)\nGNAT Coding Style: A Guide for GNAT Developers. GCC online documentation. Free Software Foundation. Retrieved 2009-01-19.  (PDF)\nZeroMQ C Language Style for Scalability (CLASS)\n", "subtitles": ["Software maintenance", "Task automation", "Language factors", "Common conventions", "See also", "References", "External links"], "title": "Coding conventions"},
{"content": "DBLP is a computer science bibliography website. Starting in 1993 at the University of Trier, Germany, it grew from a small collection of HTML files[1] and became an organization hosting a database and logic programming bibliography site. DBLP listed more than 3.66 million journal articles, conference papers, and other publications on computer science in July 2016, up from about 14,000 in 1995.[2] All important journals on computer science are tracked. Proceedings papers of many conferences are also tracked. It is mirrored at three sites across the Internet.[3][4][5]For his work on maintaining DBLP, Michael Ley received an award from the Association for Computing Machinery and the VLDB Endowment Special Recognition Award in 1997.DBLP originally stood for DataBase systems and Logic Programming. As a backronym, it has been taken to stand for Digital Bibliography & Library Project;[6] however, it is now preferred that the acronym be simply a name, hence the new title The DBLP Computer Science Bibliography.[7]Users of dblp remain unaffected by some additional attributes in the DTD as of February 2016, which are meant to support future versions of the data file. Thus consumers of the raw dblp.xml file should update their local dblp.dtd file, according to the notice on the home page, as of February 2016.[8]DBL-Browser[edit]DBL-Browser (Digital Bibliographic Library Browser) is a utility for browsing the DBLP website. The browser was written by Alexander Weber in 2005 at the University of Trier. It was designed for use off-line in reading the DBLP, which consisted of 696,000 bibliographic entries in 2005 (and in 2015 has more than 2.9 million).DBL-Browser is GPL software, available for download from SourceForge. It uses the XML DTD. Written in Java programming language, this code shows the bibliographic entry in several types of screens, ranging from graphics to text:\nAuthor page\nArticle page\nTable of contents\nRelated conferences / journals\nRelated authors (graphic representation of relationships)\nTrend analysis (graphics histogram)\nDBLP is similar to the bibliographic portion of arxiv.org which also links to articles. DBL-Browser provides a means to view some of the associated computer science articles.See also[edit]\nAssociation for Computational Linguistics\nCiteSeerX\nCogPrints\nGoogle Scholar\nLive Search Academic\nThe Collection of Computer Science Bibliographies\nDagstuhl\nReferences[edit]External links[edit]\nOfficial website\n\nCompleteSearch DBLP provides a fast search-as-you-type interface to DBLP, as well as faceted search. It is maintained by Hannah Bast and synchronized twice daily with the DBLP database. Since December 2007, the search functionality is embedded into each DBLP author page (via Javascript).\n\n\nLZI+DBLP. Schloss Dagstuhl. Leibniz Center for Informatics. Retrieved 2017-02-11. \nDBL-Browser at the Wayback Machine (archived 2011-02-22)\nFacetedDBLP provides a faceted search interface to DBLP, synchronized once per week with the DBLP database. In addition to common facets such as year, author, or venues, it contains a topic-based facet summarizing and characterizing the current result set based on the author keywords for individual publications. For the DBLP data, FacetedDBLP also provides an RDF dump (using D2R server technology) as well as an SQL dump based on the underlying mysql database.\nconfsearch Conference search engine and calendar based on DBLP.\nCloudMining DBLP is another faceted search solution with different visualizations.\n", "subtitles": ["DBL-Browser", "See also", "References", "External links"], "title": "DBLP"},
{"content": "This comparison of programming languages (syntax) compares the features of language syntax (format) for over 50 various computer programming languages.Expressions[edit]Programming language expressions can be broadly classified into four syntax structures:\nprefix notation\n\nLisp (* (+ 2 3) (expt 4 5))\n\ninfix notation\n\nFortran (2 + 3) * (4 ** 5)\n\nsuffix, postfix, or Reverse Polish notation\n\nForth 2 3 + 4 5 ** *\n\nmath-like notation\n\nTUTOR (2 + 3)(45) $$ note implicit multiply operator\nStatements[edit]Programming language statements typically have conventions for:\nstatement separators;\nstatement terminators; and\nline continuation\nA statement separator is used to demarcate boundaries between two separate statements. A statement terminator is used to demarcate the end of an individual statement. Line continuation is a convention in languages where the newline character could potentially be misinterpreted as a statement terminator. In such languages, it allows a single statement to span more than just one line.Line continuation[edit]Line continuation is generally done as part of lexical analysis: a newline normally results in a token being added to the token stream, unless line continuation is detected.\nWhitespace \u2013 Languages that do not need continuations\n\nAda \u2013 Lines terminate with semicolon\nC# \u2013 Lines terminate with semicolon\nJavaScript - Lines terminate with semicolon (which may be inferred)\nLua\nOCaml\n\nAmpersand as last character of line\n\nFortran 90, Fortran 95, Fortran 2003, Fortran 2008\n\nBackslash as last character of line\n\nbash[4] and other Unix shells\nC and C++ preprocessor\nFalcon\nMathematica and Wolfram Language\nPython[5]\nRuby\nJavaScript - only within single- or double-quoted strings\n\nBacktick as last character of line\n\nPowerShell\n\nHyphen as last character of line\n\nSQL*Plus\n\nUnderscore as last character of line\n\nAutoit\nCobra\nVisual Basic\nXojo\n\nEllipsis (as three periods\u2013not one special character)\n\nMATLAB: The ellipsis token need not be the last characters on the line, but any following it will be ignored.[6] (In essence, it begins a comment that extends through (i.e. including) the first subsequent newline character. Contrast this with an inline comment, which extends until the first subsequent newline.)\n\nComma delimiter as last character of line\n\nRuby (comment may follow delimiter)\n\nLeft bracket delimiter as last character of line\n\nBatch file: starting a parenthetical block can allow line continuation[7]\nRuby: left parenthesis, left square bracket, or left curly bracket\n\nOperator as last object of line\n\nRuby (comment may follow operator)\n\nBackslash as first character of continued line\n\nVimscript\n\nSome form of inline comment serves as line continuation\n\nTurbo Assembler: \\\nm4: dnl\nTeX: %\n\nCharacter position\n\nFortran 77: A non-comment line is a continuation of the previous non-comment line if any non-space character appears in column 6. Comment lines cannot be continued.\nCOBOL: String constants may be continued by not ending the original string in a PICTURE clause with ', then inserting a - in column 7 (same position as the * for comment is used.)\nTUTOR: Lines starting with a tab (after any indentation required by the context) continue the previous command.\n\n[End and Begin] using normal quotes\n\nC and C++ preprocessor: The string is ended normally and continues by starting with a quote on the next line.\nLibraries[edit]To import a library is a way to read external, possibly compiled, routines, programs or packages. Imports can be classified by level (module, package, class, procedure,...) and by syntax (directive name, attributes,...)\nFile import\n\nASP: #include file=filename\nAutoIt, C, C++: #include filename, #include <filename>\nCOBOL: COPY filename.\nFalcon: load filename\nFortran: include 'filename'\nLua: require(filename)\nMathematica and Wolfram Language: Import[filename]\nMATLAB: addpath(directory)[8]\nObjective-C: #import filename, #import <filename>\nPerl: require filename;\nPHP: include filename;, require filename;\nPick Basic: include [filename] program, #include [filename] program\nR: source(filename)\nRust: include!( filename);\n\nPackage import\n\nAda: with package\nC, C++: #include filename\nCobra: use Package.Name\nD: import package.module;, import altname = package.module;\nFalcon: load module, load module.submodule\nFortran 90+: use module, use module, only : identifier\nGo: import altname package/name\nHaskell: import Module, import qualified Module as M\nJava, MATLAB, kotlin: import package.*\nJavaScript: import altname from modname;, import modname;\nLua: require(modname)\nMathematica and Wolfram Language: <<name\nOberon: IMPORT module\nObjective-C: @import module;\nPascal: uses unit\nPerl: use Module;, use Module qw(import options);\nPython: import module, from module import *\nRust: mod modname;, #[path = filename] mod altname;, extern crate libname;, extern crate libname as altname;\nR: library(package)\nScala: import package._, import package\nSwift: import module\n\nClass import\n\nFalcon: import class\nJava, MATLAB, kotlin: import package.class\nJavaScript: import class from modname;, import {class} from modname;, import {class as altname} from modname;\nPHP: use Namespace\\ClassName;, use Namespace\\ClassName as AliasName;\nPython: from module import class\nScala: import package.class, import package.{ class1 => alternativeName, 'class2 }, import package._\n\nProcedure/function import\n\nD: import package.module : symbol;, import package.module : altsymbolname = symbol;\nHaskell: import Module (function)\nJavaScript: import function from modname;, import {function} from modname;, import {function as altname} from modname;\nMATLAB: import package.function\nPerl: use Module ('symbol');\nPHP: use function Namespace\\function_name;, use Namespace\\function_name as function_alias_name;\nPython: from module import function\nRust: use module::submodule::symbol;, use module::submodule::{symbol1, symbol2};, use module::submodule::symbol as altname;\nScala: import package.class.function, import package.class.{ function => alternativeName, otherFunction }\n\nConstant import\n\nPHP: use const Namespace\\CONST_NAME;\nThe above statements can also be classified by whether they are a syntactic convenience (allowing things to be referred to by a shorter name, but they can still be referred to by some fully qualified name without import), or whether they are actually required to access the code (without which it is impossible to access the code, even with fully qualified names).\nSyntactic convenience\n\nJava: import package.*, import package.class\nOCaml: open module\n\nRequired to access code\n\nGo: import altname package/name\nJavaScript: import altname from modname;\nPython: import module\nBlocks[edit]A block is a notation for a group of two or more statements, expressions or other units of code that are related in such a way as to comprise a whole.\nBraces (a.k.a. curly brackets) { ... }\n\nCurly bracket programming languages: C, C++, Objective-C, Go, Java, JavaScript/ECMAScript, C#, D, Perl, PHP (for & loop loops, or pass a block as argument), Rust, Scala, S-Lang, Swift, Windows PowerShell, Haskell (in do-notation)\n\nParentheses ( ... )\n\nOCaml, Standard ML\n\nSquare brackets [ ... ]\n\nSmalltalk (blocks are first class objects. a.k.a. closures)\n\nbegin ... end\n\nAda, ALGOL, Pascal, Ruby (for, do/while & do/until loops), OCaml, Simula, Erlang.\n\ndo ... done\n\nBash (for & while loops), Visual Basic, Fortran, TUTOR (with mandatory indenting of block body), Visual Prolog\n\ndo ... end\n\nLua, Ruby (pass blocks as arguments, for loop), Seed7 (encloses loop bodies between do and end)\n\nX ... end (e.g. if ... end):\n\nRuby (if, while, until, def, class, module statements), OCaml (for & while loops), MATLAB (if & switch conditionals, for & while loops, try clause, package, classdef, properties, methods, events, & function blocks), Lua (then / else & function)\n\n(begin ...)\n\nScheme\n\n(progn ...)\n\nLisp\n\n(do ...)\n\nClojure\n\nIndentation\n\nOff-side rule languages: Cobra, CoffeeScript, F#, Haskell (in do-notation when braces are omitted), occam, Python\nFree-form languages: most descendants from ALGOL (including C, Pascal, and Perl); Lisp languages\n\nOthers\n\nAda, Visual Basic, Seed7: if ... end if\nBash, sh, and ksh: if ... fi, do ... done, case ... esac;\nALGOL 68: begin ... end, ( ... ), if ... fi, do ... od\nLua, Pascal, Modula-2, Seed7: repeat ... until\nCOBOL: IF ... END-IF, PERFORM ... END-PERFORM, etc. for statements; ... . for sentences.\nComments[edit]Comments can be classified by:\nstyle (inline/block)\nparse rules (ignored/interpolated/stored in memory)\nrecursivity (nestable/non-nestable)\nuses (docstrings/throwaway comments/other)\nInline comments[edit]Inline comments are generally those that use a newline character to indicate the end of a comment, and an arbitrary delimiter or sequence of tokens to indicate the beginning of a comment.Examples:Block comments[edit]Block comments are generally those that use a delimiter to indicate the beginning of a comment, and another delimiter to indicate the end of a comment. In this context, whitespace and newline characters are not counted as delimiters.Examples:Unique variants[edit]Fortran[edit]\nIndenting lines in Fortran 66/77 is significant. The actual statement is in columns 7 through 72 of a line. Any non-space character in column 6 indicates that this line is a continuation of the previous line. A 'C' in column 1 indicates that this entire line is a comment. Columns 1 though 5 may contain a number which serves as a label. Columns 73 though 80 are ignored and may be used for comments; in the days of punched cards, these columns often contained a sequence number so that the deck of cards could be sorted into the correct order if someone accidentally dropped the cards. Fortran 90 removed the need for the indentation rule and added inline comments, using the ! character as the comment delimiter.\nCOBOL[edit]\nIn fixed format code, line indentation is significant. Columns 1\u20136 and columns from 73 onwards are ignored. If a * or / is in column 7, then that line is a comment. Until COBOL 2002, if a D or d was in column 7, it would define a debugging line which would be ignored unless the compiler was instructed to compile it.\nCobra[edit]\nCobra supports block comments with /# ... #/ which is like the /* ... */ often found in C-based languages, but with two differences. The # character is reused from the single-line comment form # ..., and the block comments can be nested which is convenient for commenting out large blocks of code.\nCurl[edit]\nCurl supports block comments with user-defined tags as in |foo# ... #foo|.\nLua[edit]\nLike raw strings, there can be any number of equals signs between the square brackets, provided both the opening and closing tags have a matching number of equals signs; this allows nesting as long as nested block comments/raw strings use a different number of equals signs than their enclosing comment: --[[comment --[=[ nested comment ]=] ]]. Lua discards the first newline (if present) that directly follows the opening tag.\nPerl 5[edit]\nBlock comments in Perl 5 are considered part of the documentation, and are given the name Plain Old Documentation (POD). Technically, Perl 5 does not have a convention for including block comments in source code, but POD is routinely used as a workaround. This has been addressed in Perl 6, which uses #`(...) to denote block comments.[16] Perl 6 actually allows the use of any right and left paired brackets after #` (i.e. #`(...), #`[...], #`{...}, #`<...>, and even the more complicated #`{{...}} are all valid block comments). Brackets are also allowed to be nested inside comments (i.e. #`{ a { b } c } goes to the last closing brace).\nPHP[edit]\nPHP supports standard C/C++ style comments, but supports Perl style as well.\nPython[edit]\nThe use of the triple-(double)quotes although sometimes used to comment-out lines of source, does not actually form a comment. The enclosed text becomes a string, usually a string statement. Python usually ignores a lone string as a statement (except when a string is the first statement in the body of a module, class or function; see docstring).\nRuby[edit]\nAs with Python and Perl, Ruby has no specific block-comment syntax. However, like Perl, documentation blocks can be used as block comments as they are ignored by the interpreter.\nS-Lang[edit]\nThe region of lines enclosed by the #<tag> and #</tag> delimiters are ignored by the interpreter. The tag name can be any sequence of alphanumeric characters that may be used to indicate how the enclosed block is to be deciphered. For example, #<latex> could indicate the start of a block of LaTeX formatted documentation.\nScheme and Racket[edit]\nThe next complete syntactic component (s-expression) can be commented out with #; .\nABAP[edit]ABAP supports two different kinds of comments. If the first character of a line, including indentation, is an asterisk (*) the whole line is considered as a comment, while a single double quote () begins an in-line commet which acts until the end of the line. ABAP comments are not possible between the statements EXEC SQL and ENDEXEC because Native SQL has other usages for these characters. In the most SQL dialects the double dash (--) can be used instead.Esoteric languages[edit]\nMany esoteric programming languages follow the convention that any text not executed by the instruction pointer (e.g., Befunge) or otherwise assigned a meaning (e.g., Brainfuck), is considered a comment.\nComment comparison[edit]There is a wide variety of syntax styles for declaring comments in source code. BlockComment in italics is used here to indicate block comment style. InlineComment in italics is used here to indicate inline comment style.See also[edit]\nCurly bracket programming languages, a broad family of programming language syntaxes\nPHP syntax and semantics\nC syntax\nC++ syntax\nJava syntax\nJavaScript syntax\nPython syntax and semantics\nReferences[edit]", "subtitles": ["Expressions", "Statements", "Libraries", "Blocks", "Comments", "See also", "References"], "title": "Comparison of programming languages (syntax)"},
{"content": "Devoxx (formerly named JavaPolis) is an annual Java, Android and HTML5 community conference created in 2001 by Stephan Janssen,[1] organized by the Belgian Java User Group (BeJUG). The conference takes place every year in Belgium around November.[2] With over 2,800 attendees in 2006, JavaPolis became the biggest vendor-independent Java conference in the world.[2] In 2008, the conference was renamed Devoxx.With over 3300 attendees, Devoxx 2011 was sold out 6 weeks before the event. In 2012, the conference was once again sold out 4 October [3] 6 weeks before the event, reaching 3400 attendees from 40 different countries. In 2017, Devoxx Belgium combi and conference tickets were already sold out the end of August.In 2012, the first edition of Devoxx France, organized by the Paris Java User Group took place from 18/4 until 20/4 in Paris. With more than 1200 attendees and 149 speakers Devoxx France was sold out 1 week before the event.[4][5]The first edition of Devoxx 4 Kids 2012 [6] was organized in Ghent (13 Oct) and Brussels (20 Oct) attracting 65 teenagers between 10 and 14 years. The teenagers played with Scratch,[7] programmed Lego Mindstorms and discovered the wonderful world of Mars Rovers and the NAO robot.Devoxx UK 2013 was announced during the opening keynote of Devoxx 2012 (November 14, Antwerp). The first edition of Devoxx UK was a two-day conference hosted at the Business Design Centre in London on March 26\u201327. The event ran back to back with Devoxx France and attracted over 500 attendees in its first year. Devoxx UK was spearheaded in 2013 by Ben Evans, Martijn Verburg, Dan Hardiker and Stephan Janssen in close collaboration with the London Java Community. Since January 2014, when Mark Hazell was appointed the new Chairman for Devoxx UK, he has focused Devoxx UK efforts, alongside Dan Hardiker, Stephan Janssen, James McGivern, a program committee of developer volunteers and community groups including the London Java Community.[8]In 2014, during the opening keynote at Devoxx BE, Devoxx PL was announced. Unlike all other editions, this Polish conference is not built from the ground up. It will replace a conference formerly known as 33rd Degree.[9] The first edition of this event is planned to take place in Krako\u0301w Congres Centre on 22\u201324 June 2015.In 2015, during the opening keynote at Devoxx France, Devoxx Morocco was announced. Again an existing conference that joins the Devoxx family. It replaces the conference formerly known as JMagreb[10] which takes place in Casablanca (Morocco) in Le Studio Des Arts Vivants.Devoxx US was announced during the opening keynote of Devoxx UK 2016 on June 8. This inaugural event is scheduled for March 21-23, 2017 at the San Jose Convention Center. The Eclipse Foundation will be responsible for overall operation and production of Devoxx US. The event is expected to attract more than 1000 software developers and over 30 sponsors. [11]Locations[edit]Devoxx (BE) takes place in one of the biggest European cinema complex, the Kinepolis, located in Antwerp, Belgium. Only part of the cinema complex is used for the conference. As a result, the speakers' video and slides are projected on the huge cinema screens using the available THX audio setup. Devoxx France 2014 was held at the Marriott Paris Rive Gauche conference center, while Devoxx France 2015 will be held at the Palais des congre\u0300s de Paris.The 4 editions of Devoxx UK have been held in the Business Design Centre, London, a former Victorian agricultural hall situated in Islington.The first 3 editions of Devoxx France took place in the Marriott Rive Gauche hotel and since 2015 Devoxx France moved to Le Palais des Congre\u0300s de Paris.Devoxx Poland 2015 takes place in ICE Krakow Congress Centre.Devoxx Morocco 2015 takes place in Le Studio Des Arts Vivants.Devoxx US 2017 takes place in San Jose convention center.Voxxed[edit]Voxxed is a website for software developers launched by Stephan Janssen and Mark Hazell on 12 November 2014. The site is a collection of entries submitted by its registered users, consisting of both original and syndicated content. It carries the strapline Share the Knowledge, reflecting this policy of sharing material from elsewhere on the web. The name Voxxed is a play-on-words with the word Devoxx and whilst a separate corporate entity, has strong ties to the event series. Voxxed editors are on site at Devoxx events to interview speakers and attendees, and the content is then shared on Voxxed.com.The website is divided into six different categories, consisting of; Java, JVM, Mobile, Cloud, Methodology, and Future. Each category is visible on the front page to new users and those who browse the site without logging into an account.Voxxed Days Initiative[edit]Voxxed Days is an international series of one day events for the Voxxed Community, focusing on the same areas as the Voxxed website (including: Server Side Java, Java SE, Cloud and Big Data, Web & HTML, Mobile, JVM, Architecture & Security, Methodology, and Future Technologies). Events are also attended by members of the Voxxed team, who then feature interviews and content from the event on the main Voxxed website.References[edit]External links[edit]\nDevoxx Landing home page\nDevoxx Belgium home page\nDevoxx France home page\nDevoxx UK home page\nDevoxx Poland home page\nDevoxx Morocco home page\nDevoxx US home page\nVoxxed home page\nVoxxed Days home page\n", "subtitles": ["Locations", "Voxxed", "Voxxed Days Initiative", "References", "External links"], "title": "Devoxx"},
{"content": "LWN.net is a computing webzine with an emphasis on free software and software for Linux and other Unix-like operating systems. It consists of a weekly issue, separate stories which are published most days, and threaded discussion attached to every story. Most news published daily are short summaries of articles published elsewhere, and are free to all viewers. Original articles are usually published weekly on Thursdays and are available only to subscribers for one week, after which they become free as well. LWN.net is part of Eklektix, Inc.LWN caters to a more technical audience than other Linux/free software publications. It is often praised for its in-depth coverage of Linux kernel internals.[3][4][5][6][7][8]The acronym LWN originally stood for Linux Weekly News; that name is no longer used because the site no longer covers exclusively Linux-related topics, and it has daily as well as weekly content.[9]History[edit]Founded by Jonathan Corbet and Elizabeth Coolbaugh and published since January 1998,[2] LWN was originally a free site devoted to collecting Linux news, published weekly.At the end of May 2002, LWN announced a redesigned site.[10] Among the changes was a facility for readers to post comments about stories.On July 25, 2002, LWN announced that due to its inability to raise enough funds through donations, the following issue would be its last.[11][12]Following an outpouring of support from readers, however, the editors of LWN decided to continue publishing, albeit with a subscription model. New weekly editions of LWN are initially only available to readers who subscribe at one of three levels (group subscriptions are also available). After a 1-week delay, each issue becomes freely available to readers who are unable or unwilling to pay.Contributors[edit]LWN.net staff currently consists of:[13]\nJonathan Corbet, who oversees the front and kernel pages, as well as overall executive editor functions;\nJake Edge, who manages the security page and miscellaneous functions;\nRebecca Sobol, who edits the distributions page and daily updates;\nNathan Willis, who maintains the development page.\nLWN.net also purchases a number of articles from freelance authors.[13]See also[edit]\nDistroWatch\nSlashdot\nPhoronix\nReferences[edit]External links[edit]\nOfficial website\nTimeline page - Also includes the site's own history at the bottom\n2007 Subscribers survey, showing demographics and what sections of the site are liked\n", "subtitles": ["History", "Contributors", "See also", "References", "External links"], "title": "LWN.net"},
{"content": "Library science (often termed library studies, library and information science, bibliothecography, library economy)[1] is an interdisciplinary or multidisciplinary field that applies the practices, perspectives, and tools of management, information technology, education, and other areas to libraries; the collection, organization, preservation, and dissemination of information resources; and the political economy of information. Martin Schrettinger, a Bavarian librarian, coined the discipline within his work (1808\u20131828) Versuch eines vollsta\u0308ndigen Lehrbuchs der Bibliothek-Wissenschaft oder Anleitung zur vollkommenen Gescha\u0308ftsfu\u0308hrung eines Bibliothekars.[2] Rather than classifying information based on nature-oriented elements, as was previously done in his Bavarian library, Schrettinger organized books in alphabetical order.[3] The first American school for library science was founded by Melvil Dewey at Columbia University in 1887.[4][5]Historically, library science has also included archival science.[6] This includes how information resources are organized to serve the needs of select user groups, how people interact with classification systems and technology, how information is acquired, evaluated and applied by people in and outside libraries as well as cross-culturally, how people are trained and educated for careers in libraries, the ethics that guide library service and organization, the legal status of libraries and information resources, and the applied science of computer technology used in documentation and records management.There is no generally agreed-upon distinction between the terms library science, librarianship, and library and information science, and to a certain extent they are interchangeable, perhaps differing most significantly in connotation. The term library and information science (LIS) is most often used;[7] most librarians consider it as only a terminological variation, intended to emphasize the scientific and technical foundations of the subject and its relationship with information science. LIS should not be confused with information theory, the mathematical study of the concept of information. Library and information science can also be seen as an integration of the two fields of library science and information science. Library philosophy has been contrasted with library science as the study of the aims and justifications of librarianship as opposed to the development and refinement of techniques.[8]History[edit]17th century[edit]The earliest text on library operations, Advice on Establishing a Library was published in 1627 by French librarian and scholar Gabriel Naude\u0301. Naude\u0301 wrote prolifically, producing works on many subjects including politics, religion, history, and the supernatural. He put into practice all the ideas put forth in Advice when given the opportunity to build and maintain the library of Cardinal Jules Mazarin.19th century[edit]Martin Schrettinger wrote the second textbook (the first in Germany) on the subject from 1808 to 1829.Thomas Jefferson, whose library at Monticello consisted of thousands of books, devised a classification system inspired by the Baconian method, which grouped books more or less by subject rather than alphabetically, as it was previously done.[9]The Jefferson collection provided the start of what became the Library of Congress.[10]The first American school of librarianship opened at Columbia University under the leadership of Melvil Dewey, noted for his 1876 decimal classification, on 5 January 1887 as the School of Library Economy. The term library economy was common in the U.S. until 1942, with the library science predominant through much of the 20th century.20th century[edit]Later, the term was used in the title of S. R. Ranganathan's The Five Laws of Library Science, published in 1931, and in the title of Lee Pierce Butler's 1933 book, An introduction to library science (University of Chicago Press).S. R. Ranganathan conceived the five laws of library science and the development of the first major analytico-synthetic classification system, the colon classification.[11] In India, he is considered to be the father of library science, documentation, and information science and is widely known throughout the rest of the world for his fundamental thinking in the field.In the United States, Lee Pierce Butler's new approach advocated research using quantitative methods and ideas in the social sciences with the aim of using librarianship to address society's information needs. He was one of the first faculty at the University of Chicago Graduate Library School, which changed the structure and focus of education for librarianship in the twentieth century. This research agenda went against the more procedure-based approach of library economy, which was mostly confined to practical problems in the administration of libraries.William Stetson Merrill's A Code for Classifiers, released in several editions from 1914 to 1939,[12] is an example of a more pragmatic approach, where arguments stemming from in-depth knowledge about each field of study are employed to recommend a system of classification. While Ranganathan's approach was philosophical it was also tied more to the day-to-day business of running a library. A reworking of Ranganathan's laws was published in 1995 which removes the constant references to books. Michael Gorman's Our Enduring Values: Librarianship in the 21st Century features his eight principles necessary by library professionals and incorporate knowledge and information in all their forms, allowing for digital information to be considered.In more recent years, with the growth of digital technology, the field has been greatly influenced by information science concepts. In the English speaking world the term library science seems to have been used for the first time in India[13] in the 1916 book Punjab Library Primer, written by Asa Don Dickinson and published by the University of the Punjab, Lahore, Pakistan.[14] This university was the first in Asia to begin teaching library science. The Punjab Library Primer was the first textbook on library science published in English anywhere in the world. The first textbook in the United States was the Manual of Library Economy, published in 1929. In 1923, C. C. Williamson, who was appointed by the Carnegie Corporation, published an assessment of library science education entitled The Williamson Report, which designated that universities should provide library science training.[15] This report had a significant impact on library science training and education. Library research and practical work, the area of information science, has remained largely distinct both in training and in research interests.21st century[edit]The digital age has transformed how information is accessed and retrieved. The library is now a part of a complex and dynamic educational, recreational, and informational infrastructure.[16] Mobile devices and applications with wireless networking, high-speed computers and networks, and the computing cloud have deeply impacted and developed information science and information services.[17] The evolution of the library sciences maintains its mission of access equity and community space, as well as the new means for information retrieval called information literacy skills. All catalogues, databases, and a growing number of books are all available on the Internet. In addition, the expanding free access to open source journals and sources such as Wikipedia have fundamentally impacted how information is accessed. Information literacy is the ability to determine the extent of information needed, access the needed information effectively and efficiently, evaluate information and its sources critically, incorporate selected information into one\u2019s knowledge base, use information effectively to accomplish a specific purpose, and understand the economic, legal, and social issues surrounding the use of information, and access and use information ethically and legally.[18]Education and Training[edit]Academic courses in library science include collection management, information systems and technology, research methods, information literacy, cataloging and classification, preservation, reference, statistics and management. Library science is constantly evolving, incorporating new topics like database management, information architecture and information management, among others. With the mounting acceptance of Wikipedia as a valued and reliable reference source, many libraries, museums and archives have introduced the role of Wikipedian in residence. As a result, some universities are including coursework relating to Wikipedia and Knowledge Management in their MLIS programs.Most schools in US only offer a master's degree in library and information science or an MLIS and do not offer an undergraduate degree in the subject. About fifty schools have this graduate program, and seven are still being ranked. Many have online programs, which makes attending more convenient if the college is not in a student's immediate vicinity. According to US News' online journal, University of Illinois is at the top of the list of best MLIS programs provided by universities. Second is University of North Carolina and third is University of Washington. All the listings can be found here.[19]Most professional library jobs require a professional post-baccalaureate degree in library science, or one of its equivalent terms, library and information science as a basic credential. In the United States and Canada the certification usually comes from a master's degree granted by an ALA-accredited institution, so even non-scholarly librarians have an originally academic background. In the United Kingdom, however, there have been moves to broaden the entry requirements to professional library posts, such that qualifications in, or experience of, a number of other disciplines have become more acceptable. In Australia, a number of institutions offer degrees accepted by the ALIA (Australian Library and Information Association). Global standards of accreditation or certification in librarianship have yet to be developed.[20]In academic regalia in the United States, the color for library science is lemon.Employment Outlook and Opportunities[edit]According to 'U.S. News & World Report', library and information science ranked as one of the Best Careers of 2008.[21] The median annual salary for 2016 was reported by The Bureau of Labor Statistics as $57,680  USD in the United States,[22] with additional salary breakdowns available by metropolitan area, with San Francisco coming in the highest with an average salary of $76,370.[23] This is down by 430 USD from the median salaries in 2014 at $58,110 reported by the U.S. Bureau of Labor Statistics. In December 2016, the BLS projected growth for the field at 9 percent between 2016 and 2026, which is as fast as the average for all occupations. Furthermore, the BLS states, Workers in this occupation tend to be older than workers in the rest of the economy. As a result, there may be more workers retiring from this occupation than other occupations. However, relatively large numbers of graduates from MLS programs may cause competition in some areas and for some jobs.[24]Gender and library science in the United States[edit]Librarianship manifests a dual career structure for men and women in the United States. While the ratio of female to male librarians remains roughly 4:1,[25][26] top positions are more often held by men. In large academic libraries, there is less of a discrepancy; however, overall, throughout the profession, men tend to hold higher or leadership positions.[27] Women, however, have made continuous progress toward equality.[28] Women have also been largely left out of standard histories of U.S. librarianship, but Suzanne Hildenbrand's scholarly assessment of the work done by women has expanded the historical record.[29] See also The Role of women in librarianship, 1876\u20131976: the entry, advancement, and struggle for equalization in one profession, by Kathleen Weibel, Kathleen de la Pen\u0303a McCook, and Dianne J. Ellsworth (1979), Phoenix, Ariz: Oryx Press.Gender equality and library leadership[edit]There was a Women's Meeting at the 1882 14th American Libraries Conference, where issues concerning the salaries of women librarians and what female patrons do in reading rooms were discussed.During the first 35 years of the American Library Association its presidency was held by men.[30] In 1911 Theresa Elmendorf became the first woman elected president of the ALA.[31] She was ALA president from May 24, 1911, until July 2, 1912.[32]In 1919, an ALA resolution promoting equal pay and opportunities for women in librarianship was defeated by a large margin.In 1970, Betty Wilson brought forth a resolution that would have had the ALA refrain from using facilities that discriminate against women. That resolution was also defeated by the membership.[33]In 1977, the ALA took a stand for the Equal Rights Amendment. The organization stated that they would no longer hold conferences in states that did not ratify the amendment, with the boycott measure set to take place in 1981.[34][35] An ERA Task Force was formed in 1979 towards this goal and a sum of $25,000 was allocated towards task force operations in unratified states. At the time, a number of state library associations passed pro-ERA resolutions and formed committees on women in libraries.[34]In 2013\u20132014, 82% of graduates in Master of Library Science (MLS) programs were female.[36]In 2016, Carla Hayden became the first female Librarian of Congress.[37]Professional association groups dedicated to librarianship and gender[edit]There are multiple groups within the American Library Association, dedicated to discussing, critiquing, and furthering gender-related and feminist issues within the profession.In 1969 the first women's rights task force was founded: the National Women's Liberation Front for Librarians (NWFFL or New-Waffle). It was also in 1969 that children's librarians, after being unable to find children's books that included working mothers, worked to remedy the situation and succeeded in their efforts.The American Library Association's Social Responsibilities Round Table Feminist Task Force (FTF) was founded in 1970 by women who wished to address sexism in libraries and librarianship.[38] FTF was the first ALA group to focus on women's issues.[38] In recent years during Women's History Month (March), the FTF has dedicated their efforts to expanding women\u2019s library history online, using the website Women of Library History.[39] The FTF also publishes the annual Amelia Bloomer Project list,[40] which includes some of the best feminist young adult literature of the year.The Committee on the Status of Women in Librarianship (COSWL) of the American Library Association,[41] founded in 1976, represents the diversity of women's interest within ALA and ensures that the Association considers the rights of the majority (women) in the library field, and promotes and initiates the collection, analysis, dissemination, and coordination of information on the status of women in librarianship. The bibliographic history of women in U.S. librarianship and women librarians developing services for women has been well-documented in the series of publications initially issued by the Social Responsibilities Round Table Task Force on Women and later continued by COSWL.[42]The ALA also has the Women & Gender Studies Section (WGSS) of its Division Association of College & Research Libraries; this section was formed to discuss, promote, and support women's studies collections and services in academic and research libraries.[43]Finally, the ALA has the Gay, Lesbian, Bisexual, and Transgender Roundtable (GLBTRT). While the GLBTRT deals with sexuality, different than gender identity, much of the roundtable\u2019s work is arguably feminist in nature, and concerned with issues of gender. The GLBTRT is committed to serving the information needs of the GLBT professional library community, and the GLBT information and access needs of individuals at large.[44]Library and information science scholarship relating to issues of gender[edit]Many scholars within the profession have taken up gender and its relationship to the discipline of library and information science. Scholars like Hope A. Olson and Sanford Berman have directed efforts at the problematic nature of cataloging and classification standards and schemes that are obscuring or exclusionary to marginalized groups. Others have written about the implications of gendered stereotypes in librarianship, particularly as they relate to library instruction.[45] Library instruction also intersects with feminist pedagogy, and scholars such as Maria Accardi have written about feminist pedagogical practices in libraries.[46] Library scholars have also dealt with issues of gender and leadership, having equitable gender representation in library collection development, and issues of gender and young adult and children\u2019s librarianship.Library policies relating to issues of gender[edit]The ALA Policy Manual states under B.2.1.15 Access to Library Resources and Services Regardless of Sex, Gender Identity, Gender Expression, or Sexual Orientation (Old Number 53.1.15): The American Library Association stringently and unequivocally maintains that libraries and librarians have an obligation to resist efforts that systematically exclude materials dealing with any subject matter, including sex, gender identity or expression, or sexual orientation. The Association also encourages librarians to proactively support the First Amendment rights of all library users, regardless of sex, sexual orientation, or gender identity or expression. Adopted 1993, amended 2000, 2004, 2008, 2010. [47] It also states under B.2.12 Threats to Library Materials Related to Sex, Gender Identity, or Sexual Orientation (Old Number 53.12), The American Library Association supports the inclusion in library collections of materials that reflect the diversity of our society, including those related to sex, sexual orientation, and gender identity or expression. ALA encourages all American Library Association chapters to take active stands against all legislative or other government attempts to proscribe materials related to sex, sexual orientation, and gender identity or expression; and encourages all libraries to acquire and make available materials representative of all the people in our society. Adopted 2005, Amended 2009, 2010. [48]Diversity in librarianship[edit]The field of library and information science seeks to provide a diverse working environment in libraries across the United States. Ways to change the status quo include diversifying the job field with regards to age, class, disabilities, ethnicity, gender identity, race, sex, and sexual orientation. The demographics of America are changing; those who were once minorities will become the majority.[49] Library facilities can best represent their communities by hiring diverse staffs.[50] The American Library Association and many libraries around the country realize the issue of diversity in the workplace and are addressing this problem.Statistics[edit]The majority of librarians working in the U.S. are female, between the ages of 55\u201364, and Caucasian.[51] A 2014 study by the American Library Association of research done from 2009 to 2010 shows that 98,273 of credentialed librarians were female while 20,393 were male. 15,335 of the total 111,666 were 35 and younger and only 6,222 were 65 or older. 104,393 were white; 6,160 African American, 3,260 American Pacific Islander; 185 Native American including Alaskan; 1,008 of two or more races, and 3,661 Latino. (ALA).[51]Strategies[edit]Scholarships/grants[edit]To help change the lack of diversity in library jobs in the U.S., more scholarships and grants are emerging. Most library and information science students do not belong to an underrepresented group and as a reaction to these research statistics, the field is creating ways to encourage more diversity in the classroom.[52]ALA Annual Research Diversity Grant Program[edit]The ALA Annual Research Diversity Grant Program is a way to encourage innovation in scholars and professionals to provide insight into how to diversify the field. The ALA Grant is directed toward those who have valuable and original research ideas that can add to the knowledge of diversity in the field of Librarianship. The program awards up to three individuals once a year with a grant of $2,500 each.[53] The applicants have submission guidelines, are given a timeline, and are shown the evaluation process online.[54]Cultural competencies[edit]One way to nurture cultural diversity in the library field is with cultural competencies. Scholars recommend defining skills needed to serve and work with others who belong to different cultures. It is suggested that these definitions be posted in job listings and be referred to when promoting and giving raises.[50] In library and information science graduate programs, it is also suggested by scholars that there is a lack of classes teaching students cultural competences. It is important for more classes to teach about diversity and measure the outcomes.[52]Recruitment[edit]Another strategy is to create interest in the field of library and information science from a young age. If minorities do not desire to become librarians, they will not seek to obtain an MLS or MLIS and therefore will not fill high job roles in libraries. A recommended solutions are to create a great experience for all racial group's early on in life.[55] This may inspire more young children to become interested in this field.Resources[edit]ALA Office for DiversityThe Office for Diversity is a sector of the American Library Association whose purpose is to aid libraries in providing a diverse workforce, gathering data, and teaching others about the issue of diversity related to the field of library and information science.[56]American Indian Library AssociationThe American Indian Library Association (AILA) was created in 1979. It publishes a newsletter twice a year and educates individuals and groups about Indian culture.[57]Black Caucus of the American Library AssociationBCALA promotes not only library services that can be enjoyed by the African American community but also the emergence of African American librarians and library professionals. By joining the association, patrons have access to newsletters, the entirety of their website, and networking boards.[58]CALAThe Chinese American Librarians Association (CALA) began March on 31, 1973. It was formerly known as the Mid-West Chinese American Librarians Association. It has members not only in America but in China, Hong Kong, Canada, and more. The organization promotes the Chinese culture through the outlet of libraries and communicates with others in the profession of librarianship.[59]ReformaReforma is the national library association to promote library and information services to Latino and the Spanish speaking, created in 1971. The association has pushed for Spanish collections in libraries, gives out yearly scholarships, and sends out quarterly newsletters. One of Reforma's main goals is to recruit Latinos into professional positions of the library.[60]The Deaf Community and Library Science in the United States[edit]Deaf people have the same needs as any other library visitors, and often have more difficulty accessing materials and services. Over the last few decades, libraries in the United States have begun to implement services and collections for D/deaf and HoH patrons and are working to make more of their collections, services, their communities, and even the world more accessible to this group of underserved people.The history of the role of libraries in the Deaf community in the United States is a sordid one. The American Library Association readily admits that disabled people belong to a minority that is often overlooked and underrepresented by people in the library, and the Deaf community belongs in this minority group.[61] However, in the last few decades, libraries across the United States have made great strides in the mission of making libraries more accessible to disabled people in general and to the Deaf community specifically. The Library Bill of Rights preamble states that all libraries are forums for information and ideas and as such libraries need to remove the physical and technological barriers which in turn would allow persons with disabilities full access to the resources available.[62]One notable American activist in the library community working toward accessibility for the deaf was Alice Lougee Hagemeyer.[63][64]Australian librarian Karen McQuigg stated in 2003 that even ten years ago, when I was involved in a project looking at what public libraries could offer the deaf, it seemed as if the gap between the requirements of this group and what public libraries could offer was too great for public libraries to be able to serve them effectively.[65] Clearly, not even so long ago, there was quite a dearth of information for or about the deaf community available in libraries across the nation and around the globe.New guidelines from library organizations such as International Federation of Library Associations and Institutions (IFLA) and the ALA were written in order to help libraries make their information more accessible to people with disabilities, and in some cases, specifically the deaf community. IFLA's Guidelines for Library Services to Deaf People is one such set of guidelines, was published to inform libraries of the services that should be provided for deaf patrons. Most of the guidelines pertain to ensuring that deaf patrons have equal access to all available library services. Other guidelines include training library staff to provide services for the deaf community, availability of text telephones or TTYs not only to assist patrons with reference questions but also for making outside calls, using the most recent technology in order to communicate more effectively with deaf patrons, including closed captioning services for any television services, and developing a collection that would interest the members of the deaf community.[66]Over the years, library services have begun to evolve in order to accommodate the needs and desires of local deaf communities. There is now a Library Service to People Who Are Deaf or Hard of Hearing Forum for libraries to look at to find out what they can do to better serve their Deaf/HoH users. At the Queen Borough Public Library (QBPL) in New York, the staff implemented new and innovative ideas in order to involve the community and library staff with the deaf people in their community. The QBPL hired a deaf librarian, Lori Stambler, to train the library staff about deaf culture, to teach sign language classes for family members and people who are involved with deaf people, and to teach literacy classes for deaf patrons. In working with the library, Stambler was able to help the community reach out to its deaf neighbors, and helped other deaf people become more active in their outside community.[67]Deaf libraries[edit]The library at Gallaudet University, the only deaf liberal arts university in the United States, was founded in 1876. The library's collection has grown from a small number of reference books to the world's largest collection of deaf-related materials, with over 234,000 books and thousands of other materials in different formats. The collection is so large that the library had to create a hybrid classification system based on the Dewey Decimal Classification System in order to make cataloging and location within the library easier for both library staff and users. The library also houses the university's archives, which holds some of the oldest deaf-related books and documents in the world.[68][69]In Nashville, Tennessee, Sandy Cohen manages the Library Services for the Deaf and Hard of Hearing (LSDHH). The program was created in 1979 in response to information accessibility issues for the deaf in the Nashville area. Originally, the only service provided was the news via a teletypewriter or TTY, but today, the program has expanded to serving the entire state of Tennessee by providing all different types of information and material on deafness, deaf culture, and information for family members of deaf people, as well as a historical and reference collection.[70]Theory and practice of library science[edit]Many practicing librarians do not contribute to LIS scholarship, but focus on daily operations within their own libraries or library systems. Other practicing librarians, particularly in academic libraries, do perform original scholarly LIS research and contribute to the academic end of the field.Whether or not individual professional librarians contribute to scholarly research and publication, many are involved with and contribute to the advancement of the profession and of library science and information science through local, state, regional, national and international library or information organizations.Library science is very closely related to issues of knowledge organization; however, the latter is a broader term which covers how knowledge is represented and stored (computer science/linguistics), how it might be automatically processed (artificial intelligence), and how it is organized outside the library in global systems such as the internet. In addition, library science typically refers to a specific community engaged in managing holdings as they are found in university and government libraries, while knowledge organization in general refers to this and also to other communities (such as publishers) and other systems (such as the Internet). The library system is thus one socio-technical structure for knowledge organization.[71]The terms information organization and knowledge organization are often used synonymously.[72] :106 The fundamentals of their study (particularly theory relating to indexing and classification) and many of the main tools used by the disciplines in modern times to provide access to digital resources (abstracting, metadata, resource description, systematic and alphabetic subject description, and terminology) originated in the 19th century and were developed, in part, to assist in making humanity's intellectual output accessible by recording, identifying, and providing bibliographic control of printed knowledge.[72] :105Information has been published which analyses the relations between philosophy of information (PI), library and information science (LIS), and social epistemology (SE).[73]Types of libraries[edit]Public library[edit]The study of librarianship for public libraries covers issues such as cataloging; collection development for a diverse community; information literacy; readers' advisory; community standards; public services-focused librarianship; serving a diverse community of adults, children, and teens; intellectual freedom; censorship; and legal and budgeting issues. The public library as a commons or public sphere based on the work of Ju\u0308rgen Habermas has become a central metaphor in the 21st century.[74]Most people are familiar with municipal public libraries, but there are many different types of public libraries that exist. There are four different types of public libraries: association libraries, municipal public libraries, school district libraries and special district public libraries. It is very important to be able to distinguish between the four. Each receives its funding through different sources. Each is established by a different set of voters. And, not all are subject to municipal civil service governance. Listed below is a chart from the New York State Library's library development website. This chart lists all of the information about the different public libraries.[75]School library/media center[edit]The study of school librarianship covers library services for children in schools through secondary school. In some regions, the local government may have stricter standards for the education and certification of school librarians (who are often considered a special case of teacher), than for other librarians, and the educational program will include those local criteria. School librarianship may also include issues of intellectual freedom, pedagogy, information literacy, and how to build a cooperative curriculum with the teaching staff.The study of academic librarianship covers library services for colleges and universities. Issues of special importance to the field may include copyright; technology, digital libraries, and digital repositories; academic freedom; open access to scholarly works; as well as specialized knowledge of subject areas important to the institution and the relevant reference works. Librarians often divide focus individually as liaisons on particular schools within a college or university.Some academic librarians are considered faculty, and hold similar academic ranks to those of professors, while others are not. In either case, the minimal qualification is a Master of Arts in Library Studies or Masters of Arts in Library and Information Science. Some academic libraries may only require a master's degree in a specific academic field or a related field, such as educational technology.Archives[edit]The study of archives includes the training of archivists, librarians specially trained to maintain and build archives of records intended for historical preservation. Special issues include physical preservation, conservation and restoration of materials and mass deacidification; specialist catalogs; solo work; access; and appraisal. Many archivists are also trained historians specializing in the period covered by the archive.The archival mission includes three major goals: To identify papers and records that have enduring value, to preserve the identified papers, and to make the papers available to others. [76]There are significant differences between libraries and archives, including differences in collections, records creation, item acquisition, and preferred behavior in the institution. The major difference in collections is that library collections typically comprise published items (books, magazines, etc.), while archival collections are usually unpublished works (letters, diaries, etc.) In managing their collections, libraries will categorize items individually, but archival items never stand alone. An archival record gains its meaning and importance from its relationship to the entire collection; therefore archival items are usually received by the archive in a group or batch. Library collections are created by many individuals, as each author and illustrator creates their own publication; in contrast, an archive usually collects the records of one person, family, institution, or organization, and so the archival items will have fewer source authors. [76]Another difference between a library and an archive, is that library materials are created explicitly by authors or others who are working intentionally. They choose to write and publish a book, for example, and that occurs. Archival materials are not created intentionally. Instead, the items in an archive are what remain after a business, institution, or person conducts their normal business practices.The collection of letters, documents, receipts, ledger books, etc. were created with intention to perform daily tasks, they were not created in order to populate a future archive. [76]As for item acquisition, libraries receive items individually, but archival items will usually become part of the archive's collection as a cohesive group. [76]Behavior in an archive differs from behavior in a library, as well. In most libraries, patrons are allowed and encouraged to browse the stacks, because the books are openly available to the public. Archival items almost never circulate, and someone interested in viewing documents must request them of the archivist and may only view them in a closed reading room.[76] Those who wish to visit an archive will usually begin with an entrance interview. This is an opportunity for the archivist to register the researcher, confirm their identity, and determine their research needs. This is also the opportune time for the archivist to review reading room rules, which vary but typically include policies on privacy, photocopying, the use of finding aids, and restrictions on food, drinks, and other activities or items that could damage the archival materials. [76]Special library[edit]Special libraries and special librarians include almost any other form of librarianship, including those who serve in medical libraries (and hospitals or medical schools), corporations, news agencies, government organizations, or other special collections. The issues at these libraries are specific to the industries they inhabit, but may include solo work, corporate financing, specialized collection development, and extensive self-promotion to potential patrons. Special librarians have their own professional organization, the Special Library Association.National Center for Atmospheric Research (NCAR)[77] is considered a special library. Its mission is to support, preserve, make accessible, and collaborate in the scholarly research and educational outreach activities of UCAR/NCAR.Another is the Federal Bureau of Investigations Library.[78] According to its website, The FBI Library supports the FBI in its statutory mission to uphold the law through investigation of violations of federal criminal law; to protect the United States from foreign intelligence and terrorist activities; and to provide leadership and law enforcement assistance to federal, state, local, and international agencies.Preservation[edit]Preservation librarians most often work in academic libraries. Their focus is on the management of preservation activities that seek to maintain access to content within books, manuscripts, archival materials, and other library resources. Examples of activities managed by preservation librarians include binding, conservation, digital and analog reformatting, digital preservation, and environmental monitoring.Library Associations[edit]\nWikipedia List of Library Associations\nFurther reading[edit]\nInternational Journal of Library Science (ISSN 0975-7546)\nLafontaine, Gerard S. (1958). Dictionary of Terms Used in the Paper, Printing, and Allied Industries. Toronto: H. Smith Paper Mills. 110 p.\nThe Oxford Guide to Library Research (2005) \u2013 ISBN 0-19-518998-1\nThompson, Elizabeth H. (1943). A.L.A. Glossary of Library Terms, with a Selection of Terms in Related Fields, prepared under the direction of the Committee on Library Terminology of the American Library Association. Chicago, Ill.: American Library Association. viii, 189 p. SBN 8389-0000-3\nV-LIB 1.2 (2008 Vartavan Library Classification, over 700 fields of sciences & arts classified according to a relational philosophy, currently sold under license in the UK by Rosecastle Ltd. (see http://rosecastle.atspace.com/index_files/Page382.htm)\nSee also[edit]External links[edit]\nLibrarianship Studies & Information Technology - library and information science blog\nVisualizing library and information science from the practitioner's perspective\nLISNews.org \u2013 librarian and information science news\nLISWire.com \u2013 librarian and information science wire\nLibrary and Information Science at Curlie (based on DMOZ)\nThe Library of Alexandria : centre of learning in the ancient world\nHistory[edit]\nJefferson's Library \u2013 exhibition including a sample page from Catalog of Library of Thomas Jefferson\nChronology of information science and technology \u2013 from the 17th to the 20th century\nChronology of chemical information science\nInformation science pioneers \u2013 biographies of pioneers and famous information scientists\nThe Historiography of Canadian Library History\nNew York Public Library \u2013 one of the greatest public libraries in the world\nMount Gambier Public Library \u2013 one of the best libraries in Australia\nThe Bibliotheca Alexandria \u2013 a major library and cultural center located on the shore of the Mediterranean Sea in the Egyptian city of Alexandria\nThe Howe Library in Hanover, New Hampshire, United States\nReferences[edit]Other references[edit]\nPanjab Library Primer full text. \n", "subtitles": ["History", "Education and Training", "Employment Outlook and Opportunities", "Gender and library science in the United States", "Diversity in librarianship", "Theory and practice of library science", "Types of libraries", "Library Associations", "Further reading", "See also", "External links", "References"], "title": "Library science"},
{"content": "Memory safety is the state of being protected from various software bugs and security vulnerabilities when dealing with memory access, such as buffer overflows and dangling pointers.[1] For example, Java is said to be memory-safe because its runtime error detection checks array bounds and pointer dereferences.[1] In contrast, C and C++ support arbitrary pointer arithmetic, with no provision for bounds checking,[2] and thus are termed memory-unsafe.[3]History[edit]Memory errors were first considered in the context of resource management and time-sharing systems, in an effort to avoid problems such as fork bombs.[4] Developments were mostly theoretical until the Morris worm, which exploited a buffer overflow in fingerd.[5] The field of computer security developed quickly thereafter, escalating with multitudes of new attacks such as the return-to-libc attack and defense techniques such as the non-executable stack[6] and address space layout randomization. Randomization prevents most buffer overflow attacks and requires the attacker to use heap spraying or other application-dependent methods to obtain addresses, although its adoption has been slow.[5] However, deployments of the technology are typically limited to randomizing libraries and the location of the stack.Approaches[edit]DieHard,[7] its redesign DieHarder,[8] and the Allinea Distributed Debugging Tool are special heap allocators that allocate objects in their own random virtual memory page, allowing invalid reads and writes to be stopped and debugged at the exact instruction that causes them. Protection relies upon hardware memory protection and thus overhead is typically not substantial, although it can grow significantly if the program makes heavy use of allocation.[9] Randomization provides only probabilistic protection against memory errors, but can often be easily implemented in existing software by relinking the binary.The memcheck tool of Valgrind uses an instruction set simulator and runs the compiled program in a memory-checking virtual machine, providing guaranteed detection of a subset of runtime memory errors. However, it typically slows the program down by a factor of 40,[10] and furthermore must be explicitly informed of custom memory allocators.[11][12]With access to the source code, libraries exist that collect and track legitimate values for pointers (metadata) and check each pointer access against the metadata for validity, such as the Boehm garbage collector.[13] In general, memory safety can be safely assured using tracing garbage collection and the insertion of runtime checks on every memory access; this approach has overhead, but less than that of Valgrind. All garbage-collected languages take this approach.[1] For C and C++, many tools exist that perform a compile-time transformation of the code to do memory safety checks at runtime, such as CheckPointer[14] and AddressSanitizer which imposes an average slowdown factor of 2[15].Another approach uses static program analysis and automated theorem proving to ensure that the program is free of memory errors. For example, the Rust programming language implements a borrow checker to ensure memory safety.[16] Tools such as Coverity offer static memory analysis for C.[17] C++'s support for smart pointers is a limited form of this approach.Types of memory errors[edit]Many different types of memory errors can occur:[18][19]\nAccess errors: invalid read/write of a pointer\n\nBuffer overflow - out-of-bound writes can corrupt the content of adjacent objects, or internal data (like bookkeeping information for the heap) or return addresses.\nBuffer over-read - out-of-bound reads can reveal sensitive data or help attackers bypass address space layout randomization.\nRace condition - concurrent reads/writes to shared memory\nInvalid page fault - accessing a pointer outside the virtual memory space. A null pointer dereference will often cause an exception or program termination in most environments, but can cause corruption in operating system kernels or systems without memory protection, or when use of the null pointer involves a large or negative offset.\nUse after free - dereferencing a dangling pointer storing the address of an object that has been deleted.\n\n\nUninitialized variables - a variable that has not been assigned a value is used. It may contain an undesired or, in some languages, a corrupt value.\n\nNull pointer dereference - dereferencing an invalid pointer or a pointer to memory that has not been allocated\nWild pointers arise when a pointer is used prior to initialization to some known state. They show the same erratic behaviour as dangling pointers, though they are less likely to stay undetected.\n\n\nMemory leak - when memory usage is not tracked or tracked incorrectly\n\nStack exhaustion - occurs when a program runs out of stack space, typically because of too deep recursion. A guard page typically halts the program, preventing memory corruption, but functions with large stack frames may bypass the page.\nHeap exhaustion - the program tries to allocate more memory than the amount available. In some languages, this condition must be checked for manually after each allocation.\nDouble free - repeated calls to free may prematurely free a new object at the same address. If the exact address has not been reused, other corruption may occur, especially in allocators that use free lists.\nInvalid free - passing an invalid address to free can corrupt the heap.\nMismatched free - when multiple allocators are in use, attempting to free memory with a deallocation function of a different allocator[20]\nUnwanted aliasing - when the same memory location is allocated and modified twice for unrelated purposes.\n\n\nReferences[edit]", "subtitles": ["History", "Approaches", "Types of memory errors", "References"], "title": "Memory safety"},
{"content": "Templates are a feature of the C++ programming language that allows functions and classes to operate with generic types. This allows a function or class to work on many different data types without being rewritten for each one.Templates are of great utility to programmers in C++, especially when combined with multiple inheritance and operator overloading. The C++ Standard Library provides many useful functions within a framework of connected templates.Major inspirations for C++ templates were the parameterized modules provided by CLU and the generics provided by Ada.[1]Technical overview[edit]There are three kinds of templates: function templates, class templates and, since C++14, variable templates. Since C++11, templates may be either variadic or non-variadic; in earlier versions of C++ they are always non-variadic.Function templates[edit]A function template behaves like a function except that the template can have arguments of many different types (see example). In other words, a function template represents a family of functions. The format for declaring function templates with type parameters is:Both expressions have the same meaning and behave in exactly the same way. The latter form was introduced to avoid confusion,[2] since a type parameter need not be a class. (It can also be a basic type such as int or double.)For example, the C++ Standard Library contains the function template max(x, y) which returns the larger of x and y. That function template could be defined like this:This single function definition works with many data types. Specifically, it works with all data types for which > (the greater-than operator) is defined. The usage of a function template saves space in the source code file in addition to limiting changes to one function description and making the code easier to read.A template does not produce smaller object code, though, compared to writing separate functions for all the different data types used in a specific program. For example, if a program uses both an int and a double version of the max() function template shown above, the compiler will create an object code version of max() that operates on int arguments and another object code version that operates on double arguments. The compiler output will be identical to what would have been produced if the source code had contained two separate non-templated versions of max(), one written to handle int and one written to handle double.Here is how the function template could be used:In the first two cases, the template argument Type is automatically deduced by the compiler to be int and double, respectively. In the third case automatic deduction of max(3, 7.0) would fail because the type of the parameters must in general match the template arguments exactly. Therefore, we explicitly instantiate the double version with max<double>().This function template can be instantiated with any copy-constructible type for which the expression y > x is valid. For user-defined types, this implies that the greater-than operator (>) must be overloaded in the type.Class templates[edit]A class template provides a specification for generating classes based on parameters. Class templates are generally used to implement containers. A class template is instantiated by passing a given set of types to it as template arguments.[3] The C++ Standard Library contains many class templates, in particular the containers adapted from the Standard Template Library, such as vector.Variable templates[edit]In C++14, templates can be also used for variables, as in the following example:Template specialization[edit]When a function or class is instantiated from a template, a specialization of that template is created by the compiler for the set of arguments used, and the specialization is referred to as being a generated specialization.Explicit template specialization[edit]Sometimes, the programmer may decide to implement a special version of a function (or class) for a given set of template type arguments which is called an explicit specialization. In this way certain template types can have a specialized implementation that is optimized for the type or more meaningful implementation than the generic implementation.\nIf a class template is specialized by a subset of its parameters it is called partial template specialization (function templates cannot be partially specialized).\nIf all of the parameters are specialized it is a full specialization.\nExplicit specialization is used when the behavior of a function or class for particular choices of the template parameters must deviate from the generic behavior: that is, from the code generated by the main template, or templates. For example, the template definition below defines a specific implementation of max() for arguments of type bool:Variadic templates[edit]C++11 introduced variadic templates, which can take a variable number of arguments in a manner somewhat similar to variadic functions such as std::printf. Both function templates and class templates can be variadic.Advantages and disadvantages of templates over macros[edit]Some uses of templates, such as the max() function mentioned above, were previously fulfilled by function-like preprocessor macros. For example, the following is a C++ max() macro that evaluates to the maximum of its two arguments as defined by the < operator:Both macros and templates are expanded at compile time. Macros are always expanded inline, while templates are only expanded inline when the compiler deems it appropriate. When expanded inline, macro functions and function templates have no extraneous runtime overhead. Template functions with many lines of code will incur runtime overhead when they are not expanded inline, but the reduction in code size may help the code to fit into the CPU's instruction cache.Macro arguments are not evaluated prior to expansion. The expression using the macro defined abovemay evaluate to a negative number (because std::rand() will be called twice as specified in the macro, using different random numbers for comparison and output respectively), while the call to template functionwill always evaluate to a non-negative number.As opposed to macros, templates are considered type-safe; that is, they require type-checking at compile time. Hence, the compiler can determine at compile time whether the type associated with a template definition can perform all of the functions required by that template definition.By design, templates can be utilized in very complex problem spaces, whereas macros are substantially more limited.There are fundamental drawbacks to the use of templates:\nHistorically, some compilers exhibited poor support for templates. So, the use of templates could decrease code portability.\nMany compilers lack clear instructions when they detect a template definition error. This can increase the effort of developing templates, and has prompted the development of Concepts for possible inclusion in a future C++ standard.\nSince the compiler generates additional code for each template type, indiscriminate use of templates can lead to code bloat, resulting in larger executables.\nBecause a template by its nature exposes its implementation, injudicious use in large systems can lead to longer build times.\nIt can be difficult to debug code that is developed using templates. Since the compiler replaces the templates, it becomes difficult for the debugger to locate the code at runtime.\nTemplates of templates (nested templates) are not supported by all compilers, or might have a limit on the nesting level.\nTemplates are in the headers, which require a complete rebuild of all project pieces when changes are made.\nNo information hiding. All code is exposed in the header file. No one library can solely contain the code.\nAdditionally, the use of the less than and greater than signs as delimiters is problematic for tools (such as text editors) which analyze source code syntactically. It is difficult for such tools to determine whether a use of these tokens is as comparison operators or template delimiters. For example, this line of code:may be a function call with two parameters, each the result of a comparison expression, or possibly a function call with one parameter, utilizing the C++ comma operator (whose end result would depend on possible side effects of a, b, c, and/or d). Alternatively, it could be a declaration of a constructor for class foo taking a parameter d whose type is the parameterized a < b, c >.Generic programming features in other languages[edit]Initially, the concept of templates was not included in some languages, such as Java and C# 1.0. Java's adoption of generics mimics the behavior of templates, but is technically different. C# added generics (parameterized types) in .NET 2.0. The generics in Ada predate C++ templates.Although C++ templates, Java generics, and .NET generics are often considered similar, generics only mimic the basic behavior of C++ templates.[4] Some of the advanced template features utilized by libraries such as Boost and STLSoft, and implementations of the STL itself, for template metaprogramming (explicit or partial specialization, default template arguments, template non-type arguments, template template arguments, ...) are not available with generics.In C++ templates, compile-time cases were historically performed by pattern matching over the template arguments. For example, the template base class in the Factorial example below is implemented by matching 0 rather than with an inequality test, which was previously unavailable. However, the arrival in C++11 of standard library features such as std::conditional has provided another, more flexible way to handle conditional template instantiation.With these definitions, one can compute, say 6! at compile time using the expression Factorial<6>::value. Alternatively, constexpr in C++11 can be used to calculate such values directly using a function at compile-time.See also[edit]\nTemplate metaprogramming\nMetaprogramming\nGeneric programming\nSubstitution failure is not an error\nCuriously recurring template pattern\nReferences[edit]External links[edit]\nDemonstration of the Turing-completeness of C++ templates (Lambda calculus implementation)\n", "subtitles": ["Technical overview", "Advantages and disadvantages of templates over macros", "Generic programming features in other languages", "See also", "References", "External links"], "title": "Template (C++)"},
{"content": "In computing, type introspection is the ability of a program to examine the type or properties of an object at runtime. Some programming languages possess this capability.Introspection should not be confused with reflection, which goes a step further and is the ability for a program to manipulate the values, meta-data, properties and/or functions of an object at runtime. Some programming languages - e.g. Java, Python and Go - also possess that capability.Examples[edit]Ruby[edit]Type introspection is a core feature of Ruby. In Ruby, the Object class (ancestor of every class) provides Object#instance_of? and Object#kind_of? methods for checking the instance's class. The latter returns true when the particular instance the message was sent to is an instance of a descendant of the class in question. For example, consider the following example code (you can immediately try this with the Interactive Ruby Shell):In the example above, the Class class is used as any other class in Ruby. Two classes are created, A and B, the former is being a superclass of the latter, then one instance of each class is checked. The last expression gives true because A is a superclass of the class of b.Further, you can directly ask for the class of any object, and compare them (code below assumes having executed the code above):Objective-C[edit]In Objective-C, for example, both the generic Object and NSObject (in Cocoa/OpenStep) provide the method isMemberOfClass: which returns true if the argument to the method is an instance of the specified class. The method isKindOfClass: analogously returns true if the argument inherits from the specified class.For example, say we have an Apple and Orange class inheriting from Fruit.Now, in the eat method we can writeNow, when eat is called with a generic object (an id), the function will behave correctly depending on the type of the generic object.C++[edit]C++ supports type introspection via the run-time type information (RTTI) typeid and dynamic_cast keywords. The dynamic_cast expression can be used to determine whether a particular object is of a particular derived class. For instance:The typeid operator retrieves a std::type_info object describing the most derived type of an object:Object Pascal[edit]Type introspection has been a part of Object Pascal since the original release of Delphi, which uses RTTI heavily for visual form design. In Object Pascal, all classes descend from the base TObject class, which implements basic RTTI functionality. Every class's name can be referenced in code for RTTI purposes; the class name identifier is implemented as a pointer to the class's metadata, which can be declared and used as a variable of type TClass. The language includes an is operator, to determine if an object is or descends from a given class, an as operator, providing a type-checked typecast, and several TObject methods. More deeper introspection (enumerating fields and methods) are traditionally only supported for objects declared in the $M+ (a pragma) state, typically TPersistent, and only for symbols defined in the published section. Delphi 2010 increased this to nearly all symbols.Java[edit]The simplest example of type introspection in Java is the instanceof[1] operator. The instanceof operator determines whether a particular object belongs to a particular class (or a subclass of that class, or a class that implements that interface). For instance:The java.lang.Class[2] class is the basis of more advanced introspection.For instance, if it is desirable to determine the actual class of an object (rather than whether it is a member of a particular class), Object.getClass() and Class.getName() can be used:PHP[edit]In PHP introspection can be done using instanceof operator. For instance:Perl[edit]Introspection can be achieved using the ref and isa functions in Perl.We can introspect the following classes and their corresponding instances:using:Meta-Object Protocol[edit]Much more powerful introspection in Perl can be achieved using the Moose object system[3] and the Class::MOP meta-object protocol,[4] for example this is how you can check if a given object does a role X:This is how you can list fully qualified names of all of the methods that can be invoked on the object, together with the classes in which they were defined:Python[edit]The most common method of introspection in Python is using the dir function to detail the attributes of an object. For example:Also, the built-in functions type and isinstance can be used to determine what an object is while hasattr can determine what an object does. For example:In Python 2 but not Python 3, declaring class Foo instead of class Foo(object) will result in type returning the generic instance type instead of the class.[5]ActionScript (as3)[edit]In ActionScript the function flash.utils.getQualifiedClassName can be used to retrieve the Class/Type name of an arbitrary Object.Or alternatively in actionscipt the operator is can be used to determine if an object is of a specific typeThis second function can be used to test class inheritance parents as wellMeta-Type introspection[edit]Like perl, actionscript can go further than getting the Class Name, but all the metadata, functions and other elements that make up an object using the flash.utils.describeType function, this is used when implementing reflection in actionscript.See also[edit]\nReflection (computer science)\nReification (computer science)\nReferences[edit]External links[edit]\nIntrospection on Rosetta Code\n", "subtitles": ["Examples", "See also", "References", "External links"], "title": "Type introspection"},
{"content": "In functional programming, a generalized algebraic data type (GADT, also first-class phantom type,[1] guarded recursive datatype,[2] or equality-qualified type[3]) is a generalization of parametric algebraic data types.Overview[edit]In a GADT, the product constructors (called data constructors in Haskell) can provide an explicit instantiation of the ADT as the type instantiation of their return value. This allows one to define functions with a more advanced type behaviour. For a data constructor of Haskell 98, the return value has the type instantiation implied by the instantiation of the ADT parameters at the constructor's application.They are currently implemented in the GHC compiler as a non-standard extension, used by, among others, Pugs and Darcs. OCaml supports GADT natively since version 4.00.[4]The GHC implementation provides support for existentially quantified type parameters and for local constraints.History[edit]An early version of generalized algebraic data types were described by Augustsson & Petersson (1994) and based on pattern matching in ALF.Generalized algebraic data types were introduced independently by Cheney & Hinze (2003) and prior by Xi, Chen & Chen (2003) as extensions to ML's and Haskell's algebraic data types.[5] Both are essentially equivalent to each other. They are similar to the inductive families of data types (or inductive datatypes) found in Coq's Calculus of Inductive Constructions and other dependently typed languages, modulo the dependent types and except that the latter have an additional positivity restriction which is not enforced in GADTs.[6]Sulzmann, Wazny & Stuckey (2006) introduced extended algebraic data types which combine GADTs together with the existential data types and type class constraints introduced by Perry (1991), La\u0308ufer & Odersky (1994) and La\u0308ufer (1996).Type inference in the absence of any programmer supplied type annotations is undecidable[7] and functions defined over GADTs do not admit principal types in general.[8] Type reconstruction requires several design trade-offs and is an area of active research (Peyton Jones, Washburn & Weirich 2004; Peyton Jones et al. 2006; Pottier & Re\u0301gis-Gianas 2006; Sulzmann, Schrijvers & Stuckey 2006; Simonet & Pottier 2007; Schrijvers et al. 2009; Lin & Sheard 2010a; Lin & Sheard 2010b; Vytiniotis, Peyton Jones & Schrijvers 2010; Vytiniotis et al. 2011).Applications[edit]Applications of GADTs include generic programming, modelling programming languages (higher-order abstract syntax), maintaining invariants in data structures, expressing constraints in embedded domain-specific languages, and modelling objects.[9]Higher-order abstract syntax[edit]An important application of GADTs is to embed higher-order abstract syntax in a type safe fashion. Here is an embedding of the simply typed lambda calculus with an arbitrary collection of base types, tuples and a fixed point combinator:And a type safe evaluation function:The factorial function can now be written as:We would have run into problems using regular algebraic data types. Dropping the type parameter would have made the lifted base types existentially quantified, making it impossible to write the evaluator. With a type parameter we would still be restricted to a single base type. Furthermore, ill-formed expressions such as App (Lam (\\x -> Lam (\\y -> App x y))) (Lift True) would have been possible to construct, while they are type incorrect using the GADT. A well-formed analogue is App (Lam (\\x -> Lam (\\y -> App x y))) (Lift (\\z -> True)). This is because the type of x is Lam (a -> b), inferred from the type of the Lam data constructor.See also[edit]\nType variable\nNotes[edit]Further reading[edit]External links[edit]\nGeneralised Algebraic Datatype Page on the Haskell wiki\nGeneralised Algebraic Data Types in the GHC Users' Guide\nGeneralized Algebraic Data Types and Object-Oriented Programming\nGADTs \u2013 Haskell Prime \u2013 Trac\nPapers about type inference for GADTs, bibliography by Simon Peyton Jones\nType inference with constraints, bibliography by Simon Peyton Jones\nEmulating GADTs in Java via the Yoneda lemma\n", "subtitles": ["Overview", "History", "Applications", "See also", "Notes", "Further reading", "External links"], "title": "Generalized algebraic data type"},
{"content": "The Association for Computing Machinery (ACM) is an international learned society for computing. It was founded in 1947, and is the world's largest[1] scientific and educational computing society. It is a not-for-profit professional membership group.[2] Its membership is more than 100,000 as of 2011. Its headquarters are in New York City.The ACM is an umbrella organization for academic and scholarly interests in computer science. Its motto is Advancing Computing as a Science & Profession.History[edit]The ACM was founded in 1947 under the name Eastern Association for Computing Machinery, which was changed the following year to the Association of Computing Machinery.[3]Activities[edit]ACM is organized into over 171 local chapters and 37 Special Interest Groups (SIGs), through which it conducts most of its activities. Additionally, there are over 500 college and university chapters. The first student chapter was founded in 1961 at the University of Louisiana at Lafayette.Many of the SIGs, such as SIGGRAPH, SIGPLAN, SIGCSE and SIGCOMM, sponsor regular conferences, which have become famous as the dominant venue for presenting innovations in certain fields. The groups also publish a large number of specialized journals, magazines, and newsletters.ACM also sponsors other computer science related events such as the worldwide ACM International Collegiate Programming Contest (ICPC), and has sponsored some other events such as the chess match between Garry Kasparov and the IBM Deep Blue computer.Services[edit]Publications[edit]ACM publishes over 50 journals[4] including the prestigious[5] Journal of the ACM, and two general magazines for computer professionals, Communications of the ACM (also known as Communications or CACM) and Queue. Other publications of the ACM include:\nACM XRDS, formerly Crossroads, was redesigned in 2010 and is the most popular student computing magazine in the US.\nACM Interactions, an interdisciplinary HCI publication focused on the connections between experiences, people and technology, and the third largest ACM publication.[6]\nACM Computing Surveys (CSUR)\nACM Computers in Entertainment (CIE)\nACM Special Interest Group: Computers and Society (SIGCAS) [7]\nA number of journals, specific to subfields of computer science, titled ACM Transactions. Some of the more notable transactions include:\n\nACM Transactions on Computer Systems (TOCS)\nIEEE/ACM Transactions on Computational Biology and Bioinformatics (TCBB)\nACM Transactions on Computational Logic (TOCL)\nACM Transactions on Computer-Human Interaction (TOCHI)\nACM Transactions on Database Systems (TODS)\nACM Transactions on Graphics (TOG)\nACM Transactions on Mathematical Software (TOMS)\nACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)\nIEEE/ACM Transactions on Networking (TON)\nACM Transactions on Programming Languages and Systems (TOPLAS)\n\n\nAlthough Communications no longer publishes primary research, and is not considered a prestigious venue, many of the great debates and results in computing history have been published in its pages.ACM has made almost all of its publications available to paid subscribers online at its Digital Library and also has a Guide to Computing Literature. Individual members additionally have access to Safari Books Online and Books24x7. ACM also offers insurance, online courses, and other services to its members.In 1997, ACM Press published Wizards and Their Wonders: Portraits in Computing (ISBN 0897919602), written by Christopher Morgan, with new photographs by Louis Fabian Bachrach. The book is a collection of historic and current portrait photographs of figures from the computer industry.Portal and Digital Library[edit]The ACM Portal is an online service of the ACM.[8] Its core are two main sections: ACM Digital Library and the ACM Guide to Computing Literature.[9]The ACM Digital Library is the full-text collection of all articles published by the ACM in its articles, magazines and conference proceedings. The Guide is a bibliography in computing with over one million entries.[8] The ACM Digital Library contains a comprehensive archive starting in the 1950s of the organization's journals, magazines, newsletters and conference proceedings. Online services include a forum called Ubiquity and Tech News digest. There is an extensive underlying bibliographic database containing key works of all genres from all major publishers of computing literature. This secondary database is a rich discovery service known as The ACM Guide to Computing Literature.ACM adopted a hybrid Open Access (OA) publishing model in 2013. Authors who do not choose to pay the OA fee must grant ACM publishing rights by either a copyright transfer agreement or a publishing license agreement.[10]ACM was a green publisher before the term was invented. Authors may post documents on their own websites and in their institutional repositories with a link back to the ACM Digital Library's permanently maintained Version of Record.All metadata in the Digital Library is open to the world, including abstracts, linked references and citing works, citation and usage statistics, as well as all functionality and services. Other than the free articles, the full-texts are accessed by subscription.There is also a mounting challenge to the ACM's publication practices coming from the open access movement. Some authors see a centralized peer\u2013review process as less relevant and publish on their home pages or on unreviewed sites like arXiv. Other organizations have sprung up which do their peer review entirely free and online, such as Journal of Artificial Intelligence Research (JAIR), Journal of Machine Learning Research (JMLR) and the Journal of Research and Practice in Information Technology.Membership grades[edit]In addition to student and regular members, ACM has several advanced membership grades to recognize those with multiple years of membership and demonstrated performance that sets them apart from their peers.[11]Fellows[edit]The ACM Fellows Program was established by Council of the Association for Computing Machinery in 1993 to recognize and honor outstanding ACM members for their achievements in computer science and information technology and for their significant contributions to the mission of the ACM. There are presently[when?] about 958 Fellows[12] out of about 75,000 professional members.Distinguished Members[edit]In 2006 ACM began recognizing two additional membership grades, one which was called Distinguished Members. Distinguished Members (Distinguished Engineers, Distinguished Scientists, and Distinguished Educators) have at least 15 years of professional experience and 5 years of continuous ACM membership and have made a significant impact on the computing field. Note that in 2006 when the Distinguished Members first came out, one of the three levels was called Distinguished Member and was changed about two years later to Distinguished Educator. Those who already had the Distinguished Member title had their titles changed to one of the other three titles.Senior Members[edit]Also in 2006, ACM began recognizing Senior Members. Senior Members have ten or more years of professional experience and 5 years of continuous ACM membership.Distinguished Speakers[edit]While not technically a membership grade, the ACM recognizes distinguished speakers on topics in computer science. A distinguished speaker is appointed for a three-year period. There are usually about 125 current distinguished speakers. The ACM website describes these people as 'Renowned International Thought Leaders'.[13] The distinguished speaker program is overseen by a committee [14]Norman E. Gibbs served as the president of the ACM.Chapters[edit]ACM has three kinds of chapters: Special Interest Groups,[15] Professional Chapters, and Student Chapters.[16]As of 2011, ACM has professional & SIG Chapters in 56 countries.[17]As of 2014, there exist ACM student chapters in 41 different countries.[18]Special Interest Groups[edit]Conferences[edit]ACM and its Special Interest Groups (SIGs) sponsors numerous conferences with 170 hosted worldwide in 2017. ACM Conferences page has an up-to-date complete list while a partial list is shown below. Most of the SIGs also have an annual conference. ACM conferences are often very popular publishing venues and are therefore very competitive. For example, the 2007 SIGGRAPH conference attracted about 30000 visitors, and CIKM only accepted 15% of the long papers that were submitted in 2005.\nMobiHoc: International Symposium on Mobile Ad Hoc Networking and Computing\nThe ACM is a co\u2013presenter and founding partner of the Grace Hopper Celebration of Women in Computing (GHC) with the Anita Borg Institute for Women and Technology.[25]There are some conferences hosted by ACM student branches; this includes Reflections Projections, which is hosted by UIUC ACM.[citation needed][26] . In addition, ACM sponsors regional conferences. Regional conferences facilitate increased opportunities for collaboration between nearby institutions and they are well attended.For additional non-ACM conferences, see this list of computer science conferences.Awards[edit]The ACM presents or co\u2013presents a number of awards for outstanding technical and professional achievements and contributions in computer science and information technology.[27][28][29]Over 30 of ACM's Special Interest Groups also award individuals for their contributions with a few listed below.[32]Leadership[edit]The President of ACM for 2016\u20132018 is Vicki L. Hanson, Distinguished Professor in the Department of Information Sciences and Technologies at the Rochester Institute of Technology and Professor and Chair of Inclusive Technologies at the University of Dundee, UK. She is successor of Alexander L. Wolf (2014\u20132016), Dean of the Jack Baskin School of Engineering at the University of California, Santa Cruz; Vint Cerf (2012\u20132014), an American computer scientist who is recognized as one of the fathers of the Internet; Alain Chesnais (2010\u20132012), a French citizen living in Toronto, Ontario, Canada, where he runs his company named Visual Transitions; and Dame Wendy Hall of the University of Southampton, UK (2008\u20132010).[33]ACM is led by a Council consisting of the President, Vice-President, Treasurer, Past President, SIG Governing Board Chair, Publications Board Chair, three representatives of the SIG Governing Board, and seven Members\u2013At\u2013Large. This institution is often referred to simply as Council in Communications of the ACM.Infrastructure[edit]ACM has five Boards that make up various committees and subgroups, to help Headquarters staff maintain quality services and products. These boards are as follows:\nPublications Board\nSIG Governing Board\nEducation Board\nMembership Services Board\nPractitioners Board\nACM Council on Women in Computing[edit]ACM-W,[34] the ACM council on women in computing, supports, celebrates, and advocates internationally for the full engagement of women in computing. ACM\u2013W's main programs are regional celebrations of women in computing, ACM-W chapters, and scholarships for women CS students to attend research conferences. In India and Europe these activities are overseen by ACM-W India and ACM-W Europe respectively. ACM-W collaborates with organizations such as the Anita Borg Institute, the National Center for Women & Information Technology (NCWIT), and Committee on the Status of Women in Computing Research (CRA-W).Athena Lectures[edit]The ACM-W gives an annual Athena Lecturer Award to honor outstanding women researchers who have made fundamental contributions to computer science.[35] This program began in 2006. Speakers are nominated by SIG officers.[36]\n2006\u20132007: Deborah Estrin of UCLA\n2007\u20132008: Karen Spa\u0308rck Jones of Cambridge University\n2008\u20132009: Shafi Goldwasser of MIT and the Weitzmann Institute of Science\n2009\u20132010: Susan J. Eggers of the University of Washington\n2010\u20132011: Mary Jane Irwin of the Pennsylvania State University\n2011\u20132012: Judith S. Olson of the University of California, Irvine\n2012\u20132013: Nancy Lynch of MIT\n2013\u20132014: Katherine Yelick of LBNL[37]\n2014\u20132015: Susan Dumais of Microsoft Research\n2015\u20132016: Jennifer Widom of Stanford University\n2016\u20132017: Jennifer Rexford of Princeton University\nCooperation[edit]ACM's primary partner has been the IEEE Computer Society (IEEE-CS), which is the largest subgroup of the Institute of Electrical and Electronics Engineers (IEEE). The IEEE focuses more on hardware and standardization issues than theoretical computer science, but there is considerable overlap with ACM's agenda. They have many joint activities including conferences, publications and awards.[38] ACM and its SIGs co-sponsor about 20 conferences each year with IEEE-CS and other parts of IEEE.[39] Eckert-Mauchly Award and Ken Kennedy Award, both major awards in computer science, are given jointly by ACM and the IEEE-CS.[40] They occasionally cooperate on projects like developing computing curricula.[41]ACM has also jointly sponsored on events with other professional organizations like the Society for Industrial and Applied Mathematics (SIAM).[42]See also[edit]References[edit]External links[edit]\nOfficial website\nACM portal for publications\nACM Digital Library\nAssociation for Computing Machinery Records, 1947-2009, Charles Babbage Institute, University of Minnesota.\n", "subtitles": ["History", "Activities", "Services", "Portal and Digital Library", "Membership grades", "Chapters", "Conferences", "Awards", "Leadership", "Infrastructure", "ACM Council on Women in Computing", "Cooperation", "See also", "References", "External links"], "title": "Association for Computing Machinery"},
{"content": "BIBSYS is an administrative agency set up and organized by the Ministry of Education and Research in Norway. They are a service provider, focusing on the exchange, storage and retrieval of data pertaining to research, teaching and learning \u2013 historically metadata related to library resources.BIBSYS are collaborating with all Norwegian universities and university colleges as well as research institutions and the National Library of Norway.[1][2] Bibsys is formally organized as a unit at the Norwegian University of Science and Technology (NTNU), located in Trondheim, Norway. The board of directors is appointed by Norwegian Ministry of Education and Research.BIBSYS offer researchers, students and others an easy access to library resources by providing the unified search service Oria.no and other library services. [3] They also deliver integrated products for the internal operation for research and special libraries as well as open educational resources. [4]As a DataCite member BIBSYS act as a national DataCite representative in Norway and thereby allow all of Norway's higher education and research institutions to use DOI on their research data.All their products and services are developed in cooperation with their member institutions.History[edit]BIBSYS began in 1972 as a collaborative project between the Royal Norwegian Society of Sciences and Letters Library (Det Kongelige Norske Videnskabers Selskabs Bibliotek), the Norwegian Institute of Technology Library and the Computer Centre at the Norwegian Institute of Technology. The purpose of the project was to automate internal library routines. Since 1972 Bibsys has evolved from a library system supplier for two libraries in Trondheim, to developing and operating a national library system for Norwegian research and special libraries. The target group has also expanded to include the customers of research and special libraries, by providing them easy access to library resources.BIBSYS is a public administrative agency answerable to the Ministry of Education and Research, and administratively organised as a unit at NTNU. In addition to BIBSYS Library System, the product portfolio consists of BISBYS Ask, BIBSYS Brage, BIBSYS Galleri and BIBSYS Tyr. All operation of applications and databases is performed centrally by BIBSYS. BIBSYS also offer a range of services, both in connection with their products and separate services independent of the products they supply.See also[edit]\nOpen access in Norway\nReferences[edit]External links[edit]\nOm Bibsys (in Norwegian)\n", "subtitles": ["History", "See also", "References", "External links"], "title": "Bibsys"},
{"content": "In computing, reactive programming is a declarative programming paradigm concerned with data streams and the propagation of change. This means that it becomes possible to express static (e.g. arrays) or dynamic (e.g. event emitters) data streams with ease via the employed programming language(s), and that an inferred dependency within the associated execution model exists, which facilitates the automatic propagation of the change involved with data flow.For example, in an imperative programming setting, \n  \n    \n      \n        a\n        :=\n        b\n        +\n        c\n      \n    \n    {\\displaystyle a:=b+c}\n  \n would mean that \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  \n is being assigned the result of \n  \n    \n      \n        b\n        +\n        c\n      \n    \n    {\\displaystyle b+c}\n  \n in the instant the expression is evaluated, and later, the values of \n  \n    \n      \n        b\n      \n    \n    {\\displaystyle b}\n  \n and/or \n  \n    \n      \n        c\n      \n    \n    {\\displaystyle c}\n  \n can be changed with no effect on the value of \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  \n. However, in reactive programming, the value of \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  \n is automatically updated whenever the values of \n  \n    \n      \n        b\n      \n    \n    {\\displaystyle b}\n  \n and/or \n  \n    \n      \n        c\n      \n    \n    {\\displaystyle c}\n  \n change; without the program having to re-execute the sentence \n  \n    \n      \n        a\n        :=\n        b\n        +\n        c\n      \n    \n    {\\displaystyle a:=b+c}\n  \n to determine the presently assigned value of \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  \n.Another example is a hardware description language such as Verilog, where reactive programming enables changes to be modeled as they propagate through circuits. Consequently, reactive programming has been proposed as a way to simplify the creation of interactive user interfaces, (near) real time system animation, but is constituted essentially as a general programming paradigm.For example, in a model\u2013view\u2013controller (MVC) architecture, reactive programming can facilitate changes in an underlying model that automatically are reflected in an associated view, and contrarily.[1]Definition of Reactive Programming[edit]Quoting Ge\u0301rard Berry:[2]\nIt is convenient to distinguish roughly between three kinds of computer programs. Transformational programs compute results from a given set of inputs; typical examples are compilers or numerical computation programs. Interactive programs interact at their own speed with users or with other programs; from a user point of view, a time-sharing system is interactive. Reactive programs also maintain a continuous interaction with their environment, but at a speed which is determined by the environment, not the program itself. Interactive programs work at their own pace and mostly deal with communication, while reactive programs only work in response to external demands and mostly deal with accurate interrupt handling. Real-time programs are usually reactive. However, there are reactive programs that are not usually considered as being real-time, such as protocols, system drivers, or man-machine interface handlers.\nApproaches to Creating Reactive Programming Languages[edit]Several popular approaches exist which are employed in the creation of reactive programming languages. Specification of dedicated languages which are specific to various domain constraints. Such constraints usually are characterized by real-time, embedded computing or hardware description. Another approach involves the specification of general-purpose languages which include support for reactivity. Other approaches are articulated in the definement, and use of programming libraries, or embedded domain-specific languages, that enable reactivity alongside or on top of, the employed general-purpose programming language used to specify an application. Specification, and use of these different approaches results in language capability trade-offs. In general, the more restricted a language is characterized as having to be, the more its associated compilers and analysis tools are then able to inform its using developers (e.g., in performing analysis for whether programs are able to execute in actual real time). Functional trades-offs involving specificity, then result in deterioration of a language's general applicability.Programming Models and Semantics[edit]A variety of models and semantics govern the family of reactive programming. We can loosely split them along the following dimensions:\nSynchrony: is the underlying model of time synchronous versus asynchronous?\nDeterminism: Deterministic versus non-deterministic in both evaluation process and results (the former does not necessarily imply the latter)\nUpdate process: callbacks versus dataflow versus actors\nImplementation Techniques and Challenges[edit]Essence of Implementations[edit]Reactive programming language runtimes are represented by a graph which exemplifies the dependencies among the involved reactive values. In such a graph, nodes represent the act of computating and edges model dependency relationships. Such a language's runtime employs said graph, to help it keep track of the various computations, which must be executed anew, once an involved input changes value.Change Propagation Algorithms[edit]Various abstract implementation approaches exist which enable the specification of reactive programming characterized by the representation, and facilitation of data flow through its system. The flow of data is explicitly described with employment of such a graph. The most common algorithms are:\npull\npush\nhybrid push-pull\nWhat to push?[edit]At the implementation level, event reaction consists of the propagation across a graph's information, which characterizes the existence of change. Consequently, computations that are affected by such change, then become outdated, and must be flagged for re-execution. Such computations, are then usually characterized by the transitive closure of the change in its associated source. Change propagation may then lead to an update in the value of the graph's sinks.Graph propagated information can consist of a node's complete state, i.e., the computation result of the involved node. In such cases, the node's previous output is then ignored. Another method involves delta propagation i.e. incremental change propagation. In this case, information is proliferated along a graph's edges which consist only of deltas describing how the previous node was changed. This approach is especially important when nodes hold large amounts of state data, which would otherwise be expensive to recompute from scratch.Delta propagation is essentially an optimization which has been extensively studied via the discipline of incremental computing, whose approach requires runtime satisfaction involving the view-update problem. This problem is infamously characterized by the use of database entities, which are responsible for the maintenance of changing data views.Another common optimization is employment of unary change accumulation and batch propagation. Such a solution can be faster because it reduces communication among involved nodes. Optimization strategies can then be employed which reason about the nature of the changes contained within, and make alterations accordingly. e.g. two changes in the batch can cancel each other, and thus, simply be ignored. Yet another available approach, is described as invalidity notification propagation. This approach causes nodes with invalid input to pull updates, thus resulting in the update of their own outputs.There are two principal ways employed in the building of a dependency graph:\nThe graph of dependencies are maintained implicitly within an event loop. Registration of explicit callbacks, then results in the creation of implicit dependencies. Therefore, control inversion, which is induced via callback, is thus left in place. However, making callbacks functional (i.e. returning state value instead of unit value) necessitates that such callbacks become compositional.\nA graph of dependencies is program-specific and generated by a programmer. This facilitates an addressing of the callback's control inversion in two ways: either a graph is specified explicitly (typically using a Domain-specific language (DSL), which may be embedded), or a graph is implicitly defined with expression and generation using an effective, archetypal language.\nImplementation Challenges in Reactive Programming[edit]Glitches[edit]When propagating changes, it is possible to pick propagation orders such that the value of an expression is not a natural consequence of the source program. We can illustrate this easily with an example. Suppose seconds is a reactive value that changes every second to represent the current time (in seconds). Consider this expression:\nt = seconds + 1\ng = (t > seconds)\nBecause t should always be greater than seconds, this expression should always evaluate to a true value. Unfortunately, this can depend on the order of evaluation. When seconds changes, two expressions have to update: seconds + 1 and the conditional. If the first evaluates before the second, then this invariant will hold. If, however, the conditional updates first, using the old value of t and the new value of seconds, then the expression will evaluate to a false value. This is called a glitch.Some reactive languages are glitch-free, and prove this property[citation needed]. This is usually achieved by topologically sorting expressions and updating values in topological order. This can, however, have performance implications, such as delaying the delivery of values (due to the order of propagation). In some cases, therefore, reactive languages permit glitches, and developers must be aware of the possibility that values may temporarily fail to correspond to the program source, and that some expressions may evaluate multiple times (for instance, t > seconds may evaluate twice: once when the new value of seconds arrives, and once more when t updates).Cyclic Dependencies[edit]Topological sorting of dependencies depends on the dependency graph being a directed acyclic graph (DAG). In practice, a program may define a dependency graph that has cycles. Usually, reactive programming languages expect such cycles to be broken by placing some element along a back edge to permit reactive updating to terminate. Typically, languages provide an operator like delay that is used by the update mechanism for this purpose, since a delay implies that what follows must be evaluated in the next time step (allowing the current evaluation to terminate).Interaction with Mutable State[edit]Reactive languages typically assume that their expressions are purely functional. This allows an update mechanism to choose different orders in which to perform updates, and leave the specific order unspecified (thereby enabling optimizations). When a reactive language is embedded in a programming language with state, however, it may be possible for programmers to perform mutable operations. How to make this interaction smooth remains an open problem.In some cases, it is possible to have principled partial solutions. Two such solutions include:\nA language might offer a notion of mutable cell. A mutable cell is one that the reactive update system is aware of, so that changes made to the cell propagate to the rest of the reactive program. This enables the non-reactive part of the program to perform a traditional mutation while enabling reactive code to be aware of and respond to this update, thus maintaining the consistency of the relationship between values in the program. An example of a reactive language that provides such a cell is FrTime.[3]\nProperly encapsulated object-oriented libraries offer an encapsulated notion of state. In principle, it is therefore possible for such a library to interact smoothly with the reactive portion of a language. For instance, callbacks can be installed in the getters of the object-oriented library to notify the reactive update engine about state changes, and changes in the reactive component can be pushed to the object-oriented library through getters. FrTime employs such a strategy.[4]\nDynamic Updating of the Graph of Dependencies[edit]In some reactive languages, the graph of dependencies is static, i.e., the graph is fixed throughout the program's execution. In other languages, the graph can be dynamic, i.e., it can change as the program executes. For a simple example, consider this illustrative example (where seconds is a reactive value):\nt =\n  if ((seconds mod 2) == 0):\n    seconds + 1\n  else:\n    seconds - 1\n  end\nt + 1\nEvery second, the value of this expression changes to a different reactive expression, which t + 1 then depends on. Therefore, the graph of dependencies updates every second.Permitting dynamic updating of dependencies provides significant expressive power (for instance, dynamic dependencies routinely occur in graphical user interface (GUI) programs). However, the reactive update engine must decide whether to reconstruct expressions each time, or to keep an expression's node constructed but inactive; in the latter case, ensure that they do not participate in the computation when they are not supposed to be active.Concepts[edit]Degrees of explicitness[edit]Reactive programming languages can range from very explicit ones where data flows are set up by using arrows, to implicit where the data flows are derived from language constructs that look similar to those of imperative or functional programming. For example, in implicitly lifted functional reactive programming (FRP) a function call might implicitly cause a node in a data flow graph to be constructed. Reactive programming libraries for dynamic languages (such as the Lisp Cells and Python Trellis libraries) can construct a dependency graph from runtime analysis of the values read during a function's execution, allowing data flow specifications to be both implicit and dynamic.Sometimes the term reactive programming refers to the architectural level of software engineering, where individual nodes in the data flow graph are ordinary programs that communicate with each other.Static or Dynamic[edit]Reactive programming can be purely static where the data flows are set up statically, or be dynamic where the data flows can change during the execution of a program.The use of data switches in the data flow graph could to some extent make a static data flow graph appear as dynamic, and blur the distinction slightly. True dynamic reactive programming however could use imperative programming to reconstruct the data flow graph.Higher-order reactive programming[edit]Reactive programming could be said to be of higher order if it supports the idea that data flows could be used to construct other data flows. That is, the resulting value out of a data flow is another data flow graph that is executed using the same evaluation model as the first.Data flow differentiation[edit]Ideally all data changes are propagated instantly, but this cannot be assured in practice. Instead it might be necessary to give different parts of the data flow graph different evaluation priorities. This can be called differentiated reactive programming.[5]For example, in a word processor the marking of spelling errors need not be totally in sync with the inserting of characters. Here differentiated reactive programming could potentially be used to give the spell checker lower priority, allowing it to be delayed while keeping other data-flows instantaneous.However, such differentiation introduces additional design complexity. For example, deciding how to define the different data flow areas, and how to handle event passing between different data flow areas.Evaluation models of reactive programming[edit]Evaluation of reactive programs is not necessarily based on how stack based programming languages are evaluated. Instead, when some data is changed, the change is propagated to all data that is derived partially or completely from the data that was changed. This change propagation could be achieved in a number of ways, where perhaps the most natural way is an invalidate/lazy-revalidate scheme.It could be problematic simply to naively propagate a change using a stack, because of potential exponential update complexity if the data structure has a certain shape. One such shape can be described as repeated diamonds shape, and has the following structure: An\u2192Bn\u2192An+1, An\u2192Cn\u2192An+1, where n=1,2... This problem could be overcome by propagating invalidation only when some data is not already invalidated, and later re-validate the data when needed using lazy evaluation.One inherent problem for reactive programming is that most computations that would be evaluated and forgotten in a normal programming language, needs to be represented in the memory as data-structures.[citation needed] This could potentially make reactive programming highly memory consuming. However, research on what is called lowering could potentially overcome this problem.[6]On the other side, reactive programming is a form of what could be described as explicit parallelism, and could therefore be beneficial for utilizing the power of parallel hardware.Similarities with observer pattern[edit]Reactive programming has principal similarities with the observer pattern commonly used in object-oriented programming. However, integrating the data flow concepts into the programming language would make it easier to express them and could therefore increase the granularity of the data flow graph. For example, the observer pattern commonly describes data-flows between whole objects/classes, whereas object-oriented reactive programming could target the members of objects/classes.The stack-based evaluation model of common object orientation is also not entirely suitable for data-flow propagation, as occurrences of tree feedback edges in the data structures could make the program face exponential complexities. But because of its relatively limited use and low granularity, this is rarely a problem for the observer pattern in practice.Approaches[edit]Imperative[edit]It is possible to fuse reactive programming with ordinary imperative programming. In such a paradigm, imperative programs operate upon reactive data structures.[7] Such a set-up is analogous to constraint imperative programming; however, while constraint imperative programming manages bidirectional constraints, reactive imperative programming manages one-way dataflow constraints.Object-oriented[edit]Object-oriented reactive programming (OORP) is a combination of object oriented programming and reactive programming. Perhaps the most natural way to make such a combination is as follows: Instead of methods and fields, objects have reactions that automatically re-evaluate when the other reactions they depend on have been modified.[citation needed]Below is an illustration of the A=X+Y introductory example using JavaScript and jQuery:[8]If an OORP programming language maintains its imperative methods, it would also fall under the category of imperative reactive programming.Functional[edit]Functional reactive programming (FRP) is a programming paradigm for reactive programming on functional programming.Examples[edit]Spreadsheets[edit]A modern spreadsheet is often cited as an example of reactive programming. This is problematic because the unqualified term spreadsheet may refer to either:\nThe underlying collection of cells, where each cell contains either a literal value or a formula that refers to other cells such as =B1+C1. This table of cells is effectively a computer program that determines how a set of output cells are computed from a set of input cells. This may be saved to a file, and this is often referred to as a spreadsheet (e.g. the budget spreadsheet).\nThe interactive application program with a graphical user interface that is used to edit and evaluate the underlying table of cells from (1). In virtually all spreadsheet applications, interactively changing any one cell on the sheet will result in immediately re-evaluating all formulas that directly or indirectly depend on that cell and updating the display to reflect these re-evaluations.\nConfusion arises because the spreadsheet application (2) is an example of a reactive program, while the program effectively defined by the underlying spreadsheet (1) is typically not itself a reactive program.[citation needed] Semantically, the underlying spreadsheet (1) simply determines a calculation from a set of input cells to a set of output cells, and thus could be directly translated to a simple transformational calculation (i.e. function) in a traditional programming language.See also[edit]\nRxJava Code Examples\nRxScala Code Examples\nRxSwift Framework Implementation and Notes\nReactiveChart Reactive Modul Impl online example how reactive modul javascript implementation propagate changes in reactive chart]\nService Component Architecture\nMeteor (web framework)\nQML\nElm (programming language) Reactive composition of web user interface.\nReact, a JavaScript library written by Facebook for creating user interfaces\nReactive Streams, a JVM standard for asynchronous stream processing with non-blocking backpressure\nAkka Streams, an implementation of Reactive Streams for Scala and Java\nShiny - reactive programming in R\nReferences[edit]External links[edit]\nRx Visualizer - Animated playground for Rx Observables\nReactive Marble diagrams examples\nReactiveUI A MVVM framework that integrates with the Reactive Extensions for .NET to create elegant, testable User Interfaces that run on any mobile or desktop platform. Supports Xamarin.iOS, Xamarin.Android, Xamarin.Mac, Xamarin Forms, WPF, Windows Forms, Windows Phone 8, Windows Store and Universal Windows Platform (UWP).\nMeteor.js A full-stack, JavaScript-based, reactive web application development platform.\nA survey on reactive programming A paper by E. Bainomugisha, A. Lombide Carreton, T. Van Cutsem, S. Mostinckx, and W. De Meuter that surveys and provides a taxonomy of existing reactive programming approaches.\nMIMOSA Project of INRIA - ENSMP, a general site about reactive programming.\nExperimenting with Cells Demonstration of a simple reactive programming application in Lisp, using the Cells library\nREScala Reactive programming for OO applications.\nReactive Programming in .NET Microsoft's Reactive Extensions (Rx) homepage\nReactive Extensions for Swift RxSwift homepage\nDeprecating the Observer Pattern A 2010 paper by Ingo Maier, Tiark Rompf and Martin Odersky outlining a reactive programming framework for the Scala programming language.\nDeprecating the Observer Pattern with Scala.React A 2012 paper by Ingo Maier and Martin Odersky.\nThe Reactive Manifesto\nReactors IO, A framework for building distributed systems by combining ideas from Functional Reactive Programming and the Actor model\ncellx, an implementation of reactivity for JavaScript\nRxJS, the Reactive Extensions library for composing asynchronous [...] programs using observable sequences\nProAct.js, a Reactive Programming JavaScript library, integrating both the functional and object-oriented implementation approaches.\nReacto, reactive programming for Ruby with some concurrency thrown into the mix.\nReactor, Reactor is a second-generation Reactive library for building non-blocking applications on the JVM based on the Reactive Streams Specification.\nShiny, a web application framework for R (programming language) based on reactive programming.\n", "subtitles": ["Definition of Reactive Programming", "Approaches to Creating Reactive Programming Languages", "Programming Models and Semantics", "Implementation Techniques and Challenges", "Concepts", "Approaches", "Examples", "See also", "References", "External links"], "title": "Reactive programming"},
{"content": "This article describes the features in Haskell.Examples[edit]Factorial[edit]A simple example that is often used to demonstrate the syntax of functional languages is the factorial function for non-negative integers, shown in Haskell:Or in one line:This describes the factorial as a recursive function, with one terminating base case. It is similar to the descriptions of factorials found in mathematics textbooks. Much of Haskell code is similar to standard mathematical notation in facility and syntax.The first line of the factorial function describes the type of this function; while it is optional, it is considered to be good style[1] to include it. It can be read as the function factorial (factorial) has type (::) from integer to integer (Integer -> Integer). That is, it takes an integer as an argument, and returns another integer. The type of a definition is inferred automatically if the programmer didn't supply a type annotation.The second line relies on pattern matching, an important feature of Haskell. Note that parameters of a function are not in parentheses but separated by spaces. When the function's argument is 0 (zero) it will return the integer 1 (one). For all other cases the third line is tried. This is the recursion, and executes the function again until the base case is reached.Using the product function from the Prelude, a number of small functions analogous to C's standard library, and using the Haskell syntax for arithmetic sequences, the factorial function can be expressed in Haskell as follows:Here [1..n] denotes the arithmetic sequence 1, 2, ..., n in list form. Using the Prelude function enumFromTo, the expression [1..n] can be written as enumFromTo 1 n, allowing the factorial function to be expressed aswhich, using the function composition operator (expressed as a dot in Haskell) to compose the product function with the curried enumeration function can be rewritten in point-free style:[2]In the Hugs interpreter, one often needs to define the function and use it on the same line separated by a where or let..in. For example, to test the above examples and see the output 120:orThe GHCi interpreter doesn't have this restriction and function definitions can be entered on one line (with the let syntax without the in part), and referenced later.More complex examples[edit]Calculator[edit]In the Haskell source immediately below, :: can be read as has type; a \u2014> b can be read as is a function from a to b. (Thus the Haskell calc :: String \u2014> [Float] can be read as calc has type of function from Strings to lists of Floats.) In the second line calc = ...  the equals sign can be read as can be; thus multiple lines with calc = ...  can be read as multiple possible values for calc, depending on the circumstance detailed in each line.A simple Reverse Polish notation calculator expressed with the higher-order function foldl whose argument f is defined in a where clause using pattern matching and the type class Read:The empty list is the initial state, and f interprets one word at a time, either as a function name, taking two numbers from the head of the list and pushing the result back in, or parsing the word as a floating-point number and prepending it to the list.Fibonacci sequence[edit]The following definition produces the list of Fibonacci numbers in linear time:The infinite list is produced by corecursion \u2014 the latter values of the list are computed on demand starting from the initial two items 0 and 1. This kind of a definition relies on lazy evaluation, an important feature of Haskell programming. For an example of how the evaluation evolves, the following illustrates the values of fibs and tail fibs after the computation of six items and shows how zipWith (+) has produced four items and proceeds to produce the next item:\nfibs         = 0 : 1 : 1 : 2 : 3 : 5 : ...\n               +   +   +   +   +   +\ntail fibs    = 1 : 1 : 2 : 3 : 5 : ...\n               =   =   =   =   =   =\nzipWith ...  = 1 : 2 : 3 : 5 : 8 : ...\nfibs = 0 : 1 : 1 : 2 : 3 : 5 : 8 : ...\nThe same function, written using GHC's parallel list comprehension syntax (GHC extensions must be enabled using a special command-line flag, here -XParallelListComp, or by starting the source file with {-# LANGUAGE ParallelListComp #-}):or with regular list comprehensions:or directly self-referencing:With stateful generating function:or with unfoldr:or scanl:Using data recursion with Haskell's predefined fixpoint combinator:Factorial[edit]The factorial we saw previously can be written as a sequence of functions:More examples[edit]Hamming numbers[edit]A remarkably concise function that returns the list of Hamming numbers in order:Like the various fibs solutions displayed above, this uses corecursion to produce a list of numbers on demand, starting from the base case of 1 and building new items based on the preceding part of the list. Here the function union is used as an operator by enclosing it in back-quotes. Its case clauses define how it merges two ascending lists into one ascending list without duplicate items, representing sets as ordered lists. Its companion function minus implements set difference:It is possible to generate only the unique multiples, for more efficient operation. Since there are no duplicates, there's no need to remove them:This uses the more efficient function merge which doesn't concern itself with the duplicates (also used in the following next function, mergesort ):Each vertical bar ( | ) starts a guard clause with a guard expression before the = sign and the corresponding definition after it, that is evaluated if the guard is true.Mergesort[edit]Here is a bottom-up merge sort, defined using the higher-order function until:Prime numbers[edit]The mathematical definition of primes can be translated pretty much word for word into Haskell:This finds primes by trial division. Note that it is not optimized for efficiency and has very poor performance. Slightly faster (but still unreasonably slow[3]) is the famous code by David Turner:Much faster is the optimal trial division algorithmor an unbounded sieve of Eratosthenes with postponed sieving in stages,[4]or the combined sieve implementation by Richard Bird,[5]or an even faster tree-like folding variant[6] with nearly optimal (for a list-based code) time complexity and very low space complexity achieved through telescoping multistage recursive production of primes:Working on arrays by segments between consecutive squares of primes, it's (the fastest here)The shortest possible code is probably  nubBy (((>1).).gcd) [2..].  It is quite slow.Syntax[edit]Layout[edit]Haskell allows indentation to be used to indicate the beginning of a new declaration. For example, in a where clause:The two equations for the nested function prod are aligned vertically, which allows the semi-colon separator to be omitted. In Haskell, indentation can be used in several syntactic constructs, including do, let, case, class, and instance.The use of indentation to indicate program structure originates in Landin's ISWIM language, where it was called the off-side rule. This was later adopted by Miranda, and Haskell adopted a similar (but rather more complicated) version of Miranda's off-side rule, which is called layout. Other languages to adopt whitespace-sensitive syntax include Python and F#.The use of layout in Haskell is optional. For example, the function product above can also be written:The explicit open brace after the where keyword indicates that the programmer has opted to use explicit semi-colons to separate declarations, and that the declaration-list will be terminated by an explicit closing brace. One reason for wanting support for explicit delimiters is that it makes automatic generation of Haskell source code easier.Haskell's layout rule has been criticised for its complexity. In particular, the definition states that if the parser encounters a parse error during processing of a layout section, then it should try inserting a close brace (the parse error rule). Implementing this rule in a traditional parsing/lexical-analysis combination requires two-way cooperation between the parser and lexical analyser, whereas in most languages these two phases can be considered independently.Function calls[edit]Applying a function f to a value x is expressed as simply f x.Haskell distinguishes function calls from infix operators syntactically, but not semantically. Function names which are composed of punctuation characters can be used as operators, as can other function names if surrounded with backticks; and operators can be used in prefix notation if surrounded with parentheses.This example shows the ways that functions can be called:Functions which are defined as taking several parameters can always be partially applied. Binary operators can be partially applied using section notation:List comprehensions[edit]See List comprehension#Overview for the Haskell example.Pattern matching[edit]Pattern matching is used to match on the different constructors of algebraic data types. Here are some functions, each using pattern matching on each of the types above:Using the above functions, along with the map function, we can apply them to each element of a list, to see their results:\nAbstract Types\nLists\nTuples[edit]Tuples in haskell can be used to hold a fixed number of elements. They are used to group pieces of data of differing types:Tuples are commonly used in the zip* functions to place adjacent elements in separate lists together in tuples (zip4 to zip7 are provided in the Data.List module):In the GHC compiler, tuples are defined with sizes from 2 elements up to 62 elements.\nRecords\nNamespaces[edit]In the #More_complex_examples section above, calc is used in two senses, showing that there is a Haskell type class namespace and also a namespace for values:\na Haskell type class for calc. The domain and range can be explicitly denoted in a Haskell type class.\na Haskell value, formula, or expression for calc.\nTypeclasses and polymorphism[edit]Algebraic data types[edit]Algebraic data types are used extensively in Haskell. Some examples of these are the built in list, Maybe and Either types:Users of the language can also define their own abstract data types. An example of an ADT used to represent a person's name, sex and age might look like:Type system[edit]\nType classes\nType defaulting\nOverloaded Literals\nHigher Kinded Polymorphism\nMulti-Parameter Type Classes\nFunctional Dependencies\nMonads and input/output[edit]\nOverview of the monad framework\nApplications\n\nMonadic IO\nDo-notation\nReferences\nExceptions\n\n\nST monad[edit]The ST monad allows programmers to write imperative algorithms in Haskell, using mutable variables (STRef's) and mutable arrays (STArrays and STUArrays). The advantage of the ST monad is that it allows programmers to write code that has internal side effects, such as destructively updating mutable variables and arrays, while containing these effects inside the monad. The result of this is that functions written using the ST monad appear completely pure to the rest of the program. This allows programmers to produce imperative code where it may be impractical to write functional code, while still keeping all the safety that pure code provides.Here is an example program (taken from the Haskell wiki page on the ST monad) that takes a list of numbers, and sums them, using a mutable variable:STM monad[edit]The STM monad is an implementation of Software Transactional Memory in Haskell. It is implemented in the GHC compiler, and allows for mutable variables to be modified in transactions.Arrows[edit]\nApplicative Functors\nArrows\nAs Haskell is a pure functional language, functions cannot have side effects. Being non-strict, it also does not have a well-defined evaluation order. This is a challenge for real programs, which among other things need to interact with an environment. Haskell solves this with monadic types that leverage the type system to ensure the proper sequencing of imperative constructs. The typical example is I/O, but monads are useful for many other purposes, including mutable state, concurrency and transactional memory, exception handling, and error propagation.Haskell provides a special syntax for monadic expressions, so that side-effecting programs can be written in a style similar to current imperative programming languages; no knowledge of the mathematics behind monadic I/O is required for this. The following program reads a name from the command line and outputs a greeting message:The do-notation eases working with monads. This do-expression is equivalent to, but (arguably) easier to write and understand than, the de-sugared version employing the monadic operators directly:\nSee also wikibooks:Transwiki:List of hello world programs#Haskell for another example that prints text.\nConcurrency[edit]The Haskell language definition itself does not include either concurrency or parallelism, although GHC supports both.Concurrent Haskell is an extension to Haskell that provides support for threads and synchronization.[7] GHC's implementation of Concurrent Haskell is based on multiplexing lightweight Haskell threads onto a few heavyweight OS threads,[8] so that Concurrent Haskell programs run in parallel on a multiprocessor. The runtime can support millions of simultaneous threads.[9]The GHC implementation employs a dynamic pool of OS threads, allowing a Haskell thread to make a blocking system call without blocking other running Haskell threads.[10] Hence the lightweight Haskell threads have the characteristics of heavyweight OS threads, and the programmer is unaware of the implementation details.Recently, Concurrent Haskell has been extended with support for Software Transactional Memory (STM), which is a concurrency abstraction in which compound operations on shared data are performed atomically, as transactions.[11] GHC's STM implementation is the only STM implementation to date to provide a static compile-time guarantee preventing non-transactional operations from being performed within a transaction. The Haskell STM library also provides two operations not found in other STMs: retry and orElse, which together allow blocking operations to be defined in a modular and composable fashion.References[edit]", "subtitles": ["Examples", "Syntax", "Namespaces", "Typeclasses and polymorphism", "Monads and input/output", "Concurrency", "References"], "title": "Haskell features"},
{"content": "Symmetric multiprocessing (SMP) involves a multiprocessor computer hardware and software architecture where two or more identical processors are connected to a single, shared main memory, have full access to all input and output devices, and are controlled by a single operating system instance that treats all processors equally, reserving none for special purposes. Most multiprocessor systems today use an SMP architecture. In the case of multi-core processors, the SMP architecture applies to the cores, treating them as separate processors.SMP systems are tightly coupled multiprocessor systems with a pool of homogeneous processors running independently of each other. Each processor, executing different programs and working on different sets of data, has the capability of sharing common resources (memory, I/O device, interrupt system and so on) that are connected using a system bus or a crossbar.Design[edit]SMP systems have centralized shared memory called main memory (MM) operating under a single operating system with two or more homogeneous processors. Usually each processor has an associated private high-speed memory known as cache memory (or cache) to speed up the main memory data access and to reduce the system bus traffic.Processors may be interconnected using buses, crossbar switches or on-chip mesh networks. The bottleneck in the scalability of SMP using buses or crossbar switches is the bandwidth and power consumption of the interconnect among the various processors, the memory, and the disk arrays. Mesh architectures avoid these bottlenecks, and provide nearly linear scalability to much higher processor counts at the sacrifice of programmability:\nSerious programming challenges remain with this kind of architecture because it requires two distinct modes of programming; one for the CPUs themselves and one for the interconnect between the CPUs. A single programming language would have to be able to not only partition the workload, but also comprehend the memory locality, which is severe in a mesh-based architecture.[1]\nSMP systems allow any processor to work on any task no matter where the data for that task is located in memory, provided that each task in the system is not in execution on two or more processors at the same time. With proper operating system support, SMP systems can easily move tasks between processors to balance the workload efficiently.History[edit]The earliest production system with multiple identical processors was the Burroughs B5000, which was functional around 1961. However at run-time this was asymmetric, with one processor restricted to application programs while the other processor mainly handled the operating system and hardware interrupts. The Burroughs D825 first implemented SMP in 1962.[2][3]IBM offered dual-processor computer systems based on its System/360 model 65 and the closely related model 67[4] and 67-2.[5] The operating systems that ran on these machines were OS/360 M65MP[6] and TSS/360. Other software developed at universities, notably the Michigan Terminal System (MTS), used both CPUs. Both processors could access data channels and initiate I/O. In OS/360 M65MP, peripherals could generally be attached to either processor since the operating system kernel ran on both processors (though with a big lock around the I/O handler).[7] The MTS supervisor (UMMPS) has the ability to run on both CPUs of the IBM System/360 model 67-2. Supervisor locks were small and used to protect individual common data structures that might be accessed simultaneously from either CPU.[8]Other mainframes that supported SMP included the UNIVAC 1108 II, released in 1965, which supported up to three CPUs, and the GE-635 and GE-645,[9][10] although GECOS on multiprocessor GE-635 systems ran in a master-slave asymmetric fashion, unlike Multics on multiprocessor GE-645 systems, which ran in a symmetric fashion.[11]Starting with its version 7.0 (1972), Digital Equipment Corporation's operating system TOPS-10 implemented the SMP feature, the earliest system running SMP was the DECSystem 1077 dual KI10 processor system.[12] Later KL10 system could aggregate up to 8 CPUs in a SMP manner. In contrast, DECs first multi-processor VAX system, the VAX-11/782, was asymmetric,[13] but later VAX multiprocessor systems were SMP.[14]Early Unix SMP implementations included the Sequent Computer Systems Balance 8000 (released in 1984) and Balance 21000 (released in 1986).[15] Both models were based on 10 MHz National Semiconductor NS32032 processors, each with a small write-through cache connected to a common memory to form a shared memory system. Another early commercial Unix SMP implementation was the NUMA based Honeywell Information Systems Italy XPS-100 designed by Dan Gielan of VAST Corporation in 1985. Its design supported up to 14 processors, but due to electrical limitations, the largest marketed version was a dual processor system. The operating system was derived and ported by VAST Corporation from AT&T 3B20 Unix SysVr3 code used internally within AT&T.Uses[edit]Time-sharing and server systems can often use SMP without changes to applications, as they may have multiple processes running in parallel, and a system with more than one process running can run different processes on different processors.On personal computers, SMP is less useful for applications that have not been modified. If the system rarely runs more than one process at a time, SMP is useful only for applications that have been modified for multithreaded (multitasked) processing. Custom-programmed software can be written or modified to use multiple threads, so that it can make use of multiple processors.Multithreaded programs can also be used in time-sharing and server systems that support multithreading, allowing them to make more use of multiple processors.Advantages/Disadvantages[edit]In SMP, all of the processors are tightly coupled inside the same box with a bus or switch. Some of the components that are shared are global memory, disks, and I/O devices. Only one copy of an OS runs on all the processors, and the OS must be designed to take advantage of this architecture. Some of the basic advantages involves cost-effective ways to increase throughput. To solve different problems and tasks, SMP applies multiple processors to that one problem, known as parallel programming.However, there are a few limits on the scalability of SMP due to cache coherence and shared objects.Programming[edit]Uniprocessor and SMP systems require different programming methods to achieve maximum performance. Programs running on SMP systems may experience an increase in performance even when they have been written for uniprocessor systems. This is because hardware interrupts usually suspends program execution while the kernel that handles them can execute on an idle processor instead. The effect in most applications (e.g. games) is not so much a performance increase as the appearance that the program is running much more smoothly. Some applications, particularly building software and some distributed computing projects, run faster by a factor of (nearly) the number of additional processors. (Compilers by themselves are single threaded, but, when building a software project with multiple compilation units, if each compilation unit is handled independently, this creates an embarrassingly parallel situation across the entire multi-compilation-unit project, allowing near linear scaling of compilation time. Distributed computing projects are inherently parallel by design.)Systems programmers must build support for SMP into the operating system, otherwise, the additional processors remain idle and the system functions as a uniprocessor system.SMP systems can also lead to more complexity regarding instruction sets. A homogeneous processor system typically requires extra registers for special instructions such as SIMD (MMX, SSE, etc.), while a heterogeneous system can implement different types of hardware for different instructions/uses.Performance[edit]When more than one program executes at the same time, an SMP system has considerably better performance than a uni-processor, because different programs can run on different CPUs simultaneously. Similarly, Asymmetric multiprocessing (AMP) usually allows only one processor to run a program or task at a time. For example, AMP can be used in assigning specific tasks to CPU based to priority and importance of task completion. AMP was created well before SMP in terms of handling multiple CPUs, which explains the lack of performance based on the example provided.In cases where an SMP environment processes many jobs, administrators often experience a loss of hardware efficiency. Software programs have been developed to schedule jobs so that the processor utilization reaches its maximum potential. Good software packages can achieve this maximum potential by scheduling each CPU separately, as well as being able to integrate multiple SMP machines and clusters.Access to RAM is serialized; this and cache coherency issues causes performance to lag slightly behind the number of additional processors in the system.Alternatives[edit]SMP uses a single shared system bus that represents one of the earliest styles of multiprocessor machine architectures, typically used for building smaller computers with up to 8 processors.Larger computer systems might use newer architectures such as NUMA (Non-Uniform Memory Access), which dedicates different memory banks to different processors. In a NUMA architecture, processors may access local memory quickly and remote memory more slowly. This can dramatically improve memory throughput as long as the data are localized to specific processes (and thus processors). On the downside, NUMA makes the cost of moving data from one processor to another, as in workload balancing, more expensive. The benefits of NUMA are limited to particular workloads, notably on servers where the data are often associated strongly with certain tasks or users.Finally, there is computer clustered multiprocessing (such as Beowulf), in which not all memory is available to all processors. Clustering techniques are used fairly extensively to build very large supercomputers.Variable SMP[edit]Variable Symmetric Multiprocessing (vSMP) is a specific mobile use case technology initiated by NVIDIA. This technology includes an extra fifth core in a quad-core device, called the Companion core, built specifically for executing tasks at a lower frequency during mobile active standby mode, video playback, and music playback.Project Kal-El (Tegra 3),[16] patented by NVIDIA, was the first SoC (System on Chip) to implement this new vSMP technology. This technology not only reduces mobile power consumption during active standby state, but also maximizes quad core performance during active usage for intensive mobile applications. Overall this technology addresses the need for increase in battery life performance during active and standby usage by reducing the power consumption in mobile processors.Unlike current SMP architectures, the vSMP Companion core is OS transparent meaning that the operating system and the running applications are totally unaware of this extra core but are still able to take advantage of it. Some of the advantages of the vSMP architecture includes cache coherency, OS efficiency, and power optimization. The advantages for this architecture are explained below:\nCache Coherency: There are no consequences for synchronizing caches between cores running at different frequencies since vSMP does not allow the Companion core and the main cores to run simultaneously.\nOS Efficiency: It is inefficient when multiple CPU cores are run at different asynchronous frequencies because this could lead to possible scheduling issues.[how?] With vSMP, the active CPU cores will run at similar frequencies to optimize OS scheduling.\nPower Optimization: In asynchronous clocking based architecture, each core is on a different power plane to handle voltage adjustments for different operating frequencies. The result of this could impact performance.[how?] vSMP technology is able to dynamically enable and disable certain cores for active and standby usage, reducing overall power consumption.\nThese advantages lead the vSMP architecture to considerably benefit[peacock term] over other architectures using asynchronous clocking technologies.See also[edit]\nBinary Modular Dataflow Machine\nLocale (computer hardware)\nMassively parallel\nPartitioned global address space\nSimultaneous multithreading \u2013 where functional elements of a CPU core are allocated across multiple threads of execution\nSoftware lockout\nXeon Phi\nReferences[edit]External links[edit]\nHistory of Multi-Processing\nLinux and Multiprocessing\nAMD\n", "subtitles": ["Design", "History", "Uses", "Advantages/Disadvantages", "Programming", "Performance", "Alternatives", "Variable SMP", "See also", "References", "External links"], "title": "Symmetric multiprocessing"},
{"content": "Richard J. Mayer (born 1952) is an American engineer, President of Knowledge Based Systems, Inc., known as lead engineer and principal investigator on the projects of developing part of the IDEF family of modeling languages in the field of software and systems engineering.Mayer received a PhD and worked for years at the Texas A&M University, College Station, TX. He was the lead engineer during the development of the information and data modeling methods IDEF1 and IDEF1X while at Wright Patterson Air Force Base.[1] In 1988 he cofounded Knowledge Based Systems, Inc. (KBSI), College Station, Texas. KBSI was the prime contractor for the Information Integration for Concurrent Engineering (IICE) project, funded by Armstrong Laboratory, Logistics Research Division, Wright-Patterson Air Force Base, Ohio. Richard J. Mayer was the Principal Investigator on the projects of developing IDEF3, IDEF4 en IDEF6.Publications[edit]Mayer has published several books and articles.[2][3] A selection:\n1990. Conflict management : the courage to confront. Columbus, Ohio : Battelle Press. ISBN 0-935470-51-4\n1991. IDEF6: A Design Rationale Capture Method Concept Paper With Patricia A. Griffith & Christopher P. Menzel. Defense Technical Information Center.\n1992. IDEF4 object-oriented design method report. Armstrong Laboratory, Air Force Systems Command.\n1993. Information Integration for Concurrent Engineering (IICE): IDEF3 Process Description Capture Method Report. Logistics Research Division, Wright-Patterson AFB, OH 45433\n1995. Information Integration for Concurrent Engineering (IICE) Compendium of methods report. Wright-Patterson Air Force Base, Ohio 45433-7604.\nReferences[edit]", "subtitles": [], "title": "Richard J. Mayer"},
{"content": "This page indexes the individual year in home video pages. Some years is annotated with a significant event as a reference point.1970s[edit]\n1971 - Year U-matic launched\n1972 - Year Cartrivision launched\n1973\n1974\n1975 - Year Beta launched\n1976 - Year VHS launched\n1977 - Magnetic Video, the first home video company to release theatrical films to tape, licenses 50 films from 20th Century Fox for VHS and Betamax release.\n - August 23 \u2013 VHS is introduced in the United States.[1]\n1978 - Laserdisc player launched.[2] MCA issues Universal Studios film library onto laserdisc, and later adds Warner Bros. and Disney film libraries as well.\n1979 - Paramount Pictures and Columbia Pictures form Home Video Divisions; Video Store Magazine (now Home Media Magazine) established as the industry's first trade.\n1980s[edit]\n1980 - Walt Disney Studios Home Entertainment enters the home video market. MCA, Inc. establishes MCA Videocassette, Inc. for VHS and Betamax releases of the Universal City Studios products.\n1981 - Year CED and Wizard Video launched. Magnetic Video reorganized into Twentieth Century Fox Video. NBC signs video distribution deal through Warner Home Video.\n1982 - PBV Distribution (Publishing & Broadcasting Video) becomes one of the biggest selling Australian video companies. In the mid-80s, it would later become Communications and Entertainment Limited. Star Wars is released on home video-cassette, becoming one of the most demanded videos of all time.\n1983 - JVC launches VHD format in Japan. Following a merger in 1982 between Twentieth Century Fox Video and CBS Video Enterprises, CBS/Fox Video is launched to the home video market, with divisions Key Video and Playhouse Video. In Japan, the Bandai-Emotion label is launched on VHS, and becomes one of the biggest selling Japanese home video labels of all time.\n1984 - The Sony Corp. of America v. Universal City Studios, Inc. decision (aka Betamax case). Following the so-called 'video nasty' British tabloid furore, the Video Recordings Act 1984 is introduced resulting in all feature films having to be certified by the British Board of Film Classification. The pioneering American home video company The Criterion Collection is founded, which introduced letterboxing, commentary tracks and special editions that would become standards for the home video industry.[3][4]\n1985 - Heron Communications launches the Hi-Tops Video label.\n1986 - Video Gems is launched, and becomes a top seller in the UK home video market.\n1987\n1988 - MGM gains home video rights to all United Artists titles.\n1989 - PBS begins video distribution through Pacific Arts.\n1990s[edit]\n1990 - CBS/FOX reorganized. Fox Video is formed to release mainstream Fox product in the US, while CBS/Fox Video remains for other products such as BBC Video and other non-Fox projects.\n1991\n1992 - Fujitsu gets credit for producing the first full-color plasma display panel in 1992. The very first prototype for a plasma display monitor was invented in July 1964 at the University of Illinois by professors Donald Bitzer and Gene Slottow, and then graduate student Robert Willson.\n1993\n1994 - MUSE Hi-Vision LaserDisc is launched on 20 May in Japan. The first consumer high definition video disc.\n1995 - Fox Video assumes the 20th Century Fox Home Entertainment name for the first time, with the release of the Star Wars Trilogy for the last time in its original format, while keeping the Fox Video name.\n1996 - DVD is launched in Japan\n1997 - DVD is launched in the United States.[2] Philips produced the first plasma television to be sold to consumers \u2013 the display was 42-inches diagonally and begged a premium price of $15,000.\n1998 - DVD is launched in Europe and Australia. The CBS/Fox name is dropped.\n1999 - The Fox Video name is dropped. DeCSS is released, opening the doors for large-scale DVD copyright infringment.\n2000s[edit]\n2000\n2001 - The Japanese film Tokyo Raiders becomes the last film released on Laserdisc.\n2002 - D-VHS D-Theater high definition films are finally available and affordable.\n2003 - WMV HD launched with the arrival of the film Standing in the Shadows of Motown.\n2004\n2005\n2006 - HD DVD launched on 18 April; Blu-ray Disc launched on 20 June. After nearly 30 years, VHS ends as a format for major motion pictures;\n2007 - AACS is circumvented.\n2008 - Blu-ray Becomes new video medium after long competition with HD-DVD.\n2009\n2010s[edit]\n2010\n2011\n2012\n2013\n2014\n2015\n2016 4K Ultra HD Blu-Ray launched\n2017\n2018\nReferences[edit]", "subtitles": ["1970s", "1980s", "1990s", "2000s", "2010s", "References"], "title": "List of years in home video"},
{"content": "A spreadsheet is an interactive computer application for organization, analysis and storage of data in tabular form.[1][2][3] Spreadsheets are developed as computerized simulations of paper accounting worksheets.[4] The program operates on data entered in cells of a table. Each cell may contain either numeric or text data, or the results of formulas that automatically calculate and display a value based on the contents of other cells. A spreadsheet may also refer to one such electronic document.[5][6][7]Spreadsheet users can adjust any stored value and observe the effects on calculated values. This makes the spreadsheet useful for what-if analysis since many cases can be rapidly investigated without manual recalculation. Modern spreadsheet software can have multiple interacting sheets, and can display data either as text and numerals, or in graphical form.Besides performing basic arithmetic and mathematical functions, modern spreadsheets provide built-in functions for common financial and statistical operations. Such calculations as net present value or standard deviation can be applied to tabular data with a pre-programmed function in a formula. Spreadsheet programs also provide conditional expressions, functions to convert between text and numbers, and functions that operate on strings of text.Spreadsheets have replaced paper-based systems throughout the business world. Although they were first developed for accounting or bookkeeping tasks, they now are used extensively in any context where tabular lists are built, sorted, and shared.LANPAR, available in 1969,[8] was the first electronic spreadsheet on mainframe and time sharing computers. LANPAR was an acronym: LANguage for Programming Arrays at Random.[8] VisiCalc was the first electronic spreadsheet on a microcomputer,[9] and it helped turn the Apple II computer into a popular and widely used system. Lotus 1-2-3 was the leading spreadsheet when DOS was the dominant operating system.[10] Excel now has the largest market share on the Windows and Macintosh platforms.[11][12][13] A spreadsheet program is a standard feature of an office productivity suite; since the advent of web apps, office suites now also exist in web app form. Web based spreadsheets, such as Microsoft Excel Online or Google Sheets are a relatively new category.Usage[edit]A spreadsheet consists of a table of cells arranged into rows and columns and referred to by the X and Y locations. X locations, the columns, are normally represented by letters, A, B, C, etc., while rows are normally represented by numbers, 1, 2, 3, etc. A single cell can be referred to by addressing its row and column, C10 for instance. This electronic concept of cell references was first introduced in LANPAR (Language for Programming Arrays at Random) (co-invented by Rene Pardo and Remy Landau) and a variant used in VisiCalc, and known as A1 notation. Additionally, spreadsheets have the concept of a range, a group of cells, normally contiguous. For instance, one can refer to the first ten cells in the first column with the range A1:A10. LANPAR innovated forward referencing/natural order calculation which didn't re-appear until Lotus 123 and Microsoft's MultiPlan Version 2.In modern spreadsheet applications, several spreadsheets, often known as worksheets or simply sheets, are gathered together to form a workbook. A workbook is physically represented by a file, containing all the data for the book, the sheets and the cells with the sheets. Worksheets are normally represented by tabs that flip between pages, each one containing one of the sheets, although Numbers changes this model significantly. Cells in a multi-sheet book add the sheet name to their reference, for instance, Sheet 1!C10. Some systems extend this syntax to allow cell references to different workbooks.Users interact with sheets primarily through the cells. A given cell can hold data by simply entering it in, or a formula, which is normally created by preceding the text with an equals sign. Data might include the string of text hello world, the number 5 or the date 16-Dec-91. A formula would begin with the equals sign, =5*3, but this would normally be invisible because the display shows the result of the calculation, 15 in this case, not the formula itself. This may lead to confusion in some cases.The key feature of spreadsheets is the ability for a formula to refer to the contents of other cells, which may in turn be the result of a formula. To make such a formula, one simply replaces a number with a cell reference. For instance, the formula =5*C10 would produce the result of multiplying the value in cell C10 by the number 5. If C10 holds the value 3 the result will be 15. But C10 might also hold its own formula referring to other cells, and so on.The ability to chain formulas together is what gives a spreadsheet its power. Many problems can be broken down into a series of individual mathematical steps, and these can be assigned to individual formulas in cells. Some of these formulas can apply to ranges as well, like the SUM function that adds up all the numbers within a range.Spreadsheets share many principles and traits of databases, but spreadsheets and databases are not the same thing. A spreadsheet is essentially just one table, whereas a database is a collection of many tables with machine-readable semantic relationships between them. While it is true that a workbook that contains three sheets is indeed a file containing multiple tables that can interact with each other, it lacks the relational structure of a database. Spreadsheets and databases are interoperable\u2014sheets can be imported into databases to become tables within them, and database queries can be exported into spreadsheets for further analysis.A spreadsheet program is one of the main components of an office productivity suite, which usually also contains a word processor, a presentation program, and a database management system. Programs within a suite use similar commands for similar functions. Usually sharing data between the components is easier than with a non-integrated collection of functionally equivalent programs. This was particularly an advantage at a time when many personal computer systems used text-mode displays and commands, instead of a graphical user interface.History[edit]Paper spreadsheets[edit]The word spreadsheet came from spread in its sense of a newspaper or magazine item (text or graphics) that covers two facing pages, extending across the center fold and treating the two pages as one large one. The compound word spread-sheet came to mean the format used to present book-keeping ledgers\u2014with columns for categories of expenditures across the top, invoices listed down the left margin, and the amount of each payment in the cell where its row and column intersect\u2014which were, traditionally, a spread across facing pages of a bound ledger (book for keeping accounting records) or on oversized sheets of paper (termed analysis paper) ruled into rows and columns in that format and approximately twice as wide as ordinary paper.[14]Early implementations[edit]Batch spreadsheet report generator[edit]A batch spreadsheet is indistinguishable from a batch compiler with added input data, producing an output report, i.e., a 4GL or conventional, non-interactive, batch computer program. However, this concept of an electronic spreadsheet was outlined in the 1961 paper Budgeting Models and System Simulation by Richard Mattessich.[15] The subsequent work by Mattessich (1964a, Chpt. 9, Accounting and Analytical Methods) and its companion volume, Mattessich (1964b, Simulation of the Firm through a Budget Computer Program) applied computerized spreadsheets to accounting and budgeting systems (on mainframe computers programmed in FORTRAN IV). These batch Spreadsheets dealt primarily with the addition or subtraction of entire columns or rows (of input variables), rather than individual cells.In 1962 this concept of the spreadsheet, called BCL for Business Computer Language, was implemented on an IBM 1130 and in 1963 was ported to an IBM 7040 by R. Brian Walsh at Marquette University, Wisconsin. This program was written in Fortran. Primitive timesharing was available on those machines. In 1968 BCL was ported by Walsh to the IBM 360/67 timesharing machine at Washington State University. It was used to assist in the teaching of finance to business students. Students were able to take information prepared by the professor and manipulate it to represent it and show ratios etc. In 1964, a book entitled Business Computer Language was written by Kimball, Stoffells and Walsh and both the book and program were copyrighted in 1966 and years later that copyright was renewed[16]Applied Data Resources had a FORTRAN preprocessor called Empires.In the late 1960s Xerox used BCL to develop a more sophisticated version for their timesharing system.LANPAR spreadsheet compiler[edit]A key invention in the development of electronic spreadsheets was made by Rene K. Pardo and Remy Landau, who filed in 1970 U.S. Patent 4,398,249 on a spreadsheet automatic natural order calculation algorithm. While the patent was initially rejected by the patent office as being a purely mathematical invention, following 12 years of appeals, Pardo and Landau won a landmark court case at the Predecessor Court of the Federal Circuit (CCPA), overturning the Patent Office in 1983 \u2014 establishing that something does not cease to become patentable merely because the point of novelty is in an algorithm. However, in 1995 the United States Court of Appeals for the Federal Circuit ruled the patent unenforceable.[17]The actual software was called LANPAR3 \u2014 LANguage for Programming Arrays at Random.[18] This was conceived and entirely developed in the summer of 1969, following Pardo and Landau's recent graduation from Harvard University. Co-inventor Rene Pardo recalls that he felt that one manager at Bell Canada should not have to depend on programmers to program and modify budgeting forms, and he thought of letting users type out forms in any order and having an electronic computer calculate results in the right order (Forward Referencing/Natural Order Calculation). Pardo and Landau developed and implemented the software in 1969.[19]LANPAR was used by Bell Canada, AT&T and the 18 operating telephone companies nationwide for their local and national budgeting operations. LANPAR was also used by General Motors. Its uniqueness was Pardo's co-invention incorporating forward referencing/natural order calculation (one of the first non-procedural computer languages)[20] as opposed to left-to-right, top to bottom sequence for calculating the results in each cell that was used by VisiCalc, SuperCalc, and the first version of Multiplan. Without forward referencing/natural order calculation, the user had to manually recalculate the spreadsheet as many times as necessary until the values in all the cells had stopped changing. Forward referencing/natural order calculation by a compiler was the cornerstone functionality required for any spreadsheet to be practical and successful.The LANPAR system was implemented on GE400 and Honeywell 6000 online timesharing systems, enabling users to program remotely via computer terminals and modems. Data could be entered dynamically either by paper tape, specific file access, on line, or even external data bases. Sophisticated mathematical expressions, including logical comparisons and if/then statements, could be used in any cell, and cells could be presented in any order.Autoplan/Autotab spreadsheet programming language[edit]In 1968, three former employees from the General Electric computer company headquartered in Phoenix, Arizona set out to start their own software development house. A. Leroy Ellison, Harry N. Cantrell, and Russell E. Edwards found themselves doing a large number of calculations when making tables for the business plans that they were presenting to venture capitalists. They decided to save themselves a lot of effort and wrote a computer program that produced their tables for them. This program, originally conceived as a simple utility for their personal use, would turn out to be the first software product offered by the company that would become known as Capex Corporation. AutoPlan ran on GE\u2019s Time-sharing service; afterward, a version that ran on IBM mainframes was introduced under the name AutoTab. (National CSS offered a similar product, CSSTAB, which had a moderate timesharing user base by the early 1970s. A major application was opinion research tabulation.)AutoPlan/AutoTab was not a WYSIWYG interactive spreadsheet program, it was a simple scripting language for spreadsheets. The user defined the names and labels for the rows and columns, then the formulas that defined each row or column. In 1975, Autotab-II was advertised as extending the original to a maximum of 1,500 rows and columns, combined in any proportion the user requires...[21]IBM Financial Planning and Control System[edit]The IBM Financial Planning and Control System was developed in 1976, by Brian Ingham at IBM Canada. It was implemented by IBM in at least 30 countries. It ran on an IBM mainframe and was among the first applications for financial planning developed with APL that completely hid the programming language from the end-user. Through IBM's VM operating system, it was among the first programs to auto-update each copy of the application as new versions were released. Users could specify simple mathematical relationships between rows and between columns. Compared to any contemporary alternatives, it could support very large spreadsheets. It loaded actual financial data drawn from the legacy batch system into each user's spreadsheet on a monthly basis. It was designed to optimize the power of APL through object kernels, increasing program efficiency by as much as 50 fold over traditional programming approaches.APLDOT modeling language[edit]An example of an early industrial weight spreadsheet was APLDOT, developed in 1976 at the United States Railway Association on an IBM 360/91, running at The Johns Hopkins University Applied Physics Laboratory in Laurel, MD.[22] The application was used successfully for many years in developing such applications as financial and costing models for the US Congress and for Conrail. APLDOT was dubbed a spreadsheet because financial analysts and strategic planners used it to solve the same problems they addressed with paper spreadsheet pads.VisiCalc[edit]Because of Dan Bricklin and Bob Frankston's implementation of VisiCalc on the Apple II in 1979 and the IBM PC in 1981, the spreadsheet concept became widely known in the late 1970s and early 1980s. VisiCalc was the first spreadsheet that combined all essential features of modern spreadsheet applications (except for forward referencing/natural order recalculation), such as WYSIWYG interactive user interface, automatic recalculation, status and formula lines, range copying with relative and absolute references, formula building by selecting referenced cells. Unaware of LANPAR at the time PC World magazine called VisiCalc the first electronic spreadsheet.[23]Bricklin has spoken of watching his university professor create a table of calculation results on a blackboard. When the professor found an error, he had to tediously erase and rewrite a number of sequential entries in the table, triggering Bricklin to think that he could replicate the process on a computer, using the blackboard as the model to view results of underlying formulas. His idea became VisiCalc, the first application that turned the personal computer from a hobby for computer enthusiasts into a business tool.VisiCalc went on to become the first killer application,[24][25] an application that was so compelling, people would buy a particular computer just to use it. VisiCalc was in no small part responsible for the Apple II's success. The program was later ported to a number of other early computers, notably CP/M machines, the Atari 8-bit family and various Commodore platforms. Nevertheless, VisiCalc remains best known as an Apple II program.SuperCalc[edit]SuperCalc was a spreadsheet application published by Sorcim in 1980, and originally bundled (along with WordStar) as part of the CP/M software package included with the Osborne 1 portable computer. It quickly became the de facto standard spreadsheet for CP/M and was ported to MS-DOS in 1982.Lotus 1-2-3 and other MS-DOS spreadsheets[edit]The acceptance of the IBM PC following its introduction in August, 1981, began slowly, because most of the programs available for it were translations from other computer models. Things changed dramatically with the introduction of Lotus 1-2-3 in November, 1982, and release for sale in January, 1983. Since it was written especially for the IBM PC, it had good performance and became the killer app for this PC. Lotus 1-2-3 drove sales of the PC due to the improvements in speed and graphics compared to VisiCalc on the Apple II.[26]Lotus 1-2-3, along with its competitor Borland Quattro, soon displaced VisiCalc. Lotus 1-2-3 was released on January 26, 1983, started outselling then-most-popular VisiCalc the very same year, and for a number of years was the leading spreadsheet for DOS.Microsoft Excel[edit]Microsoft released the first version of Excel for the Macintosh on September 30, 1985, and then ported[27] it to Windows, with the first version being numbered 2.05 (to synchronize with the Macintosh version 2.2) and released in November 1987. The Windows 3.x platforms of the early 1990s made it possible for Excel to take market share from Lotus. By the time Lotus responded with usable Windows products, Microsoft had begun to assemble their Office suite. By 1995, Excel was the market leader, edging out Lotus 1-2-3,[14] and in 2013, IBM discontinued Lotus-1-2-3 altogether.[28]Web based spreadsheets[edit]With the advent of advanced web technologies such as Ajax circa 2005, a new generation of online spreadsheets has emerged. Equipped with a rich Internet application user experience, the best web based online spreadsheets have many of the features seen in desktop spreadsheet applications. Some of them such as EditGrid, Google Sheets, Microsoft Excel Online, Smartsheet, or Zoho Office Suite also have strong multi-user collaboration features or offer real time updates from remote sources such as stock prices and currency exchange rates.Other spreadsheets[edit]Gnumeric is a free, cross-platform spreadsheet program that is part of the GNOME Free Software Desktop Project. OpenOffice.org Calc and the closely related LibreOffice Calc (using the LGPL license) are free and open-source spreadsheets.Notable current spreadsheet software:\nCalligra Sheets (formerly KCalc)\nCorel Quattro Pro (WordPerfect Office)\nKingsoft Spreadsheets\nNeoOffice\nNumbers is Apple Inc.'s spreadsheet software, part of iWork.\nPyspread\nDiscontinued spreadsheet software:\n3D-Calc for Atari ST computers\nFramework by Forefront Corporation/Ashton-Tate (1983/84)\nGNU Oleo \u2013 A traditional terminal mode spreadsheet for UNIX/UNIX-like systems\nIBM Lotus Symphony (2007)\nJavelin Software\nKCells\nLotus Improv[29]\nLotus Jazz for Macintosh\nLotus Symphony (1984)\nMultiPlan\nClaris' Resolve (Macintosh)\nResolver One\nBorland's Quattro Pro\nSIAG\nSuperCalc\nT/Maker\nTarget Planner Calc for CP/M and TRS-DOS[30][31]\nTrapeze for Macintosh[32]\nWingz for Macintosh\nOther products[edit]A number of companies have attempted to break into the spreadsheet market with programs based on very different paradigms. Lotus introduced what is likely the most successful example, Lotus Improv, which saw some commercial success, notably in the financial world where its powerful data mining capabilities remain well respected to this day.Spreadsheet 2000 attempted to dramatically simplify formula construction, but was generally not successful.Concepts[edit]The main concepts are those of a grid of cells, called a sheet, with either raw data, called values, or formulas in the cells. Formulas say how to mechanically compute new values from existing values. Values are generally numbers, but can also be pure text, dates, months, etc. Extensions of these concepts include logical spreadsheets. Various tools for programming sheets, visualizing data, remotely connecting sheets, displaying cells' dependencies, etc. are commonly provided.Cells[edit]A cell can be thought of as a box for holding data. A single cell is usually referenced by its column and row (A2 would represent the cell containing the value 10 in the example table below). Usually rows, representing the dependent variables, are referenced in decimal notation starting from 1, while columns representing the independent variables use 26-adic bijective numeration using the letters A-Z as numerals. Its physical size can usually be tailored to its content by dragging its height or width at box intersections (or for entire columns or rows by dragging the column- or row-headers).An array of cells is called a sheet or worksheet. It is analogous to an array of variables in a conventional computer program (although certain unchanging values, once entered, could be considered, by the same analogy, constants). In most implementations, many worksheets may be located within a single spreadsheet. A worksheet is simply a subset of the spreadsheet divided for the sake of clarity. Functionally, the spreadsheet operates as a whole and all cells operate as global variables within the spreadsheet (each variable having 'read' access only except its own containing cell).A cell may contain a value or a formula, or it may simply be left empty. By convention, formulas usually begin with = sign.Values[edit]A value can be entered from the computer keyboard by directly typing into the cell itself. Alternatively, a value can be based on a formula (see below), which might perform a calculation, display the current date or time, or retrieve external data such as a stock quote or a database value.\nThe Spreadsheet Value Rule\nComputer scientist Alan Kay used the term value rule to summarize a spreadsheet's operation: a cell's value relies solely on the formula the user has typed into the cell.[33] The formula may rely on the value of other cells, but those cells are likewise restricted to user-entered data or formulas. There are no 'side effects' to calculating a formula: the only output is to display the calculated result inside its occupying cell. There is no natural mechanism for permanently modifying the contents of a cell unless the user manually modifies the cell's contents. In the context of programming languages, this yields a limited form of first-order functional programming.[34]\nAutomatic recalculation[edit]A standard of spreadsheets since the 1980s, this optional feature eliminates the need to manually request the spreadsheet program to recalculate values (nowadays typically the default option unless specifically 'switched off' for large spreadsheets, usually to improve performance). Some earlier spreadsheets required a manual request to recalculate, since recalculation of large or complex spreadsheets often reduced data entry speed. Many modern spreadsheets still retain this option.Recalculation generally requires that there are no circular dependencies in a spreadsheet. A dependency graph is a graph that has a vertex for each object to be updated, and an edge connecting two objects whenever one of them needs to be updated earlier than the other. Dependency graphs without circular dependencies form directed acyclic graphs, representations of partial orderings (in this case, across a spreadsheet) that can be relied upon to give a definite result.[35]Real-time update[edit]This feature refers to updating a cell's contents periodically with a value from an external source\u2014such as a cell in a remote spreadsheet. For shared, Web-based spreadsheets, it applies to immediately updating cells another user has updated. All dependent cells must be updated also.Locked cell[edit]Once entered, selected cells (or the entire spreadsheet) can optionally be locked to prevent accidental overwriting. Typically this would apply to cells containing formulas but might be applicable to cells containing constants such as a kilogram/pounds conversion factor (2.20462262 to eight decimal places). Even though individual cells are marked as locked, the spreadsheet data are not protected until the feature is activated in the file preferences.Data format[edit]A cell or range can optionally be defined to specify how the value is displayed. The default display format is usually set by its initial content if not specifically previously set, so that for example 31/12/2007 or 31 Dec 2007 would default to the cell format of date. Similarly adding a % sign after a numeric value would tag the cell as a percentage cell format. The cell contents are not changed by this format, only the displayed value.Some cell formats such as numeric or currency can also specify the number of decimal places.This can allow invalid operations (such as doing multiplication on a cell containing a date), resulting in illogical results without an appropriate warning.Cell formatting[edit]Depending on the capability of the spreadsheet application, each cell (like its counterpart the style in a word processor) can be separately formatted using the attributes of either the content (point size, color, bold or italic) or the cell (border thickness, background shading, color). To aid the readability of a spreadsheet, cell formatting may be conditionally applied to data; for example, a negative number may be displayed in red.A cell's formatting does not typically affect its content and depending on how cells are referenced or copied to other worksheets or applications, the formatting may not be carried with the content.Named cells[edit]In most implementations, a cell, or group of cells in a column or row, can be named enabling the user to refer to those cells by a name rather than by a grid reference. Names must be unique within the spreadsheet, but when using multiple sheets in a spreadsheet file, an identically named cell range on each sheet can be used if it is distinguished by adding the sheet name. One reason for this usage is for creating or running macros that repeat a command across many sheets. Another reason is that formulas with named variables are readily checked against the algebra they are intended to implement (they resemble Fortran expressions). Use of named variables and named functions also makes the spreadsheet structure more transparent.Cell reference[edit]In place of a named cell, an alternative approach is to use a cell (or grid) reference. Most cell references indicate another cell in the same spreadsheet, but a cell reference can also refer to a cell in a different sheet within the same spreadsheet, or (depending on the implementation) to a cell in another spreadsheet entirely, or to a value from a remote application.A typical cell reference in A1 style consists of one or two case-insensitive letters to identify the column (if there are up to 256 columns: A\u2013Z and AA\u2013IV) followed by a row number (e.g., in the range 1\u201365536). Either part can be relative (it changes when the formula it is in is moved or copied), or absolute (indicated with $ in front of the part concerned of the cell reference). The alternative R1C1 reference style consists of the letter R, the row number, the letter C, and the column number; relative row or column numbers are indicated by enclosing the number in square brackets. Most current spreadsheets use the A1 style, some providing the R1C1 style as a compatibility option.When the computer calculates a formula in one cell to update the displayed value of that cell, cell reference(s) in that cell, naming some other cell(s), cause the computer to fetch the value of the named cell(s).A cell on the same sheet is usually addressed as:\n=A1\nA cell on a different sheet of the same spreadsheet is usually addressed as:\n=SHEET2!A1             (that is; the first cell in sheet 2 of same spreadsheet).\nSome spreadsheet implementations in Excel allow a cell references to another spreadsheet (not the current open and active file) on the same computer or a local network. It may also refer to a cell in another open and active spreadsheet on the same computer or network that is defined as shareable. These references contain the complete filename, such as:\n='C:\\Documents and Settings\\Username\\My spreadsheets\\[main sheet]Sheet1!A1\nIn a spreadsheet, references to cells automatically update when new rows or columns are inserted or deleted. Care must be taken, however, when adding a row immediately before a set of column totals to ensure that the totals reflect the additional rows values\u2014which they often do not.A circular reference occurs when the formula in one cell refers\u2014directly, or indirectly through a chain of cell references\u2014to another cell that refers back to the first cell. Many common errors cause circular references. However, some valid techniques use circular references. These techniques, after many spreadsheet recalculations, (usually) converge on the correct values for those cells.Cell ranges[edit]Likewise, instead of using a named range of cells, a range reference can be used. Reference to a range of cells is typically of the form (A1:A6), which specifies all the cells in the range A1 through to A6. A formula such as =SUM(A1:A6) would add all the cells specified and put the result in the cell containing the formula itself.Sheets[edit]In the earliest spreadsheets, cells were a simple two-dimensional grid. Over time, the model has expanded to include a third dimension, and in some cases a series of named grids, called sheets. The most advanced examples allow inversion and rotation operations which can slice and project the data set in various ways.Formulas[edit]A formula identifies the calculation needed to place the result in the cell it is contained within. A cell containing a formula therefore has two display components; the formula itself and the resulting value. The formula is normally only shown when the cell is selected by clicking the mouse over a particular cell; otherwise it contains the result of the calculation.A formula assigns values to a cell or range of cells, and typically has the format:where the expression consists of:\nvalues, such as 2, 9.14 or 6.67E-11;\nreferences to other cells, such as, e.g., A1 for a single cell or B1:B3 for a range;\narithmetic operators, such as +, -, *, /, and others;\nrelational operators, such as >=, <, and others; and,\nfunctions, such as SUM(), TAN(), and many others.\nWhen a cell contains a formula, it often contains references to other cells. Such a cell reference is a type of variable. Its value is the value of the referenced cell or some derivation of it. If that cell in turn references other cells, the value depends on the values of those. References can be relative (e.g., A1, or B1:B3), absolute (e.g., $A$1, or $B$1:$B$3) or mixed row\u2013 or column-wise absolute/relative (e.g., $A1 is column-wise absolute and A$1 is row-wise absolute).The available options for valid formulas depends on the particular spreadsheet implementation but, in general, most arithmetic operations and quite complex nested conditional operations can be performed by most of today's commercial spreadsheets. Modern implementations also offer functions to access custom-build functions, remote data, and applications.A formula may contain a condition (or nested conditions)\u2014with or without an actual calculation\u2014and is sometimes used purely to identify and highlight errors. In the example below, it is assumed the sum of a column of percentages (A1 through A6) is tested for validity and an explicit message put into the adjacent right-hand cell.\n=IF(SUM(A1:A6) > 100, More than 100%, SUM(A1:A6))\nFurther examples:\n=IF(AND(A1<>,B1<>),A1/B1,) means that if both cells A1 and B1 are not <> empty , then divide A1 by B1 and display, other do not display anything.\n=IF(AND(A1<>,B1<>),IF(B1<>0,A1/B1,Division by zero),) means that if cells A1 and B1 are not empty, and B1 is not zero, then divide A1 by B1, if B1 is zero, then display Division by zero, and do not display anything if either A1 and B1 are empty.\n=IF(OR(A1<>,B1<>),Either A1 or B1 show text,) means to display the text if either cells A1 or B1 are not empty.\nThe best way to build up conditional statements is step by step composing followed by trial and error testing and refining code.A spreadsheet does not, in fact, have to contain any formulas at all, in which case it could be considered merely a collection of data arranged in rows and columns (a database) like a calendar, timetable or simple list. Because of its ease of use, formatting and hyperlinking capabilities, many spreadsheets are used solely for this purpose.Functions[edit]Spreadsheets usually contain a number of supplied functions, such as arithmetic operations (for example, summations, averages and so forth), trigonometric functions, statistical functions, and so forth. In addition there is often a provision for user-defined functions. In Microsoft Excel these functions are defined using Visual Basic for Applications in the supplied Visual Basic editor, and such functions are automatically accessible on the worksheet. In addition, programs can be written that pull information from the worksheet, perform some calculations, and report the results back to the worksheet. In the figure, the name sq is user-assigned, and function sq is introduced using the Visual Basic editor supplied with Excel. Name Manager displays the spreadsheet definitions of named variables x & y.Subroutines[edit]Functions themselves cannot write into the worksheet, but simply return their evaluation. However, in Microsoft Excel, subroutines can write values or text found within the subroutine directly to the spreadsheet. The figure shows the Visual Basic code for a subroutine that reads each member of the named column variable x, calculates its square, and writes this value into the corresponding element of named column variable y. The y column contains no formula because its values are calculated in the subroutine, not on the spreadsheet, and simply are written in.Remote spreadsheet[edit]Whenever a reference is made to a cell or group of cells that are not located within the current physical spreadsheet file, it is considered as accessing a remote spreadsheet. The contents of the referenced cell may be accessed either on first reference with a manual update or more recently in the case of web based spreadsheets, as a near real time value with a specified automatic refresh interval.Charts[edit]Many spreadsheet applications permit charts, graphs or histograms to be generated from specified groups of cells that are dynamically re-built as cell contents change. The generated graphic component can either be embedded within the current sheet or added as a separate object.Multi-dimensional spreadsheets[edit]In the late 1980s and early 1990s, first Javelin Software and Lotus Improv appeared. Unlike models in a conventional spreadsheet, they utilized models built on objects called variables, not on data in cells of a report. These multi-dimensional spreadsheets enabled viewing data and algorithms in various self-documenting ways, including simultaneous multiple synchronized views. For example, users of Javelin could move through the connections between variables on a diagram while seeing the logical roots and branches of each variable. This is an example of what is perhaps its primary contribution of the earlier Javelin\u2014the concept of traceability of a user's logic or model structure through its twelve views. A complex model can be dissected and understood by others who had no role in its creation.In these programs, a time series, or any variable, was an object in itself, not a collection of cells that happen to appear in a row or column. Variables could have many attributes, including complete awareness of their connections to all other variables, data references, and text and image notes. Calculations were performed on these objects, as opposed to a range of cells, so adding two time series automatically aligns them in calendar time, or in a user-defined time frame. Data were independent of worksheets\u2014variables, and therefore data, could not be destroyed by deleting a row, column or entire worksheet. For instance, January's costs are subtracted from January's revenues, regardless of where or whether either appears in a worksheet. This permits actions later used in pivot tables, except that flexible manipulation of report tables was but one of many capabilities supported by variables. Moreover, if costs were entered by week and revenues by month, the program could allocate or interpolate as appropriate. This object design enabled variables and whole models to reference each other with user-defined variable names, and to perform multidimensional analysis and massive, but easily editable consolidations.Trapeze,[32] a spreadsheet on the Mac, went further and explicitly supported not just table columns, but also matrix operators.Logical spreadsheets[edit]Spreadsheets that have a formula language based upon logical expressions, rather than arithmetic expressions are known as logical spreadsheets. Such spreadsheets can be used to reason deductively about their cell values.Programming issues[edit]Just as the early programming languages were designed to generate spreadsheet printouts, programming techniques themselves have evolved to process tables (also known as spreadsheets or matrices) of data more efficiently in the computer itself.End-user development[edit]Spreadsheets are a popular end-user development tool.[36] EUD denotes activities or techniques in which people who are not professional developers create automated behavior and complex data objects without significant knowledge of a programming language. Many people find it easier to perform calculations in spreadsheets than by writing the equivalent sequential program. This is due to several traits of spreadsheets.\nThey use spatial relationships to define program relationships. Humans have highly developed intuitions about spaces, and of dependencies between items. Sequential programming usually requires typing line after line of text, which must be read slowly and carefully to be understood and changed.\nThey are forgiving, allowing partial results and functions to work. One or more parts of a program can work correctly, even if other parts are unfinished or broken. This makes writing and debugging programs easier, and faster. Sequential programming usually needs every program line and character to be correct for a program to run. One error usually stops the whole program and prevents any result.\nModern spreadsheets allow for secondary notation. The program can be annotated with colors, typefaces, lines, etc. to provide visual cues about the meaning of elements in the program.\nExtensions that allow users to create new functions can provide the capabilities of a functional language.[37]\nExtensions that allow users to build and apply models from the domain of machine learning.[38][39]\nSpreadsheets are versatile. With their boolean logic and graphics capabilities, even electronic circuit design is possible.[40]\nSpreadsheets can store relational data and spreadsheet formulas can express all queries of SQL. There exists a query translator, which automatically generates the spreadsheet implementation from the SQL code.[41]\nSpreadsheet programs[edit]A spreadsheet program is designed to perform general computation tasks using spatial relationships rather than time as the primary organizing principle.It is often convenient to think of a spreadsheet as a mathematical graph, where the nodes are spreadsheet cells, and the edges are references to other cells specified in formulas. This is often called the dependency graph of the spreadsheet. References between cells can take advantage of spatial concepts such as relative position and absolute position, as well as named locations, to make the spreadsheet formulas easier to understand and manage.Spreadsheets usually attempt to automatically update cells when the cells they depend on change. The earliest spreadsheets used simple tactics like evaluating cells in a particular order, but modern spreadsheets calculate following a minimal recomputation order from the dependency graph. Later spreadsheets also include a limited ability to propagate values in reverse, altering source values so that a particular answer is reached in a certain cell. Since spreadsheet cells formulas are not generally invertible, though, this technique is of somewhat limited value.Many of the concepts common to sequential programming models have analogues in the spreadsheet world. For example, the sequential model of the indexed loop is usually represented as a table of cells, with similar formulas (normally differing only in which cells they reference).Spreadsheets have evolved to use scripting programming languages like VBA as a tool for extensibility beyond what the spreadsheet language makes easy.Shortcomings[edit]While spreadsheets represented a major step forward in quantitative modeling, they have deficiencies. Their shortcomings include the perceived unfriendliness of alpha-numeric cell addresses.[42]\nResearch by ClusterSeven has shown huge discrepancies in the way financial institutions and corporate entities understand, manage and police their often vast estates of spreadsheets and unstructured financial data (including comma-separated values (CSV) files and Microsoft Access databases). One study in early 2011 of nearly 1,500 people in the UK found that 57% of spreadsheet users have never received formal training on the spreadsheet package they use. 72% said that no internal department checks their spreadsheets for accuracy. Only 13% said that Internal Audit reviews their spreadsheets, while a mere 1% receive checks from their risk department.[43]\nSpreadsheets have significant reliability problems. Research studies estimate that roughly 94% of spreadsheets deployed in the field contain errors, and 5.2% of cells in unaudited spreadsheets contain errors.[44]\n\n\n\nDespite the high error risks often associated with spreadsheet authorship and use, specific steps can be taken to significantly enhance control and reliability by structurally reducing the likelihood of error occurrence at their source.[45]\n\n\n\nThe practical expressiveness of spreadsheets can be limited unless their modern features are used. Several factors contribute to this limitation. Implementing a complex model on a cell-at-a-time basis requires tedious attention to detail. Authors have difficulty remembering the meanings of hundreds or thousands of cell addresses that appear in formulas.\n\n\n\nThese drawbacks are mitigated by the use of named variables for cell designations, and employing variables in formulas rather than cell locations and cell-by-cell manipulations. Graphs can be used to show instantly how results are changed by changes in parameter values. In fact, the spreadsheet can be made invisible except for a transparent user interface that requests pertinent input from the user, displays results requested by the user, creates reports, and has built-in error traps to prompt correct input.[46]\n\n\n\nSimilarly, formulas expressed in terms of cell addresses are hard to keep straight and hard to audit. Research shows that spreadsheet auditors who check numerical results and cell formulas find no more errors than auditors who only check numerical results.[44] That is another reason to use named variables and formulas employing named variables.\nThe alteration of a dimension demands major surgery. When rows (or columns) are added to or deleted from a table, one has to adjust the size of many downstream tables that depend on the table being changed. In the process, it is often necessary to move other cells around to make room for the new columns or rows, and to adjust graph data sources. In large spreadsheets, this can be extremely time consuming.[47][48]\nAdding or removing a dimension is so difficult, one generally has to start over. The spreadsheet as a paradigm really forces one to decide on dimensionality right of the beginning of one's spreadsheet creation, even though it is often most natural to make these choices after one's spreadsheet model has matured. The desire to add and remove dimensions also arises in parametric and sensitivity analyses.[47][48]\nCollaboration in authoring spreadsheet formulas can be difficult when such collaboration occurs at the level of cells and cell addresses.\nOther problems associated with spreadsheets include:[49][50]\nSome sources advocate the use of specialized software instead of spreadsheets for some applications (budgeting, statistics)[51][52][53]\nMany spreadsheet software products, such as Microsoft Excel[54] (versions prior to 2007) and OpenOffice.org Calc[55] (versions prior to 2008), have a capacity limit of 65,536 rows by 256 columns (216 and 28 respectively). This can present a problem for people using very large datasets, and may result in data loss.\nLack of auditing and revision control. This makes it difficult to determine who changed what and when. This can cause problems with regulatory compliance. Lack of revision control greatly increases the risk of errors due the inability to track, isolate and test changes made to a document.[citation needed]\nLack of security. Spreadsheets lack controls on who can see and modify particular data. This, combined with the lack of auditing above, can make it easy for someone to commit fraud.[56]\nBecause they are loosely structured, it is easy for someone to introduce an error, either accidentally or intentionally, by entering information in the wrong place or expressing dependencies among cells (such as in a formula) incorrectly.[47][57][58]\nThe results of a formula (example =A1*B1) applies only to a single cell (that is, the cell the formula is actually located in\u2014in this case perhaps C1), even though it can extract data from many other cells, and even real time dates and actual times. This means that to cause a similar calculation on an array of cells, an almost identical formula (but residing in its own output cell) must be repeated for each row of the input array. This differs from a formula in a conventional computer program, which typically makes one calculation that it applies to all the input in turn. With current spreadsheets, this forced repetition of near identical formulas can have detrimental consequences from a quality assurance standpoint and is often the cause of many spreadsheet errors. Some spreadsheets have array formulas to address this issue.\nTrying to manage the sheer volume of spreadsheets that may exist in an organization without proper security, audit trails, unintentional introduction of errors, and other items listed above can become overwhelming.\nWhile there are built-in and third-party tools for desktop spreadsheet applications that address some of these shortcomings, awareness and use of these is generally low. A good example of this is that 55% of Capital market professionals don't know how their spreadsheets are audited; only 6% invest in a third-party solution[59]Spreadsheet risk[edit]Spreadsheet risk is the risk associated with deriving a materially incorrect value from a spreadsheet application that will be utilised in making a related (usually numerically-based) decision. Examples include the valuation of an asset, the determination of financial accounts, the calculation of medicinal doses or the size of load-bearing beam for structural engineering. The risk may arise from inputting erroneous or fraudulent data values, from mistakes (or incorrect changes) within the logic of the spreadsheet or the omission of relevant updates (e.g., out of date exchange rates). Some single-instance errors have exceeded US$1 billion.[60][61] Because spreadsheet risk is principally linked to the actions (or inaction) of individuals it is defined as a sub-category of operational risk.In the report into the 2012 JPMorgan Chase trading loss, a lack of control over spreadsheets used for critical financial functions was cited as a factor in the trading losses of more than six billion dollars which were reported as a result of derivatives trading gone bad.Despite this, research[62] carried out by ClusterSeven revealed that around half (48%) of c-level executives and senior managers at firms reporting annual revenues over \u00a350m said there were either no usage controls at all or poorly applied manual processes over the use of spreadsheets at the firms.[62][63]In 2013 Thomas Herndon, a graduate student of economics at the University of Massachusetts Amherst found major coding flaws in the spreadsheet used by the economists Carmen Reinhart and Kenneth Rogoff in a very influential 2010 journal article. The Reinhart and Rogoff article was widely used as justification to drive 2010\u20132013 European austerity programs.[64]See also[edit]\nAttribute-value system\nComparison of spreadsheet software\nMoving and copying in spreadsheets\nList of spreadsheet software\nModel audit\nReferences[edit]External links[edit]\nA Spreadsheet Programming article on DevX\ncomp.apps.spreadsheets FAQ by Russell Schulz\nExtending the Concept of Spreadsheet by Jocelyn Paine\nSpreadsheet at Curlie (based on DMOZ)\nSpreadsheet \u2013 Its First Computerization (1961\u20131964) by Richard Mattessich\nCICS history and introduction of IBM 3270 by Bob Yelavich\nAutoplan & Autotab article by Creative Karma\nSpreadsheets in Science\n", "subtitles": ["Usage", "History", "Concepts", "Programming issues", "Shortcomings", "Spreadsheet risk", "See also", "References", "External links"], "title": "Spreadsheet"},
{"content": "A planetary system is a set of gravitationally bound non-stellar objects in or out of orbit around a star or star system. Generally speaking, systems with one or more planets constitute a planetary system, although such systems may also consist of bodies such as dwarf planets, asteroids, natural satellites, meteoroids, comets, planetesimals[1][2] and circumstellar disks. The Sun together with its planetary system, which includes Earth, is known as the Solar System.[3][4] The term exoplanetary system is sometimes used in reference to other planetary systems.As of 1 April 2018, there are 3,758 confirmed planets in 2,808 systems, with 627 systems having more than one planet.[5] Debris disks are also known to be common, though other objects are more difficult to observe.Of particular interest to astrobiology is the habitable zone of planetary systems where planets could have surface liquid water, and thus the capacity to harbor Earth-like life.History[edit]Heliocentrism[edit]Historically, heliocentrism (the doctrine that the Sun is the centre of the universe) was opposed to geocentrism (placing the Earth at the center of the universe).The notion of a heliocentric Solar System, with the Sun at the center, is possibly first suggested in the Vedic literature of ancient India, which often refer to the Sun as the centre of spheres. Some interpret Aryabhatta's writings in A\u0304ryabhat\u0323i\u0304ya as implicitly heliocentric.The idea was first proposed in Western philosophy and Greek astronomy as early as the 3rd century BC by Aristarchus of Samos,[6] but received no support from most other ancient astronomers.Discovery of the Solar System[edit]De revolutionibus orbium coelestium by Nicolaus Copernicus, published in 1543, was the first mathematically predictive heliocentric model of a planetary system. 17th-century successors Galileo Galilei, Johannes Kepler, and Isaac Newton developed an understanding of physics which led to the gradual acceptance of the idea that the Earth moves round the Sun and that the planets are governed by the same physical laws that governed the Earth.Speculation on extrasolar planetary systems[edit]In the 16th century the Italian philosopher Giordano Bruno, an early supporter of the Copernican theory that the Earth and other planets orbit the Sun, put forward the view that the fixed stars are similar to the Sun and are likewise accompanied by planets. He was burned at the stake for his ideas by the Roman Inquisition.[7]In the 18th century the same possibility was mentioned by Isaac Newton in the General Scholium that concludes his Principia. Making a comparison to the Sun's planets, he wrote And if the fixed stars are the centers of similar systems, they will all be constructed according to a similar design and subject to the dominion of One.[8]His theories gained traction through the 19th and 20th centuries despite a lack of supporting evidence. Long before their confirmation by astronomers, conjecture on the nature of planetary systems had been a focus of the search for extraterrestrial intelligence and has been a prevalent theme in fiction, particularly science fiction.Detection of exoplanets[edit]The first confirmed detection of an exoplanet was in 1992, with the discovery of several terrestrial-mass planets orbiting the pulsar PSR B1257+12. The first confirmed detection of exoplanets of a main-sequence star was made in 1995, when a giant planet, 51 Pegasi b, was found in a four-day orbit around the nearby G-type star 51 Pegasi. The frequency of detections has increased since then, particularly through advancements in methods of detecting extrasolar planets and dedicated planet finding programs such as the Kepler mission.Origin and evolution[edit]Planetary systems come from protoplanetary disks that form around stars as part of the process of star formation.During formation of a system much material is gravitationally scattered into far-flung orbits and some planets are ejected completely from the system becoming rogue planets.Evolved systems[edit]High-mass stars[edit]Planets orbiting pulsars have been discovered. Pulsars are the remnants of the supernova explosions of high-mass stars, but a planetary system that existed before the supernova would likely be mostly destroyed. Planets would either evaporate, be pushed off of their orbits by the masses of gas from the exploding star, or the sudden loss of most of the mass of the central star would see them escape the gravitational hold of the star, or in some cases the supernova would kick the pulsar itself out of the system at high velocity so any planets that had survived the explosion would be left behind as free-floating objects. Planets found around pulsars may have formed as a result of pre-existing stellar companions that were almost entirely evaporated by the supernova blast, leaving behind planet-sized bodies. Alternatively, planets may form in an accretion disk of fallback matter surrounding a pulsar.[9] Fallback disks of matter that failed to escape orbit during a supernova may also form planets around black holes.[10]Lower-mass stars[edit]As stars evolve and turn into red giants, asymptotic giant branch stars and planetary nebulae they engulf the inner planets, evaporating or partially evaporating them depending on how massive they are. As the star loses mass, planets that are not engulfed move further out from the star.If an evolved star is in a binary or multiple system then the mass it loses can transfer to another star, creating new protoplanetary disks and second- and third-generation planets which may differ in composition from the original planets which may also be affected by the mass transfer.\nPlanets in evolved binary systems, Hagai B. Perets, 13 Jan 2011\nCan Planets survive Stellar Evolution?, Eva Villaver, Mario Livio, Feb 2007\nThe Orbital Evolution of Gas Giant Planets around Giant Stars, Eva Villaver, Mario Livio, 13 Oct 2009\nOn the survival of brown dwarfs and planets engulfed by their giant host star, Jean-Claude Passy, Mordecai-Mark Mac Low, Orsola De Marco, 2 Oct 2012\nForetellings of Ragnaro\u0308k: World-engulfing Asymptotic Giants and the Inheritance of White Dwarfs, Alexander James Mustill, Eva Villaver, 5 Dec 2012\nSystem architectures[edit]The Solar System consists of an inner region of small rocky planets and outer region of large gas giants. However, other planetary systems can have quite different architectures. Studies suggest that architectures of planetary systems are dependent on the conditions of their initial formation.[12] Many systems with a hot Jupiter gas giant very close to the star have been found. Theories, such as planetary migration or scattering, have been proposed for the formation of large planets close to their parent stars.[13] At present, few systems have been found to be analogous to the Solar System with terrestrial planets close to the parent star. More commonly, systems consisting of multiple Super-Earths have been detected.[14]Components[edit]Planets[edit]Some studies suggest that there is at least one planet on average per star.[15] This would suggest that, like the Solar System, most stars have planets (or exoplanets). However, the proportion of stars is uncertain because not all planets can yet be detected. The radial-velocity method and the transit method (which between them are responsible for the vast majority of detections) are most sensitive to large planets in small orbits. Thus, many known exoplanets are hot Jupiters: planets of Jovian mass or larger in very small orbits with periods of only a few days. A 2005 survey of radial-velocity-detected planets found that about 1.2% of Sun-like stars have a hot Jupiter, where Sun-like star refers to any main-sequence star of spectral classes late-F, G, or early-K without a close stellar companion.[16] This 1.2% is more than double the frequency of hot jupiters detected by the Kepler spacecraft, which may be because the Kepler field of view covers a different region of the Milky Way where the metallicity of stars is different.[17] It is further estimated that 3% to 4.5% of Sun-like stars possess a giant planet with an orbital period of 100 days or less, where giant planet means a planet of at least 30 Earth masses.[18]It is known that small planets (of roughly Earth-like mass or somewhat larger) are more common than giant planets.[19] It also appears that there are more planets in large orbits than in small orbits. Based on this, it is estimated that perhaps 20% of Sun-like stars have at least one giant planet, whereas at least 40% may have planets of lower mass.[18][20][21] A 2012 study of gravitational microlensing data collected between 2002 and 2007 concludes the proportion of stars with planets is much higher and estimates an average of 1.6 planets orbiting between 0.5\u201310 AU per star in the Milky Way. The authors of this study conclude that stars are orbited by planets as a rule, rather than the exception.[15]Whatever the proportion of stars with planets, the total number of exoplanets must be very large. Because the Milky Way has at least 200 billion stars, it must also contain tens or hundreds of billions of planets.Most known exoplanets orbit stars roughly similar to the Sun, that is, main-sequence stars of spectral categories F, G, or K. One reason is that planet-search programs have tended to concentrate on such stars. In addition, statistical analyses indicate that lower-mass stars (red dwarfs, of spectral category M) are less likely to have planets massive enough to be detected by the radial-velocity method.[18][22] Nevertheless, several tens of planets around red dwarfs have been discovered by the Kepler spacecraft by the transit method, which can detect smaller planets.Stars of spectral categories A and B typically rotate very quickly, which makes it very difficult to measure the small Doppler shifts induced by orbiting planets because the spectral lines are very broad. However, this type of massive star eventually evolves into a cooler red giant that rotates more slowly and thus can be measured using the radial-velocity method. A few tens of planets have been found around red giants.Observations using the Spitzer Space Telescope indicate that extremely massive stars of spectral category O, which are much hotter than the Sun, produce a photo-evaporation effect that inhibits planetary formation.[23] When the O-type star goes supernova any planets that had formed would become free-floating due to the loss of stellar mass unless the natal kick of the resulting remnant pushes it in the same direction as an escaping planet.[24] Fallback disks of matter that failed to escape orbit during a supernova may form planets around neutron stars and black holes.[10]Doppler surveys around a wide variety of stars indicate about 1 in 6 stars having twice the mass of the Sun are orbited by one or more Jupiter-sized planets, vs. 1 in 16 for Sun-like stars and only 1 in 50 for red dwarfs. On the other hand, microlensing surveys indicate that long-period Neptune-mass planets are found around 1 in 3 red dwarfs. [25] Kepler Space Telescope observations of planets with up to one year periods show that occurrence rates of Earth- to Neptune-sized planets (1 to 4 Earth radii) around M, K, G, and F stars are successively higher towards cooler, less massive stars.[26]At the low-mass end of star-formation are sub-stellar objects that don't fuse hydrogen: the brown dwarfs and sub-brown dwarfs, of spectral classification L,T and Y. Planets and protoplanetary disks have been discovered around brown dwarfs, and disks have been found around sub-brown dwarfs (e.g. OTS 44).Ordinary stars are composed mainly of the light elements hydrogen and helium. They also contain a small proportion of heavier elements, and this fraction is referred to as a star's metallicity (even if the elements are not metals in the traditional sense),[16] denoted [m/H] and expressed on a logarithmic scale where zero is the Sun's metallicity.A 2012 study of the Kepler spacecraft data found that smaller planets, with radii smaller than Neptune's were found around stars with metallicities in the range \u22120.6 < [m/H] < +0.5 (about four times less than that of the Sun to three times more),[a] whereas larger planets were found mostly around stars with metallicities at the higher end of this range (at solar metallicity and above). In this study small planets occurred about three times as frequently as large planets around stars of metallicity greater than that of the Sun, but they occurred around six times as frequently for stars of metallicity less than that of the Sun. The lack of gas giants around low-metallicity stars could be because the metallicity of protoplanetary disks affects how quickly planetary cores can form and whether they accrete a gaseous envelope before the gas dissipates. However, Kepler can only observe planets very close to their star and the detected gas giants probably migrated from further out, so a decreased efficiency of migration in low-metallicity disks could also partly explain these findings.[27]A 2014 study found that not only giant planets, but planets of all sizes, have an increased occurrence rate around metal-rich stars compared to metal-poor stars, although the larger the planet, the greater this increase as the metallicity increases. The study divided planets into three groups based on radius: gas giants, gas dwarfs, and terrestrial planets with the dividing lines at 1.7 and 3.9 Earth radii. For these three groups, the planet occurrence rates are 9.30, 2.03, and 1.72 times higher for metal-rich stars than for metal-poor stars, respectively. There is a bias against detecting smaller planets because metal-rich stars tend to be larger, making it more difficult to detect smaller planets, which means that these increases in occurrence rates are lower limits.[28]It has also been shown that stars with planets are more likely to be deficient in lithium.[29]Most stars form in open clusters, but very few planets have been found in open clusters and this led to the hypothesis that the open-cluster environment hinders planet formation. However, a 2011 study concluded that there have been an insufficient number of surveys of clusters to make such a hypothesis.[30] The lack of surveys was because there are relatively few suitable open clusters in the Milky Way. Recent discoveries of both giant planets[31] and low-mass planets[32] in open clusters are consistent with there being similar planet occurrence rates in open clusters as around field stars. The open cluster NGC 6811 contains two known planetary systems Kepler-66 and Kepler-67.Circumstellar disks and dust structures[edit]After planets, circumstellar disks are one of the most commonly observed properties of planetary systems, particularly of young stars. The Solar System possesses at least four major circumstellar disks (the asteroid belt, Kuiper belt, scattered disc, and Oort cloud) and clearly observable disks have been detected around nearby solar analogs including Epsilon Eridani and Tau Ceti. Based on observations of numerous similar disks, they are assumed to be quite common attributes of stars on the main sequence.Interplanetary dust clouds have been studied in the Solar System and analogs are believed to be present in other planetary systems. Exozodiacal dust, an exoplanetary analog of zodiacal dust, the 1\u2013100 micrometre-sized grains of amorphous carbon and silicate dust that fill the plane of the Solar System[33] has been detected around the 51 Ophiuchi, Fomalhaut,[34][35] Tau Ceti,[35][36] and Vega systems.Comets[edit]As of November 2014[update] there are 5,253 known Solar System comets[37] and they are thought to be common components of planetary systems. The first exocomets were detected in 1987[38][39] around Beta Pictoris, a very young A-type main-sequence star. There are now a total of 11 stars around which the presence of exocomets have been observed or suspected.[40][41][42][43] All discovered exocometary systems (Beta Pictoris, HR 10,[40] 51 Ophiuchi, HR 2174,[41] 49 Ceti, 5 Vulpeculae, 2 Andromedae, HD 21620, HD 42111, HD 110411,[42][44] and more recently HD 172555[43]) are around very young A-type stars.Other components[edit]Computer modelling of an impact in 2013 detected around the star NGC 2547 by the Spitzer Space Telescope and confirmed by ground observations suggests the involvement of large asteroids or protoplanets similar to the events believed to have led to the formation of terrestrial planets like the Earth.[45]Based on observations of the Solar System's large collection of natural satellites, they are believed common components of planetary systems; however, exomoons have so far eluded confirmation. The star 1SWASP J140747.93-394542.6, in the constellation Centaurus, is a strong candidate for a natural satellite.[46] Indications suggest that the confirmed extrasolar planet WASP-12b also has at least one satellite.[47]Orbital configurations[edit]Unlike the Solar System, which has orbits that are nearly circular, many of the known planetary systems display much higher orbital eccentricity.[48] An example of such a system is 16 Cygni.Mutual inclination[edit]The mutual inclination between two planets is the angle between their orbital planes. Many compact systems with multiple close-in planets interior to the equivalent orbit of Venus are expected to have very low mutual inclinations, so the system (at least the close-in part) would be even flatter than the solar system. Captured planets could be captured into any arbitrary angle to the rest of the system. The only system where mutual inclinations have actually been measured is the Upsilon Andromedae system: the planets, c and d, have a mutual inclination of about 30 degrees.[49][50]Orbital dynamics[edit]Planetary systems can be categorized according to their orbital dynamics as resonant, non-resonant-interacting, hierarchical, or some combination of these. In resonant systems the orbital periods of the planets are in integer ratios. The Kepler-223 system contains four planets in an 8:6:4:3 orbital resonance.[51] Giant planets are found in mean-motion resonances more often than smaller planets.[52] In interacting systems the planets orbits are close enough together that they perturb the orbital parameters. The Solar System could be described as weakly interacting. In strongly interacting systems Kepler's laws do not hold.[53] In hierarchical systems the planets are arranged so that the system can be gravitationally considered as a nested system of two-bodies, e.g. in a star with a close-in hot jupiter with another gas giant much further out, the star and hot jupiter form a pair that appears as a single object to another planet that is far enough out.Other, as yet unobserved, orbital possibilities include: double planets; various co-orbital planets such as quasi-satellites, trojans and exchange orbits; and interlocking orbits maintained by precessing orbital planes.[54]\nExtrasolar Binary Planets I: Formation by tidal capture during planet-planet scattering, H. Ochiai, M. Nagasawa, S. Ida, 26 Jun 2014\nDisruption of co-orbital (1:1) planetary resonances during gas-driven orbital migration, Arnaud Pierens, Sean Raymond, 19 May 2014\nNumber of planets, relative parameters and spacings[edit]\nOn The Relative Sizes of Planets Within Kepler Multiple Candidate Systems, David R. Ciardi et al. 9 Dec 2012\nThe Kepler Dichotomy among the M Dwarfs: Half of Systems Contain Five or More Coplanar Planets, Sarah Ballard, John Asher Johnson, 15 Oct 2014\nExoplanet Predictions Based on the Generalised Titius-Bode Relation, Timothy Bovaird, Charles H. Lineweaver, 1 Aug 2013\nThe Solar System and the Exoplanet Orbital Eccentricity - Multiplicity Relation, Mary Anne Limbach, Edwin L. Turner, 9 Apr 2014\nThe period ratio distribution of Kepler's candidate multiplanet systems, Jason H. Steffen, Jason A. Hwang, 11 Sep 2014\nAre Planetary Systems Filled to Capacity? A Study Based on Kepler Results, Julia Fang, Jean-Luc Margot, 28 Feb 2013\nPlanet capture[edit]Free-floating planets in open clusters have similar velocities to the stars and so can be recaptured. They are typically captured into wide orbits between 100 and 105 AU. The capture efficiency decreases with increasing cluster size, and for a given cluster size it increases with the host/primary mass. It is almost independent of the planetary mass. Single and multiple planets could be captured into arbitrary unaligned orbits, non-coplanar with each other or with the stellar host spin, or pre-existing planetary system. Some planet\u2013host metallicity correlation may still exist due to the common origin of the stars from the same cluster. Planets would be unlikely to be captured around neutron stars because these are likely to be ejected from the cluster by a pulsar kick when they form. Planets could even be captured around other planets to form free-floating planet binaries. After the cluster has dispersed some of the captured planets with orbits larger than 106 AU would be slowly disrupted by the galactic tide and likely become free-floating again through encounters with other field stars or giant molecular clouds.[55]Zones[edit]Habitable zone[edit]The habitable zone around a star is the region where the temperature is just right to allow liquid water to exist on a planet; that is, not too close to the star for the water to evaporate and not too far away from the star for the water to freeze. The heat produced by stars varies depending on the size and age of the star so that the habitable zone can be at different distances. Also, the atmospheric conditions on the planet influence the planet's ability to retain heat so that the location of the habitable zone is also specific to each type of planet.Habitable zones have usually been defined in terms of surface temperature; however, over half of Earth's biomass is from subsurface microbes,[56] and the temperature increases as one goes deeper underground, so the subsurface can be conducive for life when the surface is frozen and if this is considered, the habitable zone extends much further from the star.[57]Studies in 2013 indicated an estimated frequency of 22\u00b18% of Sun-like[b] stars have an Earth-sized[c] planet in the habitable[d] zone.[58][59]Venus zone[edit]The Venus zone is the region around a star where a terrestrial planet would have runaway greenhouse conditions like Venus, but not so near the star that the atmosphere completely evaporates. As with the habitable zone, the location of the Venus zone depends on several factors, including the type of star and properties of the planets such as mass, rotation rate, and atmospheric clouds. Studies of the Kepler spacecraft data indicate that 32% of red dwarfs have potentially Venus-like planets based on planet size and distance from star, rising to 45% for K-type and G-type stars.[e] Several candidates have been identified, but spectroscopic follow-up studies of their atmospheres are required to determine whether they are like Venus.[60][61]Galactic distribution of planets[edit]The Milky Way is 100,000 light-years across, but 90% of planets with known distances lie within about 2000 light years of Earth, as of July 2014. One method that can detect planets much further away is microlensing. The WFIRST spacecraft could use microlensing to measure the relative frequency of planets in the galactic bulge vs. galactic disk.[62] So far, the indications are that planets are more common in the disk than the bulge.[63] Estimates of the distance of microlensing events is difficult: the first planet considered with high probability of being in the bulge is MOA-2011-BLG-293Lb at a distance of 7.7 kiloparsecs (about 25,000 light years).[64]Population I, or metal-rich stars, are those young stars whose metallicity is highest. The high metallicity of population I stars makes them more likely to possess planetary systems than older populations, because planets form by the accretion of metals.[citation needed] The Sun is an example of a metal-rich star. These are common in the spiral arms of the Milky Way.[citation needed] Generally, the youngest stars, the extreme population I, are found farther in and intermediate population I stars are farther out, etc. The Sun is considered an intermediate population I star. Population I stars have regular elliptical orbits around the Galactic Center, with a low relative velocity.[65]Population II, or metal-poor stars, are those with relatively low metallicity which can have hundreds (e.g. BD +17\u00b0 3248) or thousands (e.g. Sneden's Star) times less metallicity than the Sun. These objects formed during an earlier time of the universe.[citation needed] Intermediate population II stars are common in the bulge near the center of the Milky Way,[citation needed] whereas Population II stars found in the galactic halo are older and thus more metal-poor.[citation needed] Globular clusters also contain high numbers of population II stars.[66] In 2014 the first planets around a halo star were announced around Kapteyn's star, the nearest halo star to Earth, around 13 light years away. However, later research suggests that Kapteyn b is just an artefact of stellar activity and that Kapteyn c needs more study to be confirmed.[67] The metallicity of Kapteyn's star is estimated to be about 8[f] times less than the Sun.[68]Different types of galaxies have different histories of star formation and hence planet formation. Planet formation is affected by the ages, metallicities, and orbits of stellar populations within a galaxy. Distribution of stellar populations within a galaxy varies between the different types of galaxies.[69] Stars in elliptical galaxies are much older than stars in spiral galaxies. Most elliptical galaxies contain mainly low-mass stars, with minimal star-formation activity.[70] The distribution of the different types of galaxies in the universe depends on their location within galaxy clusters, with elliptical galaxies found mostly close to their centers.[71]See also[edit]\nProtoplanetary disk\nList of exoplanets\nList of multiplanetary systems\nList of exoplanetary host stars\nReferences[edit]Further reading[edit]\nOn the Relationship Between Debris Disks and Planets, A\u0301gnes Ko\u0301spa\u0301l, David R. Ardila, Attila Moo\u0301r, Pe\u0301ter A\u0301braha\u0301m, 30 Jun 2009\nSignatures of exosolar planets in dust debris disks, Leonid M. Ozernoy, Nick N. Gorkavyi, John C. Mather, Tanya Taidakova, 4 Jul 2000\n", "subtitles": ["History", "Origin and evolution", "System architectures", "Zones", "Galactic distribution of planets", "See also", "References", "Further reading"], "title": "Planetary system"},
{"content": "The sensory nervous system is a part of the nervous system responsible for processing sensory information. A sensory system consists of sensory neurons (including the sensory receptor cells), neural pathways, and parts of the brain involved in sensory perception. Commonly recognized sensory systems are those for vision, hearing, touch, taste, smell, and balance. In short, senses are transducers from the physical world to the realm of the mind where we interpret the information, creating our perception of the world around us.[1]\nOrganisms need information to solve at least three kinds of problems: (a) to maintain an appropriate environment, i.e., homeostasis; (b) to time activities (e.g., seasonal changes in behavior) or synchronize activities with those of conspecifics; and (c) to locate and respond to resources or threats (e.g., by moving towards resources or evading or attacking threats). Organisms also need to transmit information in order to influence another's behavior: to identify themselves, warn conspecifics of danger, coordinate activities, or deceive.[2]\nThe receptive field is the area of the body or environment to which a receptor organ and receptor cells respond. For instance, the part of the world an eye can see, is its receptive field; the light that each rod or cone can see, is its receptive field.[3] Receptive fields have been identified for the visual system, auditory system and somatosensory system.Stimulus[edit]Sensory systems code for four aspects of a stimulus; type (modality), intensity, location, and duration. Arrival time of a sound pulse and phase differences of continuous sound are used for sound localization. Certain receptors are sensitive to certain types of stimuli (for example, different mechanoreceptors respond best to different kinds of touch stimuli, like sharp or blunt objects). Receptors send impulses in certain patterns to send information about the intensity of a stimulus (for example, how loud a sound is). The location of the receptor that is stimulated gives the brain information about the location of the stimulus (for example, stimulating a mechanoreceptor in a finger will send information to the brain about that finger). The duration of the stimulus (how long it lasts) is conveyed by firing patterns of receptors. These impulses are transmitted to the brain through afferent neurons.Senses and receptors[edit]While debate exists among neurologists as to the specific number of senses due to differing definitions of what constitutes a sense, Gautama Buddha and Aristotle classified five \u2018traditional\u2019 human senses which have become universally accepted: touch, taste, smell, sight, and hearing. Other senses that have been well-accepted in most mammals, including humans, include nociception, equilibrioception, kinaesthesia, and thermoception. Furthermore, some nonhuman animals have been shown to possess alternate senses, including magnetoception and electroreception.[4]Receptors[edit]The initialization of sensation stems from the response of a specific receptor to a physical stimulus. The receptors which react to the stimulus and initiate the process of sensation are commonly characterized in four distinct categories: chemoreceptors, photoreceptors, mechanoreceptors, and thermoreceptors. All receptors receive distinct physical stimuli and transduce the signal into an electrical action potential. This action potential then travels along afferent neurons to specific brain regions where it is processed and interpreted.[5]Chemoreceptors[edit]Chemoreceptors, or chemosensors, detect certain chemical stimuli and transduce that signal into an electrical action potential. The two primary types of chemoreceptors are:\nDistance chemoreceptors are integral to receiving stimuli in the olfactory system through both olfactory receptor neurons and neurons in the vomeronasal organ.\nDirect chemoreceptors include the taste buds in the gustatory system as well as receptors in the aortic bodies which detect changes in oxygen concentration.[6]\nPhotoreceptors[edit]Photoreceptors are capable of phototransduction, a process which converts light (electromagnetic radiation) into, among other types of energy, a membrane potential. The three primary types of photoreceptors are: Cones are photoreceptors which respond significantly to color. In humans the three different types of cones correspond with a primary response to short wavelength (blue), medium wavelength (green), and long wavelength (yellow/red).[7] Rods are photoreceptors which are very sensitive to the intensity of light, allowing for vision in dim lighting. The concentrations and ratio of rods to cones is strongly correlated with whether an animal is diurnal or nocturnal. In humans rods outnumber cones by approximately 20:1, while in nocturnal animals, such as the tawny owl, the ratio is closer to 1000:1.[7] Ganglion Cells reside in the adrenal medulla and retina where they are involved in the sympathetic response. Of the ~1.3 million ganglion cells present in the retina, 1-2% are believed to be photosensitive ganglia.[8] These photosensitive ganglia play a role in conscious vision for some animals,[9] and are believed to do the same in humans.[10]Mechanoreceptors[edit]Mechanoreceptors are sensory receptors which respond to mechanical forces, such as pressure or distortion.[11] While mechanoreceptors are present in hair cells and play an integral role in the vestibular and auditory systems, the majority of mechanoreceptors are cutaneous and are grouped into four categories:\nSlowly adapting type 1 receptors have small receptive fields and respond to static stimulation. These receptors are primarily used in the sensations of form and roughness.\nSlowly adapting type 2 receptors have large receptive fields and respond to stretch. Similarly to type 1, they produce sustained responses to a continued stimuli.\nRapidly adapting receptors have small receptive fields and underlie the perception of slip.\nPacinian receptors have large receptive fields and are the predominant receptors for high-frequency vibration.\nThermoreceptors[edit]Thermoreceptors are sensory receptors which respond to varying temperatures. While the mechanisms through which these receptors operate is unclear, recent discoveries have shown that mammals have at least two distinct types of thermoreceptors:[12]\nThe end-bulb of Krause, or bulboid corpuscle, detects temperatures above body temperature.\nRuffini\u2019s end organ detects temperatures below body temperature.\nNociceptors[edit]Nociceptors respond to potentially damaging stimuli by sending signals to the spinal cord and brain. This process, called nociception, usually causes the perception of pain.[13] They are found in internal organs, as well as on the surface of the body. Nociceptors detect different kinds of damaging stimuli or actual damage. Those that only respond when tissues are damaged are known as sleeping or silent nociceptors.\nThermal nociceptors are activated by noxious heat or cold at various temperatures.\nMechanical nociceptors respond to excess pressure or mechanical deformation.\nChemical nociceptors respond to a wide variety of chemicals, some of which are signs of tissue damage. They are involved in the detection of some spices in food.\nSensory cortex[edit]All stimuli received by the receptors listed above are transduced to an action potential, which is carried along one or more afferent neurons towards a specific area of the brain. While the term sensory cortex is often used informally to refer to the somatosensory cortex, the term more accurately refers to the multiple areas of the brain at which senses are received to be processed. For the five traditional senses in humans, this includes the primary and secondary cortexes of the different senses: the somatosensory cortex, the visual cortex, the auditory cortex, the primary olfactory cortex, and the gustatory cortex.[14] Other modalities have corresponding sensory cortex areas as well, including the vestibular cortex for the sense of balance.[15]Somatosensory cortex[edit]Located in the parietal lobe, the primary somatosensory cortex is the primary receptive area for the sense of touch and proprioception in the somatosensory system. This cortex is further divided into Brodmann areas 1, 2, and 3. Brodmann area 3 is considered the primary processing center of the somatosensory cortex as it receives significantly more input from the thalamus, has neurons highly responsive to somatosensory stimuli, and can evoke somatic sensations through electrical stimulation. Areas 1 and 2 receive most of their input from area 3. There are also pathways for proprioception (via the cerebellum), and motor control (via Brodmann area 4). See also: S2 Secondary somatosensory cortex.Visual cortex[edit]The visual cortex refers to the primary visual cortex, labeled V1 or Brodmann area 17, as well as the extrastriate visual cortical areas V2-V5.[16] Located in the occipital lobe, V1 acts as the primary relay station for visual input, transmitting information to two primary pathways labeled the dorsal and ventral streams. The dorsal stream includes areas V2 and V5, and is used in interpreting visual \u2018where\u2019 and \u2018how.\u2019 The ventral stream includes areas V2 and V4, and is used in interpreting \u2018what.\u2019[17] Increases in Task-negative activity are observed in the ventral attention network, after abrupt changes in sensory stimuli,[18] at the onset and offset of task blocks,[19] and at the end of a completed trial.[20]Auditory cortex[edit]Located in the temporal lobe, the auditory cortex is the primary receptive area for sound information. The auditory cortex is composed of Brodmann areas 41 and 42, also known as the anterior transverse temporal area 41 and the posterior transverse temporal area 42, respectively. Both areas act similarly and are integral in receiving and processing the signals transmitted from auditory receptors.Primary olfactory cortex[edit]Located in the temporal lobe, the primary olfactory cortex is the primary receptive area for olfaction, or smell. Unique to the olfactory and gustatory systems, at least in mammals, is the implementation of both peripheral and central mechanisms of action. The peripheral mechanisms involve olfactory receptor neurons which transduce a chemical signal along the olfactory nerve, which terminates in the olfactory bulb. The chemo-receptors involved in olfactory nervous cascade involve using G-protein receptors to send their chemical signals down said cascade. The central mechanisms include the convergence of olfactory nerve axons into glomeruli in the olfactory bulb, where the signal is then transmitted to the anterior olfactory nucleus, the piriform cortex, the medial amygdala, and the entorhinal cortex, all of which make up the primary olfactory cortex.In contrast to vision and hearing, the olfactory bulbs are not cross-hemispheric; the right bulb connects to the right hemisphere and the left bulb connects to the left hemisphere.Gustatory cortex[edit]The gustatory cortex is the primary receptive area for taste. The word taste is used in a technical sense to refer specifically to sensations coming from taste buds on the tongue. The five qualities of taste detected by the tongue include sourness, bitterness, sweetness, saltiness, and the protein taste quality, called umami. In contrast, the term flavor refers to the experience generated through integration of taste with smell and tactile information. The gustatory cortex consists of two primary structures: the anterior insula, located on the insular lobe, and the frontal operculum, located on the frontal lobe. Similarly to the olfactory cortex, the gustatory pathway operates through both peripheral and central mechanisms. Peripheral taste receptors, located on the tongue, soft palate, pharynx, and esophagus, transmit the received signal to primary sensory axons, where the signal is projected to the nucleus of the solitary tract in the medulla, or the gustatory nucleus of the solitary tract complex. The signal is then transmitted to the thalamus, which in turn projects the signal to several regions of the neocortex, including the gustatory cortex.[21]The neural processing of taste is affected at nearly every stage of processing by concurrent somatosensory information from the tongue, that is, mouthfeel. Scent, in contrast, is not combined with taste to create flavor until higher cortical processing regions, such as the insula and orbitofrontal cortex.[22]Human sensory system[edit]The human sensory system consists of the following subsystems:\nVisual system consists of the photoreceptor cells, optic nerve, and V1\nAuditory system\nSomatosensory system consists of the receptors, transmitters (pathways) leading to S1, and S1 that experiences the sensations labelled as touch or pressure, temperature (warm or cold), pain (including itch and tickle), and the sensations of muscle movement and joint position including posture, movement, and facial expression (collectively also called proprioception)\nGustatory system\nOlfactory system\nVestibular system\nDiseases[edit]\nAmblyopia\nAnacusis\nColor blindness\nDeafness\nSee also[edit]\nMultisensory integration\nNeural adaptation\nNeural coding\nSensor\nSensory augmentation\nSensory neuroscience\nReferences[edit]External links[edit]", "subtitles": ["Stimulus", "Senses and receptors", "Sensory cortex", "Human sensory system", "Diseases", "See also", "References", "External links"], "title": "Sensory nervous system"},
{"content": "A political system is a system of politics and government. It is usually compared to the legal system, economic system, cultural system, and other social systems. However, this is a very simplified view of a much more complex system of categories involving the questions of who should have authority and what the government's influence on its people and economy should be.Anthropological forms[edit]Anthropologists generally recognize four kinds of political systems, two of which are uncentralized and two of which are centralized.[1]\nUncentralized systems\n\nBand society\n\nSmall family group, no larger than an extended family or clan; it has been defined as consisting of no more than 30 to 50 individuals.\nA band can cease to exist if only a small group walks out.\n\n\nTribe\n\nGenerally larger, consisting of many families. Tribes have more social institutions, such as a chief or elders.\nMore permanent than bands. Many tribes are sub-divided into bands.\n\n\n\n\nCentralized governments\n\nChiefdom\n\nMore complex than a tribe or a band society, and less complex than a state or a civilization\nCharacterized by pervasive inequality and centralization of authority.\nA single lineage/family of the elite class becomes the ruling elite of the chiefdom\nComplex chiefdoms have two or even three tiers of political hierarchy.\nAn autonomous political unit comprising a number of villages or communities under the permanent control of a paramount chief\n\n\nSovereign state\n\nA sovereign state is a state with a permanent population, a defined territory, a government and the capacity to enter into relations with other sovereign states.\n\n\n\n\nSupranational political systems\n\nSupranational political systems are created by independent nations to reach a common goal or gain strength from forming an alliance.\n\n\nEmpires\n\nEmpires are widespread states or communities under a single rule. They are characterized by the rulers desire for unanimous religious affiliation or posing as threat for other empires in times of war.* Empires often[which?] made considerable progress in ways of democratic structures, creating and building city infrastructures, and maintaining civility within the diverse communities. Because of the intricate organization of the empires, they were often able to hold a large majority of power on a universal level.*\n\n\nLeagues\n\nLeagues are international organizations composed of states coming together for a single common purpose.* In this way leagues are different from empires, as they only seek to fulfill a single goal. Often leagues are formed on the brink of a military or economic downfall. Meetings and hearings are conducted in a neutral location with representatives of all involved nations present.\n\n\nSociology[edit]The sociological interest in political systems is figuring out who holds power within the relationship of the government and its people and how the government\u2019s power is used. There are three types of political systems that sociologists consider:\nAuthoritarianism\n\nIn authoritarian governments, the people have no power or representation and it is characterized by absolute or blind obedience to [formal] authority, as against individual freedom and related to the expectation of unquestioning obedience. The elite leaders handle all economic, military, and foreign relations. A prime example of authoritarianism is a dictatorship.\nTotalitarianism is the most extreme form of authoritarianism because it controls all aspects of life including the communication between citizens, media censorship, and threatens by the means of terror.\n\n\nMonarchies\n\nA monarchy is a government controlled by a king or queen determined by a predisposed line of sovereignty. In other words, it can be seen as an undivided rule or absolute sovereignty by a single person. In the modern world there are two types of monarchies, absolute monarchies and constitutional monarchies. An absolute monarchy works like a dictatorship in that the king has complete rule over his country. A constitutional monarchy gives the royal family limited powers and usually works in accordance with an elected body of officials. Social revolutions of the 18th, 19th, and 20th century overthrew the majority[citation needed] of existing monarchies in favor of more democratic governments and a rising middle class, as well as of authoritarian regimes like the Soviet Union.\n\n\nDemocracy\n\nA democracy is a form of government in which the citizens create and vote for laws directly, or indirectly via representatives (democratic republic). The idea of democracy stems back from ancient Greece and the profound works of ancient academics. However, the presence of democracy does not always mean citizen\u2019s wishes will be equally represented. For example, in many democratic countries[which?] immigrants, and racial and ethnic minorities, do not receive the same rights[clarification needed] as the majority citizens.[according to whom?]\n\n\nSee also[edit]\nGovernment\nJacobism\nMing dynasty\nPolitical structure\nPolity\nRomanovs\nTractatus Politicus\nTudors\nVoting system\nNotes[edit]References[edit]External links[edit]\nFor further resources on political theory and the mechanics of political system design, see the Governance and Social Development Resource Centre's topic guide on political systems\n", "subtitles": ["Anthropological forms", "Sociology", "See also", "Notes", "References", "External links"], "title": "Political system"},
{"content": "Adi Shamir (Hebrew: \u05e2\u05d3\u05d9 \u05e9\u05de\u05d9\u05e8\u200e; born July 6, 1952) is an Israeli cryptographer. He is a co-inventor of the RSA algorithm (along with Ron Rivest and Len Adleman), a co-inventor of the Feige\u2013Fiat\u2013Shamir identification scheme (along with Uriel Feige and Amos Fiat), one of the inventors of differential cryptanalysis and has made numerous contributions to the fields of cryptography and computer science.Education[edit]Born in Tel Aviv, Shamir received a BSc degree in mathematics from Tel Aviv University in 1973 and obtained his MSc and PhD degrees in Computer Science from the Weizmann Institute in 1975 and 1977 respectively. His thesis was titled, Fixed Points of Recursive Programs and their Relation in Differential Agard Calculus. After a year postdoc at University of Warwick, he did research at MIT from 1977\u20131980 before returning to be a member of the faculty of Mathematics and Computer Science at the Weizmann Institute. Starting from 2006, he is also an invited professor at E\u0301cole Normale Supe\u0301rieure in Paris.Research[edit]In addition to RSA, Shamir's other numerous inventions and contributions to cryptography include the Shamir secret sharing scheme, the breaking of the Merkle-Hellman knapsack cryptosystem, visual cryptography, and the TWIRL and TWINKLE factoring devices. Together with Eli Biham, he discovered differential cryptanalysis, a general method for attacking block ciphers. It later emerged that differential cryptanalysis was already known \u2014 and kept a secret \u2014 by both IBM[1] and the NSA.[2]Shamir has also made contributions to computer science outside of cryptography, such as finding the first linear time algorithm for 2-satisfiability[3] and showing the equivalence of the complexity classes PSPACE and IP.Awards[edit]Shamir has received a number of awards, including the following:\nthe 2002 ACM Turing Award, together with Rivest and Adleman, in recognition of his contributions to cryptography[4]\nthe Paris Kanellakis Theory and Practice Award;[5]\nthe Erdo\u030bs Prize of the Israel Mathematical Society,\nthe 1986 IEEE W.R.G. Baker Award[6]\nthe UAP Scientific Prize;\nThe Vatican's PIUS XI Gold Medal;\nthe 2000 IEEE Koji Kobayashi Computers and Communications Award[7]\nthe Israel Prize, in 2008, for computer sciences.[8][9]\nan honorary DMath (Doctor of Mathematics) degree from the University of Waterloo[10]\n2017 (33rd) Japan Prize in the field of Electronics, Information and Communication for his contribution to information security through pioneering research on cryptography [11]\nSee also[edit]\nList of Israel Prize recipients\nNDS Group\nFluhrer, Mantin and Shamir attack\nReferences[edit]External links[edit]\nAdi Shamir at DBLP Bibliography Server\nAdi Shamir's US Patents, 1976-present\n", "subtitles": ["Education", "Research", "Awards", "See also", "References", "External links"], "title": "Adi Shamir"},
{"content": "Ronald Linn Rivest (/r\u026a\u02c8v\u025bst/;[5][6] born May 6, 1947) is a cryptographer and an Institute Professor at MIT.[2] He is a member of MIT's Department of Electrical Engineering and Computer Science (EECS) and a member of MIT's Computer Science and Artificial Intelligence Laboratory (CSAIL). He was a member of the Election Assistance Commission's Technical Guidelines Development Committee, tasked with assisting the EAC in drafting the Voluntary Voting System Guidelines.[7]Rivest is one of the inventors of the RSA algorithm (along with Adi Shamir and Len Adleman).[1] He is the inventor of the symmetric key encryption algorithms RC2, RC4, RC5, and co-inventor of RC6. The RC stands for Rivest Cipher, or alternatively, Ron's Code. (RC3 was broken at RSA Security during development; similarly, RC1 was never published.) He also authored the MD2, MD4, MD5 and MD6 cryptographic hash functions. In 2006, he published his invention of the ThreeBallot voting system, a voting system that incorporates the ability for the voter to discern that their vote was counted while still protecting their voter privacy. Most importantly, this system does not rely on cryptography at all. Stating Our democracy is too important, he simultaneously placed ThreeBallot in the public domain.Rivest frequently collaborates with other researchers in combinatorics; for example working with David A. Klarner to find an upper bound on the number of polyominoes of a given order,[8] and working with Jean Vuillemin to prove the deterministic form of the Aanderaa\u2013Rosenberg conjecture.[9]Education[edit]Rivest earned a Bachelor's degree in Mathematics from Yale University in 1969, and a Ph.D. degree in Computer Science from Stanford University in 1974 for research supervised by Robert W. Floyd.[3]Career and research[edit]Rivest is a co-author of Introduction to Algorithms (also known as CLRS), a standard textbook on algorithms, with Thomas H. Cormen, Charles E. Leiserson and Clifford Stein. He is a member of the MIT Computer Science and Artificial Intelligence Laboratory (CSAIL) in the Theory of Computation Group, and a founder of its Cryptography and Information Security Group. He was also a founder of RSA Data Security (now merged with Security Dynamics to form RSA Security), Verisign, and of Peppercoin. Rivest has research interests in algorithms, cryptography and voting.[2] His former doctoral students include Avrim Blum,[3] Burt Kaliski,[3] Ron Pinter,[3] Robert Schapire,[3] Alan Sherman,[3] and Mona Singh.[4]Publications[edit]His publications[2] include:\nCormen, Thomas H.; Leiserson, Charles; Rivest, Ronald (1990). Introduction to Algorithms (first ed.). MIT Press and McGraw-Hill. ISBN 0-262-03141-8. \nCormen, Thomas H.; Leiserson, Charles; Rivest, Ronald; Stein, Clifford (2001). Introduction to Algorithms (second ed.). MIT Press and McGraw-Hill. ISBN 0-262-53196-8. \nCormen, Thomas H.; Leiserson, Charles; Rivest, Ronald; Stein, Clifford (2009). Introduction to Algorithms (third ed.). MIT Press. ISBN 0-262-03384-4. \nHonors and awards[edit]Rivest is a member of the National Academy of Engineering, the National Academy of Sciences, and is a Fellow of the Association for Computing Machinery, the International Association for Cryptologic Research, and the American Academy of Arts and Sciences. Together with Adi Shamir and Len Adleman, he has been awarded the 2000 IEEE Koji Kobayashi Computers and Communications Award and the Secure Computing Lifetime Achievement Award. He also shared with them the Turing Award. Rivest has received an honorary degree (the laurea honoris causa) from the Sapienza University of Rome.[10] In 2005, he received the MITX Lifetime Achievement Award. Rivest was named the 2007 the Marconi Fellow, and on May 29, 2008 he also gave the Chesley lecture at Carleton College. He was named an Institute Professor at MIT in June 2015.[11]References[edit]External links[edit]\nList of Ron Rivest's patents on IPEXL\nHome page of Ronald L. Rivest\nOfficial site of RSA Security Inc.\nRon Rivest election research papers\nThe ThreeBallot Voting System (PDF)\n", "subtitles": ["Education", "Career and research", "References", "External links"], "title": "Ron Rivest"},
{"content": "James Nicholas Gray (1944 \u2013 2007) was an American computer scientist who received the Turing Award[4] in 1998 for seminal contributions to database and transaction processing research and technical leadership in system implementation.Early years and personal life[edit]Gray was born in San Francisco, California, the second child of a mother who was a teacher and a father in the U.S. Army; the family moved to Rome where Gray spent most of the first three years of his life, learning to speak Italian before English.[2] The family then moved to Virginia, spending about four years there, until Gray's parents divorced, after which he returned to San Francisco with his mother.[2] His father, an amateur inventor, patented a design for a ribbon cartridge for typewriters that earned him a substantial royalty stream.[2]After being turned down for the Air Force Academy he entered the University of California, Berkeley as a freshman in 1961.[2] To help pay for college he worked as a co-op for General Dynamics, where he learned to use a Monroe calculator. Discouraged by his chemistry grades, he left Berkeley for six months, returning after an experience in industry he later described as dreadful.[2] Gray earned his B.S. in Engineering Mathematics (Math and Statistics) in 1966.[5]After marrying, Gray moved with his wife Loretta to New Jersey, his wife's home state; she got a job as a teacher and he got one at Bell Labs working on a digital simulation that was to be part of Multics. At Bell, he worked three days a week and spent two days as a Master's student at New York University's Courant Institute. After a year they traveled for several months before settling again in Berkeley, where Gray entered graduate school with Michael A. Harrison as his advisor. In 1969 he received his Ph.D. in programming languages, then did two years of post-doctoral work for IBM.[2]While at Berkeley, Gray and Loretta had a daughter; they were later divorced. His second wife was Donna Carnes.Research[edit]Gray pursued his career primarily working as a researcher and software designer at a number of industrial companies, including IBM, Tandem Computers, and DEC. He joined Microsoft in 1995 and was a Technical Fellow for the company[1][6][7][8][9][10][11][12][13] until he was lost at sea in 2007.[14]Gray contributed to several major database and transaction processing systems. IBM's System R was the precursor of the SQL relational databases that have become a standard throughout the world. For Microsoft, he worked on TerraServer-USA and Skyserver.Among his best known achievements are:\ngranular database locking[15]\ntwo-tier transaction commit semantics\nthe five-minute rule for allocating storage\nthe data cube operator for data warehousing applications\ndescribing the requirements for reliable transaction processing (memorably called the ACID test) and implementing them in software.\nHe assisted in the development of Virtual Earth.[16][17][18][19] He was also one of the co-founders of the Conference on Innovative Data Systems Research.Disappearance[edit]Gray, an experienced sailor, owned a forty-foot yacht. On January 28, 2007 he failed to return from a short solo trip to the Farallon Islands near San Francisco to scatter his mother's ashes. The weather was clear, and no distress call was received, nor was any signal detected from the boat's automatic Emergency Position-Indicating Radio Beacon.A four-day Coast Guard search using using planes, helicopters, and boats found nothing.[20][21][22][23] On February 1, 2007, the DigitalGlobe satellite did a scan of the area[24] and the thousands of images were posted to Amazon Mechanical Turk. Students, colleagues, and friends of Gray, and computer scientists around the world formed a Jim Gray Group to study these images for clues. On February 16 this search was suspended,[25] and an underwater search using sophisticated equipment ended May 31.[9][26][27][28][29][30]The University of California, Berkeley and Gray's family hosted a tribute on May 31, 2008.[31] Microsoft's WorldWide Telescope software is dedicated to Gray. In 2008, Microsoft opened a research center in Madison, Wisconsin, named after Jim Gray.[32] On January 28, 2012 Gray was declared legally dead.[33][34]Jim Gray eScience Award[edit]Each year, Microsoft Research presents the Jim Gray eScience Award[35] to a researcher who has made an outstanding contribution to the field of data-intensive computing. Award recipients are selected for their ground-breaking, fundamental contributions to the field of eScience. Previous award winners include Alex Szalay (2007), Carole Goble (2008), Jeff Dozier (2009), Phil Bourne (2010), Mark Abbott (2011), Antony John Williams (2012), and Dr. David Lipman, M.D. (2013).References[edit]External links[edit]\nGray's Microsoft Research home page, last accessed 23 June 2013\nJames (Jim) Nicholas Gray, Turing Award citation\nVideo Behind the Code on Channel 9, interviewed by Barbara Fox, 2005\nVideo The Future of Software and Databases, expert panel discussion with Rick Cattell, Don Chamberlin, Daniela Florescu, Jim Gray and Jim Melton, Software Development 2002 conference\nOral History Interview with Jim Gray, Charles Babbage Institute, University of Minnesota. Oral history interview by Philip L. Frana, 3 January 2002, San Francisco, California.\nThe Future of Databases, SQL Down Under. Interview with Dr Greg Low, 2005.\nTribute by Mark Whitehorn for The Register April 30, 2007\nEE380: The Search for Jim Gray, Panel Discussion at Stanford University (video archive) May 28, 2008\n(Proceedings) May 31, 2008\nTribute by James Hamilton\nWhy Do Computers Stop and What Can Be Done About It?, a technical report by Jim Gray, 1985\n", "subtitles": ["Early years and personal life", "Research", "Disappearance", "Jim Gray eScience Award", "References", "External links"], "title": "Jim Gray (computer scientist)"},
{"content": "Vinton Gray Cerf[2] ForMemRS,[1] (/s\u025c\u02d0rf/; born June 23, 1943) is an American Internet pioneer, who is recognized as one of[7] the fathers of the Internet,[8] sharing this title with TCP/IP co-inventor Bob Kahn.[9][10] His contributions have been acknowledged and lauded, repeatedly, with honorary degrees and awards that include the National Medal of Technology,[2] the Turing Award,[11] the Presidential Medal of Freedom,[12] the Marconi Prize and membership in the National Academy of Engineering.In the early days, Cerf was a manager for the United States' Defense Advanced Research Projects Agency (DARPA) funding various groups to develop TCP/IP technology. When the Internet began to transition to a commercial opportunity during the late 1980s,[citation needed] Cerf moved to MCI where he was instrumental in the development of the first commercial email system (MCI Mail) connected to the Internet.Cerf was instrumental in the funding and formation of ICANN from the start. He waited a year before stepping forward to join the ICANN Board, and eventually became chairman. He was elected as the president of the Association for Computing Machinery in May 2012,[13] and in August 2013 he joined the Council on CyberSecurity's Board of Advisors.[14]Cerf is active in many organizations that are working to help the Internet deliver humanitarian value to the world. He is supportive of innovative projects that are experimenting with new approaches to global problems, including the digital divide, the gender gap, and the changing nature of jobs. Cerf is also known for his sartorial style, typically appearing in a three-piece suit\u2014a rarity in an industry known for its casual dress norms.[15][16]Life and career[edit]Cerf was born in New Haven, Connecticut, the son of Muriel (ne\u0301e Gray), a housewife, and Vinton Thurston Cerf, an aerospace executive.[17][18] Cerf went to Van Nuys High School in California along with Jon Postel and Steve Crocker; he wrote the former's obituary. Both were also instrumental in the creation of the Internet. While in high school, Cerf worked at Rocketdyne on the Apollo program, including helping to write statistical analysis software for the non-destructive tests of the F-1 engines.[19] Cerf's first job after obtaining his B.S. degree in mathematics from Stanford University was at IBM, where he worked for two years as a systems engineer supporting QUIKTRAN.[2] He left IBM to attend graduate school at UCLA where he earned his M.S. degree in 1970 and his PhD degree in 1972.[6][20] During his graduate student years, he studied under Professor Gerald Estrin, worked in Professor Leonard Kleinrock's data packet networking group that connected the first two nodes of the ARPANet,[21] the predecessor[21] to the Internet, and contributed to a host-to-host protocol for the ARPANet.[22] While at UCLA, he also met Bob Kahn, who was working on the ARPANet hardware architecture.[22] After receiving his doctorate, Cerf became an assistant professor at Stanford University from 1972\u20131976, where he conducted research on packet network interconnection protocols and co-designed the DoD TCP/IP protocol suite with Kahn.[22] Cerf then moved to DARPA in 1976, where he stayed until 1982.As vice president of MCI Digital Information Services from 1982 to 1986, Cerf led the engineering of MCI Mail, the first commercial email service to be connected to the Internet. In 1986, he joined Bob Kahn at the Corporation for National Research Initiatives as its vice president, working with Kahn on Digital Libraries, Knowledge Robots, and gigabit speed networks. It was during this time, in 1992, that he and Kahn, among others, founded the Internet Society (ISOC) to provide leadership in education, policy and standards related to the Internet. Cerf served as the first president of ISOC. Cerf rejoined MCI during 1994 and served as Senior Vice President of Technology Strategy. In this role, he helped to guide corporate strategy development from a technical perspective. Previously, he served as MCI's senior vice president of Architecture and Technology, leading a team of architects and engineers to design advanced networking frameworks, including Internet-based solutions for delivering a combination of data, information, voice and video services for business and consumer use.During 1997, Cerf joined the Board of Trustees of Gallaudet University, a university for the education of the deaf and hard-of-hearing.[23] Cerf himself is hard of hearing.[24] He has also served on the university's Board of Associates.[25]Cerf, as leader of MCI's internet business, was criticized due to MCI's role in providing the IP addresses used by Send-Safe.com, a vendor of spamware that uses a botnet in order to send spam. MCI refused to terminate the spamware vendor.[26][27] At the time, Spamhaus also listed MCI as the ISP with the most Spamhaus Block List listings.[28]Cerf has worked for Google as a Vice President and Chief Internet Evangelist since October 2005.[5] In this function he has become well known for his predictions on how technology will affect future society, encompassing such areas as artificial intelligence, environmentalism, the advent of IPv6 and the transformation of the television industry and its delivery model.[29]Since 2010, Cerf has served as a Commissioner for the Broadband Commission for Digital Development, a UN body which aims to make broadband internet technologies more widely available.Cerf joined the board of the Internet Corporation for Assigned Names and Numbers (ICANN) in 1999, and served until November 2007.[30] He was chairman from November 2000 to his departure from the Board.Cerf was a member of Bulgarian President Georgi Parvanov's IT Advisory Council (from March 2002 \u2013 January 2012). He is also a member of the Advisory Board of Eurasia Group, the political risk consultancy.[31]Cerf is also working on the Interplanetary Internet, together with NASA's Jet Propulsion Laboratory and other NASA laboratories. It will be a new standard to communicate from planet to planet, using radio/laser communications that are tolerant of signal degradations including variable delay and disruption caused, for example, by celestial motion.[32]On February 7, 2006, Cerf testified before the U.S. Senate Committee on Commerce, Science, and Transportation's hearing on network neutrality. Speaking as Google's Chief Internet Evangelist, Cerf noted that nearly half of all consumers lacked meaningful choice in broadband providers and expressed concerns that without network neutrality government regulation, broadband providers would be able to use their dominance to limit options for consumers and charge companies like Google for their use of bandwidth.[33]Cerf currently serves on the board of advisors of Scientists and Engineers for America, an organization focused on promoting sound science in American government.[34] He also serves on the advisory council of CRDF Global (Civilian Research and Development Foundation) and was on the International Multilateral Partnership Against Cyber Threats (IMPACT) International Advisory Board.[35]Cerf is chairman of the board of trustees of ARIN, the Regional Internet Registry (RIR) of IP addresses for United States, Canada, and part of the Caribbean.[36] Until Fall 2015, Cerf chaired the board of directors of StopBadware, a non-profit anti-malware organization that started as a project at Harvard University's Berkman Center for Internet & Society.[37][38] Cerf is on the board of advisors to The Liquid Information Company Ltd of the UK, which works to make the web more usefully interactive and which has produced the Mac OS X utility called \u2018Liquid'.[39] Vint Cerf is a member of the CuriosityStream Advisory Board.[40]During 2008 Cerf chaired the Internationalized domain name (IDNAbis) working group of the IETF.[41] In 2008 Cerf was a major contender to be designated the US's first Chief Technology Officer by President Barack Obama.[42] Cerf is the co-chair of Campus Party Silicon Valley, the US edition of one of the largest technology festivals in the world, along with Al Gore and Tim Berners-Lee.[43] From 2009\u20132011, Cerf was an elected member of the Governing Board of the Smart Grid Interoperability Panel (SGIP). SGIP is a public-private consortium established by NIST in 2009 and provides a forum for businesses and other stakeholder groups to participate in coordinating and accelerating development of standards for the evolving Smart Grid.[44] Cerf was elected to a two-year term as President of the Association for Computing Machinery (ACM) beginning July 1, 2012.[45] In 2015 Cerf co-founded (with Mei Lin Fung), and is currently chairman of, People-Centered Internet (PCI).[46] On January 16, 2013, US President Barack Obama announced his intent to appoint Cerf to the National Science Board.[47]Cerf is also among the 15 members of governing council of International Institute of Information Technology, Hyderabad.[48]In June 2016 his work with NASA led to Delay-tolerant networking being installed on the International Space Station with an aim towards an Interplanetary Internet.[49]Awards and honors[edit]Cerf has received a number of honorary degrees, including doctorates, from the University of the Balearic Islands, ETHZ in Zurich, Switzerland, Capitol College, Gettysburg College, Yale University, George Mason University, Marymount University, Bethany College (Kansas), University of Pisa, University of Rovira and Virgili (Tarragona, Spain), Rensselaer Polytechnic Institute, Lulea\u030a University of Technology (Sweden), University of Twente (Netherlands), Beijing University of Posts and Telecommunications, Tsinghua University (Beijing), Brooklyn Polytechnic, UPCT (University of Cartagena, Spain), Zaragoza University (Spain), University of Reading (United Kingdom), Royal Roads University (Canada), MGIMO (Moscow State University of International Relations), Buenos Aires Institute of Technology (Argentina), Polytechnic University of Madrid, Keio University (Japan), University of South Australia (Australia), University of St Andrews (Scotland), University of Pittsburgh and [50] Gallaudet University (United States). Other awards include:See also[edit]\nList of pioneers in computer science\nPartial bibliography[edit]Author[edit]Co-author[edit]References[edit]Further reading[edit]\nCerf, Vinton G. (April 24, 1990), Oral history interview with Vinton G. Cerf, Minnesota, Minneapolis: Charles Babbage Institute \nCerf, Vinton (May 17, 1999), Dr. Vinton Cerf: An Interview Conducted by David Hochfelder, Institute of Electrical and Electronics Engineers \nExternal links[edit]\nBio at Google\nVint Cerf on the ICANN wiki\nDr. Vint Cerf on Reinventing the Internet (YouTube). Internet Society. (May 13, 2013)\nVint Cerf at TED \n", "subtitles": ["Life and career", "Awards and honors", "See also", "Partial bibliography", "References", "Further reading", "External links"], "title": "Vint Cerf"},
{"content": "The Biografisch Portaal (Biography Portal) is an initiative based at the Huygens Institute for Dutch History in The Hague, with the aim of making biographical texts of the Netherlands more accessible.The project was started in February 2010 with material for 40,000 digitized biographies, with the goal to grant digital access to all reliable information about (deceased) people of the Netherlands from the earliest beginnings of history up to modern times.[1]The Netherlands as a geographic term includes former colonies, and the term people refers both to people born in the Netherlands and its former colonies, and also to people born elsewhere but active in the Netherlands and its former colonies. As of 2011[update], only biographical information about deceased people is included. The system used is based on the standards of the Text Encoding Initiative. Access to the Biografisch Portaal is available free through a web-based interface.The project is a cooperative undertaking by ten scientific and cultural bodies in the Netherlands with the Huygens Institute as main contact. The other bodies are:\nThe Biografie Instituut\nThe Centraal Bureau voor Genealogie (CBG)\nThe Digital Library for Dutch Literature (DBNL)\nData Archiving and Networked Services (DANS)\nThe International Institute of Social History (IISG)\nThe Onderzoekscentrum voor Geschiedenis en Cultuur (OGC),\nThe Parlementair Documentatie Centrum (PDC)\nThe Netherlands Institute for Art History (RKD)\nBesides ongoing digital projects, Dutch biographical dictionaries originally published in book form that have been digitized and incorporated into the indexes of the Biografisch Portaal are:[2]\nThe work of Abraham van der Aa, which was the first Dutch biographical dictionary\nThe BWN, or Biografisch Woordenboek van Nederland\nThe NNBW, or Nieuw Nederlandsch Biografisch Woordenboek\nThe work of Johan Engelbert Elias on the Amsterdam regency known as Vroedschap van Amsterdam\nThe work of Barend Glasius known as Godgeleerd Nederland\nThe work of Roeland van Eynden and Adriaan van der Willigen, known as Geschiedenis der vaderlandsche schilderkunst\nThe work of Jan van Gool known as Nieuwe Schouburg\nThe work of Jacob Campo Weyerman known as The Lives of Dutch painters and paintresses[3]\nThe BLNP, or Biografisch lexicon voor de geschiedenis van het Nederlands protestantisme\nAs of November 2012 the Biografisch Portaal contained 80,206 persons in 125,592 biographies. In February 2012, a new project was started called BiographyNed to build an analytical tool for use with the Biografisch Portaal that will link biographies to events in time and space.[4] The main goal of the three-year project is to formulate \u2018the boundaries of the Netherlands\u2019.[5]See also[edit]\nList of Dutch people\nReferences[edit]External links[edit]\nOfficial website\n", "subtitles": [], "title": "Biografisch Portaal"},
{"content": "The following is a partial list of social science journals, including history and area studies. There are thousands of academic journals covering the social sciences in publication, and many more have been published at various points in the past. The list given here is far from exhaustive, and contains the most influential, currently publishing journals in each field. As a rule of thumb, each field should be represented by at most ten positions, chosen by their impact factors and other ratings. -reviewed journals. They are not listed here.For a list of periodicals in the physical, life, and applied sciences, see List of scientific journals.Anthropology[edit]\nAmerican Anthropologist\nChungara\nCultural Survival\nCurrent Anthropology\nSocial Evolution & History\nStructure and Dynamics: eJournal of the Anthropological and Related Sciences\nTerrain\nArchaeology[edit]\nAmerican Antiquity\nAmerican Journal of Archaeology\nAncient TL\nAntiquity\nArabian Archaeology and Epigraphy\nInternational Journal of South American Archaeology\nJournal of Anthropological Archaeology\nLatin American Antiquity\nArea studies[edit]\nAbbia: Cameroon Cultural Review\nAmerican Journal of Chinese Studies\nAmerican Quarterly\nAzure\nCentral Asia Monitor\nCentral Asian Review\nCentral European Journal of International and Security Studies\nDebatte: Journal of Contemporary Central and Eastern Europe\nHarvard Journal of Asiatic Studies\nIranian Studies Journal\nJournal of the American Research Center in Egypt\nJournal of Asian Studies\nJournal of Japanese Studies\nJournal of Latin American Studies\nJournal of Modern Greek Studies\nJournal of Near Eastern Studies\nLatin American Perspectives\nMonumenta Nipponica\nNorte Grande Geography Journal\nPacific Northwest Quarterly\nPalestine-Israel Journal\nProblems of Post-Communism\nRevista Universum\nSarmatian Review\nSOAS Bulletin of Burma Research\nBusiness, Management & Organization Theory[edit]\nAcademy of Management Review\nAcademy of Management Journal\nBeta. Scandinavian Journal of Business Research\nBusiness and Professional Communication Quarterly\nBusiness Process Management Journal\nInternational Marketing Review\nInternational Small Business Journal\nJournal of Media Business Studies\nManagement Science\nMeasuring Business Excellence\nOrganization Development Journal\nOrganization Science\nAdministrative Science Quarterly\nJournal of Management\nStrategic Management Journal\nSmall Enterprises Development, Management & Extension (Sedme) Journal\nCommunication[edit]\nCommunication Monographs\nCommunication Research\nCommunication Theory\nHuman Communication Research\nJournal of Communication\nDemography[edit]\nCanadian Studies in Population\nDemography\nInternational Migration Review\nJournal of Population Economics\nPopulation and Development Review\nEconomics[edit]\nAmerican Economic Review\nEconomic Journal\nJournal of Economic Literature\nJournal of Financial Counseling and Planning\nJournal of Financial Economics\nJournal of Political Economy\nQuarterly Journal of Economics\nEducation and educational technology[edit]\nAmerican Educational Research Journal\nCollege Teaching\nEducational Administration Quarterly\nEducational Researcher\nEducational Technology & Society\nHarvard Educational Review\nJournal of Higher Education\nResearch in Learning Technology\nReview of Educational Research\nStudies in Higher Education\nTeachers College Record\nEnvironmental social science[edit]\nEcology and Society\nEnvironmental Research Letters\nEnvironmental Values\nJournal of Political Ecology\nNature and Culture\nOrganization & Environment\nPopulation and Environment\nGeography[edit]\nAntipode\nArea\nThe Geographical Journal\nJournal of Biogeography\nJournal of Quaternary Science\nPolar Research\nThe Professional Geographer\nTransactions of the Institute of British Geographers\nHistory[edit]\nAmerican Historical Review\nJournal of American History\nJournal of the American Research Center in Egypt\nPast & Present\nLaw[edit]\nCalifornia Law Review\nHarvard Law Review\nLaw & Critique\nMichigan Law Review\nStanford Law Review\nYale Law Journal\nMedicine[edit]\nSocial Science & Medicine\nPlanning[edit]\nEnvironment and Planning\nJournal of Planning Education and Research\nJournal of Planning History\nJournal of Planning Literature\nPlanning Theory\nUrban Geography\nPolitical science[edit]\nAmerican Journal of Political Science\nAmerican Political Science Review\nAnnual Review of Political Science\nComparative Politics\nJournal of Conflict Resolution\nJournal of Democracy\nJournal of Politics & Society\nPolitical Geography\nPsychology[edit]Semiotics[edit]\nThe American Journal of Semiotics\nSemiotica\nSign Systems Studies\nSocial policy[edit]\nCritical Social Policy\nGlobal Social Policy\nInternational Journal of Social Welfare\nJournal of Disability Policy Studies\nJournal of European Social Policy\nRehabilitation Research, Policy, and Education\nSocial Choice and Welfare\nSocial work[edit]\nChildren & Society\nClinical Social Work Journal\nInternational Social Work\nJournal of Social Work\nQualitative Social Work\nResearch on Social Work Practice\nUrban Social Work\nSociology[edit]\nAmerican Journal of Sociology\nAmerican Sociological Review\nAnnual Review of Sociology\nBritish Journal of Sociology\nSocial Forces\nTourism[edit]\nInternational Journal of Tourism Sciences\nWomen's studies[edit]\nAsian Women\nGender & Society\nJournal of Women's Health\nPsychology of Women Quarterly\nSex Roles\nWomen & Health\nWomen's Health Issues\nSee also[edit]\nList of journals available free online\nList of academic databases and search engines\nWikibook Textbook\nExternal links[edit]\nSciences/Publications/ Social Science: Publications at Curlie (based on DMOZ)\nDirectories and Lists of Electronic Journals and Newsletters\nScholarly Journals Distributed Via the World Wide Web\n", "subtitles": ["Anthropology", "Archaeology", "Area studies", "Business, Management & Organization Theory", "Communication", "Demography", "Economics", "Education and educational technology", "Environmental social science", "Geography", "History", "Law", "Medicine", "Planning", "Political science", "Psychology", "Semiotics", "Social policy", "Social work", "Sociology", "Tourism", "Women's studies", "See also", "External links"], "title": "List of social science journals"},
{"content": "The following outline is provided as an overview of and topical guide to social science:Social science \u2013 branch of science concerned with society and human behaviors.What type of thing is social science?[edit]Social science can be described as all of the following:\nBranch of science \u2013 systematic enterprise that builds and organizes knowledge in the form of testable explanations and predictions about the universe.[1][2][3]\nMajor category of academic disciplines \u2013 an academic discipline is focused study in one academic field or profession. A discipline incorporates expertise, people, projects, communities, challenges, studies, inquiry, and research areas that are strongly associated with academic areas of study or areas of professional practice. For example, the branches of science are commonly referred to as the scientific disciplines. For instance, gravitation is strongly associated with the discipline of physics, and is considered to be part of that disciplinary knowledge.\nBranches of social science[edit]\nAnthropology - study of humans, past and present, that draws and builds upon knowledge from the social sciences and biological sciences, as well as the humanities and the natural sciences.\n\nAnthropology of religion \u2013 study of religious institutions in relation to other social institutions, and the comparison of religious beliefs and practices across cultures\nApplied anthropology \u2013 application of the method and theory of anthropology to the analysis and solution of practical problems.\nArchaeology \u2013 study of cultures via material remains and environmental data (Outline of archaeology)\nCultural anthropology \u2013 branch of anthropology focused on the study of cultural variation among humans, collecting data about the effect of global economic and political processes on local cultural realities.\nEthnobiology \u2013 scientific study of dynamic relationships between peoples, biota, and environments, from the distant past to the immediate present.\nEthnobotany \u2013 is the study of a region's plants and their practical uses through the traditional knowledge of a local culture and people.\nEthnography \u2013 systematic study of people and cultures.\nEthnology \u2013 branch of anthropology that compares and analyzes the origins, distribution, technology, religion, language, and social structure of the ethnic, racial, and/or national divisions of humanity.\nEthnopoetics \u2013 method of recording text versions of oral poetry or narrative performances (i.e., verbal lore) that uses poetic lines, verses, and stanzas (instead of prose paragraphs) to capture the formal, poetic performance elements which would otherwise be lost in the written texts.\nEvolutionary anthropology \u2013 interdisciplinary study of the evolution of human physiology and human behaviour and the relation between hominids and non-hominid primates.\nExperimental archaeology \u2013 Experimental archaeology employs a number of different methods, techniques, analyses, and approaches in order to generate and test hypotheses, based upon archaeological source material, like ancient structures or artifacts.\nHistorical archaeology \u2013 form of archaeology dealing with topics that are already attested in written records.\nLinguistic anthropology \u2013 is the interdisciplinary study of how language influences social life.\nMedical anthropology \u2013 interdisciplinary field which studies human health and disease, health care systems, and biocultural adaptation.\nPhysical anthropology \u2013 study of the physical development of the human species.\nPsychological anthropology \u2013 interdisciplinary subfield of anthropology that studies the interaction of cultural and mental processes.\nZooarchaeology \u2013 study of faunal remains.\nAnthrozoology \u2013 study of human-animal interaction.\n\n\nBusiness studies \u2013 academic area that consists of many sub-areas pertaining to the social relationships that comprise the human economic systems.\n\nAccountancy \u2013 the measurement, processing and communication of financial information about economic entities.\nFinance \u2013 a field dealing with the study of investments.\nHuman resource management \u2013 a function in organizations designed to maximize employee performance in service of an employer's strategic objectives.\nManagement \u2013 the administration of an organization, whether it be a business, a not-for-profit organization, or government body.\nMarketing \u2013 the study and management of exchange relationships.\nOrganizational studies \u2013 the examination of how individuals construct organizational structures, processes, and practices and how these, in turn, shape social relations and create institutions that ultimately influence people.\nEconomics \u2013 details of this area and its own sub-areas are provided in this taxonomy below.\n\n\nCivics \u2013 study of the theoretical and practical aspects of citizenship, its rights and duties; the duties of citizens to each other as members of a political body and to the government.\nCognitive Science \u2013 interdisciplinary scientific study of the mind and its processes. It examines what cognition is, what it does and how it works.\nCriminology \u2013 study of the nature, extent, causes, and control of criminal behavior in both the individual and in society.\nCultural studies \u2013 academic field grounded in critical theory and literary criticism.\nDemography \u2013 statistical study of human populations and sub-populations.\nDevelopment studies \u2013 multidisciplinary branch of social science which addresses issues of concern to developing countries.\nEconomics \u2013 analyzes the production, distribution, and consumption of goods and services. It aims to explain how economies work and how economic agents interact. (The term 'economics' is erroneously conflated with the current mainstream Neoclassical economics.)\n\nMacroeconomics \u2013 branch of economics dealing with the performance, structure, behavior, and decision-making of the whole economy\nMicroeconomics \u2013 branch of economics that studies the behavior of individual households and firms in making decisions on the allocation of limited resources\nBehavioural economics \u2013 Behavioral economics and the related field, behavioral finance, study the effects of social, cognitive and emotional factors on the economic decisions of individuals and institutions and the consequences for market prices, returns and the resource allocation.\nBioeconomics \u2013 applies the laws of thermodynamics to economic theory\nComparative economics \u2013 comparative study of different systems of economic organization, such as capitalism, socialism, feudalism and the mixed economy.\n\nSocialist economics \u2013 economic theories and practices of hypothetical and existing socialist economic systems.\n\n\nDevelopment economics \u2013 branch of economics which deals with economic aspects of the development process in low-income countries.\nEcological economics \u2013 an interdisciplinary and transdisciplinary field that aims to address the interdependence and coevolution of human economies and natural ecosystems.\nEconomic geography \u2013 study of the location, distribution and spatial organization of economic activities across the world.\nEconomic history \u2013 study of economies or economic phenomena in the past.\nEconomic sociology \u2013 studies both the social effects and the social causes of various economic phenomena.\nEnergy economics \u2013 broad scientific subject area which includes topics related to supply and use of energy in societies\nEntrepreneurial Economics \u2013 study of the entrepreneur and entrepreneurship within the economy.\nEnvironmental economics \u2013 subfield of economics concerned with environmental issues.\nEvolutionary economics \u2013 part of mainstream economics as well as heterodox school of economic thought that is inspired by evolutionary biology.\nFinancial economics \u2013 branch of economics concerned with the allocation and deployment of economic resources, both spatially and across time, in an uncertain environment.\nHeterodox economics \u2013 approaches or to schools of economic thought that are considered outside of mainstream economics and sometimes contrasted by expositors with neoclassical economics.\n\nGreen economics \u2013 one that results in improved human well-being and social equity, while significantly reducing environmental risks\nFeminist economics \u2013 diverse area of economic inquiry that highlights the androcentric biases of traditional economics through critical examinations of economic methodology, epistemology, history and empirical study.\nIslamic economics \u2013 body of Islamic studies literature that identifies and promotes an economic order that conforms to Islamic scripture and traditions, and in the economic world an interest-free Islamic banking system, grounded in Sharia's condemnation of interest (riba).\n\n\nIndustrial organization \u2013 field of economics that builds on the theory of the firm in examining the structure of, and boundaries between, firms and markets.\nInternational economics \u2013 study of the effects upon economic activity of international differences in productive resources and consumer preferences and the institutions that affect them.\nInstitutional economics \u2013 study of the role of the evolutionary process and the role of institutions in shaping economic behaviour.\nLabor economics \u2013 seeks to understand the functioning and dynamics of the markets for labour.\nLaw and Economics \u2013 application of economic methods to analysis of law.\nManagerial economics \u2013 application of economic concepts and economic analysis to the problems of formulating rational managerial decisions\nMonetary economics \u2013 branch of economics that historically prefigured and remains integrally linked to macroeconomics.\nNeoclassical economics \u2013 focuses on goods, outputs, and income distributions in markets through supply and demand.\nNeuroeconomics \u2013 interdisciplinary field that seeks to explain human decision making, the ability to process multiple alternatives and to choose an optimal course of action.\nPublic finance \u2013 study of the role of the government in the economy.\nPublic economics \u2013 study of government policy through the lens of economic efficiency and equity.\nReal estate economics \u2013 application of economic techniques to real estate markets.\nResource economics \u2013 study of supply, demand, and allocation of the Earth's natural resources.\nWelfare economics \u2013 branch of economics that uses microeconomic techniques to evaluate economic well-being, especially relative to competitive general equilibrium within an economy as to economic efficiency and the resulting income distribution associated with it.\nPolitical economy \u2013 study of the production, buying, and selling, and their relations with law, custom, and government, as well as with the distribution of national income and wealth, including through the budget process.\nSocioeconomics \u2013 considers behavioral interactions of individuals and groups through social capital and social markets (not excluding for example, sorting by marriage) and the formation of social norms.\nTransport economics \u2013 branch of economics that deals with the allocation of resources within the transport sector and has strong linkages with civil engineering.\nEconomic methodology \u2013 study of methods, especially the scientific method, in relation to economics, including principles underlying economic reasoning.\n\nComputational economics \u2013 research discipline at the interface between computer science and economic and management science.\nEconometrics \u2013 application of mathematics and statistical methods to economic data\n\nMathematical economics \u2013 application of mathematical methods to represent economic theories and analyze problems posed in economics.\nEconomic statistics \u2013 topic in applied statistics that concerns the collection, processing, compilation, dissemination, and analysis of economic data.\n\nTime series \u2013 sequence of data points, measured typically at successive time instants spaced at uniform time intervals.\n\n\n\n\nExperimental economics \u2013 application of experimental methods to study economic questions.\n\n\n\n\nEducation \u2013 in the general sense is any act or experience that has a formative effect on the mind, character, or physical ability of an individual. In its technical sense, education is the process by which society deliberately transmits its accumulated knowledge, skills, and values from one generation to another.\nEnvironmental studies \u2013 interdisciplinary academic field which systematically studies human interaction with the environment.\nGender and sexuality studies \u2013 field of interdisciplinary study and academic field devoted to gender identity and gendered representation as central categories of analysis.\nGeography \u2013 study of the lands, features, inhabitants, and phenomena of Earth.\n\nCartography \u2013 study and practice of making maps or globes.\nHuman geography \u2013 branch of the social sciences that studies the world, its people, communities, and cultures with an emphasis on relations of and across space and place.\n\nCritical geography \u2013 takes a critical theory (Frankfurt School) approach to the study and analysis of geography.\nCultural geography \u2013 study of cultural products and norms and their variations across and relations to spaces and places.\nFeminist geography \u2013 approach in human geography which applies the theories, methods and critiques of feminism to the study of the human environment, society and geographical space.\nEconomic geography \u2013 study of the location, distribution and spatial organization of economic activities across the world.\nDevelopment geography \u2013 branch of geography with reference to the standard of living and quality of life of its human inhabitants.\nHistorical geography \u2013 study of the human, physical, fictional, theoretical, and real geographies of the past.\nTime geography \u2013\nPolitical geography & geopolitics \u2013 field of human geography that is concerned with the study of both the spatially uneven outcomes of political processes and the ways in which political processes are themselves affected by spatial structures.\nMarxist geography \u2013 strand of critical geography that uses the theories and philosophy of Marxism to examine the spatial relations of human geography.\nMilitary geography \u2013 sub-field of geography that is used by, not only the military, but also academics and politicians to understand the geopolitical sphere through the militaristic lens.\nStrategic geography \u2013 concerned with the control of, or access to, spatial areas that affect the security and prosperity of nations.\nPopulation geography \u2013 study of the ways in which spatial variations in the distribution, composition, migration, and growth of populations are related to the nature of places.\nSocial geography \u2013 branch of human geography that is most closely related to social theory in general and sociology in particular, dealing with the relation of social phenomena and its spatial components.\nBehavioral geography \u2013 approach to human geography that examines human behavior using a disaggregate approach.\nChildren's geographies \u2013 area of study within human geography and Childhood Studies which involves researching the places and spaces of children's lives.\nHealth geography \u2013 application of geographical information, perspectives, and methods to the study of health, disease, and health care.\nTourism geography \u2013 study of travel and tourism, as an industry and as a social and cultural activity.\nUrban geography \u2013 study of areas which have a high concentration of buildings and infrastructure.\n\n\nEnvironmental geography \u2013 branch of geography that describes the spatial aspects of interactions between humans and the natural world.\nPhysical geography \u2013 branch of natural science which deals with the study of processes and patterns in the natural environment like the atmosphere, biosphere and geosphere, as opposed to the cultural or built environment, the domain of human geography.\n\nBiogeography \u2013 study of the distribution of species (biology), organisms, and ecosystems in geographic space and through geological time.\nClimatology \u2013 Atmospheric physics Atmospheric dynamics (category)\nPalaeoclimatology \u2013 study of changes in climate taken on the scale of the entire history of Earth.\nCoastal geography \u2013 study of the dynamic interface between the ocean and the land, incorporating both the physical geography (i.e. coastal geomorphology, geology and oceanography) and the human geography (sociology and history) of the coast.\nGeomorphology \u2013 scientific study of landforms and the processes that shape them.\nGeodesy \u2013 scientific discipline that deals with the measurement and representation of the Earth, including its gravitational field, in a three-dimensional time-varying space.\nHydrology \u2013 study of the movement, distribution, and quality of water on Earth and other planets, including the hydrologic cycle, water resources and environmental watershed sustainability.\nHydrography \u2013 mapping (charting) of water topographic features through the measurement of the depths, the tides and currents of a body of water and establishment of the sea, river or lake bed topography and morphology.\nGlaciology \u2013 study of glaciers, or more generally ice and natural phenomena that involve ice.\nLimnology \u2013 study of inland waters.\nOceanography \u2013 branch of Earth science that studies the ocean.\nPedology \u2013 study of soils in their natural environment.\nLandscape ecology \u2013 science of studying and improving relationships between ecological processes in the environment and particular ecosystems.\nPalaeogeography \u2013 study of what the geography was in times past.\n\n\nRegional geography \u2013 study of world regions.\n\n\nGerontology \u2013 study of the social, psychological and biological aspects of aging.\nHistory \u2013 discovery, collection, organization, and presentation of information about past events. History can also mean the period of time after writing was invented. This category includes many sub-domains of history such as, art history, diplomatic history, history of science, economic history, environmental history, military history, political history, urban history, women's history and many others.\nIndustrial relations \u2013 multidisciplinary field that studies the employment relationship.\nInformation science \u2013 interdisciplinary field primarily concerned with the analysis, collection, classification, manipulation, storage, retrieval and dissemination of information.\nInternational studies \u2013 study of the major political, economic, social, cultural and sacral issues that dominate the international agenda\nLaw \u2013 set of rules and principles (laws) by which a society is governed, through enforcement by governmental authorities.\nLegal management \u2013 social sciences discipline that is designed for students interested in the study of State and its elements, Law, Law Practice, Legal Research and Jurisprudence, legal Philosophy, Criminal Justice, Governance, Government structure, Political history and theories, Business Organization and Management, Entrepreneurship, Public Administration and Human Resource Development.\n\nParalegal studies \u2013 social sciences discipline that is designed for students interested in the study of State and its elements, Law, Law Practice, Legal Research and Jurisprudence, legal Philosophy, Criminal Justice, Governance, Government structure, Political history and theories, Business Organization and Management, Entrepreneurship, Public Administration and Human Resource Development.\n\n\nLibrary science \u2013 study of issues related to libraries and the information fields.\nLinguistics \u2013 scientific study of natural language.\n\nAnthropological linguistics \u2013 study of the relations between language and culture and the relations between human biology, cognition and language.\nApplied linguistics \u2013 interdisciplinary field of study that identifies, investigates, and offers solutions to language-related real-life problems.\nBiolinguistics \u2013 study of the biology and evolution of language.\nClinical linguistics and speech and language pathology \u2013 sub-discipline of linguistics which involves the application of linguistic theory to the field of Speech-Language Pathology.\nCognitive linguistics \u2013 branch of linguistics that interprets language in terms of the concepts, sometimes universal, sometimes specific to a particular tongue, which underlie its forms.\nComparative linguistics \u2013 branch of historical linguistics that is concerned with comparing languages to establish their historical relatedness.\nComputational linguistics \u2013 interdisciplinary field dealing with the statistical or rule-based modeling of natural language from a computational perspective.\nDevelopmental linguistics \u2013 study of the development of linguistic ability in an individual, particularly the acquisition of language in childhood.\n\n\n\nlanguage acquisition \u2013 the process by which humans acquire the capacity to perceive and comprehend language, as well as to produce and use words to communicate.\n\n\n\n\nDialectology \u2013 scientific study of linguistic dialect, a sub-field of sociolinguistics.\n\ndialectometry \u2013 the study of high levels of structure in geographical dialect networks.\n\n\nDiscourse analysis \u2013 general term for a number of approaches to analyzing use of written, oral or sign language or any significant semiotic event.\nEtymology \u2013 study of the history of words, their origins, and how their form and meaning have changed over time.\nEvolutionary linguistics \u2013 the scientific study of both the origins and development of language as well as the cultural evolution of languages.\nForensic linguistics \u2013 application of linguistic knowledge, methods and insights to the forensic context of law, language, crime investigation, trial, and judicial procedure.\nGeolinguistics \u2013 branch of human geography that studies the geographic distribution of language or its constituent elements.\nHistorical linguistics \u2013 study of language change.\nLexis \u2013 total vocabulary or lexicon having items of lexical, rather than grammatical, meaning.\nLinguistic typology \u2013 subfield of linguistics that studies and classifies languages according to their structural features.\nMorphology \u2013 identification, analysis and description of the structure of a given language's morphemes and other linguistic units, such as words, affixes, parts of speech, intonation/stress, or implied context (words in a lexicon are the subject matter of lexicology).\nNeurolinguistics \u2013 study of the neural mechanisms in the human brain that control the comprehension, production, and acquisition of language.\nPhilology \u2013 study of language in written historical sources; it is a combination of literary studies, history and linguistics.\nPhonetics \u2013 branch of linguistics that comprises the study of the sounds of human speech, or the equivalent aspects of sign.\nPhonology \u2013 branch of linguistics concerned with the systematic organization of sounds in languages.\nPhraseology \u2013 study of set or fixed expressions, such as idioms, phrasal verbs, and other types of multi-word lexical units (often collectively referred to as phrasemes), in which the component parts of the expression take on a meaning more specific than or otherwise not predictable from the sum of their meanings when used independently.\nPragmatics \u2013 subfield of linguistics which studies the ways in which context contributes to meaning.\nPsycholinguistics \u2013 study of the psychological and neurobiological factors that enable humans to acquire, use, comprehend and produce language.\nSociolinguistics \u2013 descriptive study of the effect of any and all aspects of society, including cultural norms, expectations, and context, on the way language is used, and the effects of language use on society.\nSpeech science \u2013 Speech science refers to the study of production, transmission and perception of speech. Speech science involves anatomy, in particular the anatomy of the oro-facial region and neuroanatomy, physiology, and acoustics.\nStylistics \u2013 study and interpretation of texts from a linguistic perspective.\nSyntax \u2013 the study of the principles and processes by which sentences are constructed in particular languages.\nSemantics \u2013 study of meaning.\nWriting systems and orthography \u2013 representation of language in a textual medium through the use of a set of signs or symbols (known as a writing system).\n\n\nManagement \u2013 in addition to the administration of an organization, it is the act of getting people together to accomplish desired goals and objectives using available resources efficiently and effectively.\nMedia studies \u2013 academic discipline and field of study that deals with the content, history and effects of various media; in particular, the 'mass media'.\n\nCommunication studies \u2013 academic field that deals with processes of human communication, commonly defined as the sharing of symbols to create meaning.\n\n\nPhilosophy \u2013 study of general and fundamental problems concerning matters such as existence, knowledge, values, reason, mind, and language. Academic philosophy is considered a science by some.[4] Others say that philosophy is not a science but it is instead a precursor of it.[5] The role of philosophy is also a philosophical question.[6]\n\nPhilosophy of language \u2013 is concerned with four central problems: the nature of meaning, language use, language cognition, and the relationship between language and reality.\nPhilosophy of information \u2013 (PI) is the area of research that studies conceptual issues arising at the intersection of computer science, information science, information technology, and philosophy.\nPolitical philosophy \u2013 is the study of topics such as politics, liberty, justice, property, rights, law, and the enforcement of a legal code by authority.\nEpistemology \u2013 study of how we know what we know; study of the nature and scope of knowledge.\nEthics \u2013 major branch of philosophy, encompassing right conduct and good life. It is significantly broader than the common conception of analyzing right and wrong.\nLogic \u2013 formal science of using reason\nPhilosophy of mind \u2013 branch of philosophy that studies the nature of the mind, mental events, mental functions, mental properties, consciousness and their relationship to the physical body, particularly the brain.\nPhilosophy of science \u2013 questions the assumptions, foundations, methods and implications of science; questions the use and merit of science; sometimes overlaps metaphysics and epistemology by questioning whether scientific results are actually a study of truth.\nSocial philosophy \u2013 is the study of questions about social behavior and interpretations of society and social institutions in terms of ethical values rather than empirical relations.\nAesthetics \u2013 is a branch of philosophy dealing with the nature of art, beauty, and taste, with the creation and appreciation of beauty.\nPhilosophy of mathematics \u2013 is the branch of philosophy that studies the philosophical assumptions, foundations, and implications of mathematics. The aim of the philosophy of mathematics is to provide an account of the nature and methodology of mathematics and to understand the place of mathematics in people's lives.\nPhilosophy of education \u2013 Philosophy of education can refer to either the academic field of applied philosophy or to one of any educational philosophies that promote a specific type or vision of education, and/or which examine the definition, goals and meaning of education.\n\n\nPlanning\n\nEnvironmental planning \u2013 is the process of facilitating decision making to carry out land development with the consideration given to the natural environment, social, political, economic and governance factors and provides a holistic framework to achieve sustainable outcomes. A major goal of environmental planning is to create sustainable communities, which aim to conserve and protect undeveloped land.\nUrban planning \u2013 studies the development and use of land, protection and use of the environment, public welfare, and the design of the urban environment, including air, water, and the infrastructure passing into and out of urban areas, such as transportation, communications, and distribution networks.\nRegional planning \u2013 deals with the efficient placement of land-use activities, infrastructure, and settlement growth across a larger area of land than an individual city or town.\n\n\nPolitical science \u2013 social science discipline concerned with the study of the state, government, and politics.\n\nComparative politics \u2013 field and a method used in political science, characterized by an empirical approach based on the comparative method.\nGame theory \u2013 study of strategic decision making.\nGeopolitics \u2013 theory that describes the relation between politics and territory whether on local or international scale.\n\npolitical geography \u2013 field of human geography that is concerned with the study of both the spatially uneven outcomes of political processes and the ways in which political processes are themselves affected by spatial structures.\n\n\nIdeology \u2013 set of ideas that constitute one's goals, expectations, and actions.\nPolitical economy \u2013 Political economy originally was the term for studying production, buying, and selling, and their relations with law, custom, and government, as well as with the distribution of national income and wealth, including through the budget process. Political economy originated in moral philosophy. It developed in the 18th century as the study of the economies of states, polities, hence political economy.\nPolitical psychology, bureaucratic, administrative and judicial behaviour \u2013\nPsephology \u2013 branch of political science which deals with the study and scientific analysis of elections.\nVoting systems \u2013 methods by which voters make a choice between options, often in an election or on a policy referendum.\nPublic administration \u2013 houses the implementation of government policy and an academic discipline that studies this implementation and that prepares civil servants for this work.\n\nPublic policy \u2013 generally the principled guide to action taken by the administrative or executive branches of the state with regard to a class of issues in a manner consistent with law and institutional customs.\nPublic health \u2013 the science and art of preventing disease, prolonging life and promoting human health through organized efforts and informed choices of society, organizations, public and private, communities and individuals\nLocal government studies \u2013 form of public administration which in a majority of contexts, exists as the lowest tier of administration within the a given state.\nInternational politics \u2013 study of relationships between countries, including the roles of states, inter-governmental organizations (IGOs), international nongovernmental organizations (INGOs), non-governmental organizations (NGOs) and multinational corporations (MNCs).\n\n\nInternational relations theory \u2013 study of international relations from a theoretical perspective; it attempts to provide a conceptual framework upon which international relations can be analyzed.\n\n\nPsychology \u2013 science of behavior and mental processes\n\nApplied psychology \u2013 use of psychological principles and theories to overcome problems in other areas, such as mental health, business management, education, health, product design, ergonomics, and law.\n\nPsychological testing \u2013 field characterized by the use of samples of behavior in order to assess psychological construct(s), such as cognitive and emotional functioning, about a given individual.\nClinical psychology \u2013 integration of science, theory and clinical knowledge for the purpose of understanding, preventing, and relieving psychologically based distress or dysfunction and to promote subjective well-being and personal development.\nCommunity psychology \u2013 Sense of community Social capital\nConsumer behaviour \u2013 study of when, why, how, and where people do or do not buy a product.\nCounseling psychology \u2013 psychological specialty that encompasses research and applied work in several broad domains: counseling process and outcome; supervision and training; career development and counseling; and prevention and health.\nEducational psychology \u2013 study of how humans learn in educational settings, the effectiveness of educational interventions, the psychology of teaching, and the social psychology of schools as organizations.\nForensic psychology \u2013 intersection between psychology and the courtroom\u2014criminal, civil, family and Federal.\nHealth psychology \u2013 concerned with understanding how biological, psychological, environmental, and cultural factors are involved in physical health and illness.\nIndustrial and organizational psychology \u2013 scientific study of employees, workplaces, and organizations.\nLegal psychology \u2013 involves empirical, psychological research of the law, legal institutions, and people who come into contact with the law.\nMedia psychology \u2013 seeks an understanding of how people perceive, interpret, use, and respond to a media-rich world.\nMilitary psychology \u2013 research, design and application of psychological theories and experimentation data towards understanding, predicting and countering behaviours either in friendly or enemy forces or civilian population that may be undesirable, threatening or potentially dangerous to the conduct of military operations.\nOccupational health psychology \u2013 concerned with the psychosocial characteristics of workplaces that contribute to the development of health-related problems in people who work.\nPastoral psychology \u2013 application of psychological methods and interpretive frameworks to religious traditions, as well as to both religious and irreligious individuals.\nPolitical psychology \u2013 interdisciplinary academic field dedicated to understanding political science, politicians and political behavior through the use of psychological theories.\nPsychometrics \u2013 field of study concerned with the theory and technique of psychological measurement, which includes the measurement of knowledge, abilities, attitudes, personality traits, and educational measurement.\nSchool psychology \u2013 field that applies principles of clinical psychology and educational psychology to the diagnosis and treatment of children's and adolescents' behavioral and learning problems.\nSport psychology \u2013 interdisciplinary science that draws on knowledge from the fields of Kinesiology and Psychology.\nSystems psychology \u2013 branch of applied psychology that studies human behaviour and experience in complex systems.\nTraffic psychology \u2013 study of the behavior of road users and the psychological processes underlying that behavior (Rothengatter, 1997, 223) as well as to the relationship between behavior and accidents\n\n\nBehavior analysis \u2013 philosophy of psychology based on the proposition that all things that organisms do can and should be regarded as behaviors, and that psychological disorders are best treated by altering behavior patterns or modifying the environment.\nBiopsychology \u2013 application of the principles of biology (in particular neurobiology), to the study of physiological, genetic, and developmental mechanisms of behavior in human and non-human animals.\nCognitive psychology \u2013 subdiscipline of psychology exploring internal mental processes.\nClinical psychology \u2013 integration of science, theory and clinical knowledge for the purpose of understanding, preventing, and relieving psychologically based distress or dysfunction and to promote subjective well-being and personal development.\nCultural psychology \u2013 field of psychology which assumes the idea that culture and mind are inseparable, and that psychological theories grounded in one culture are likely to be limited in applicability when applied to a different culture.\nDevelopmental psychology \u2013 scientific study of systematic psychological changes, emotional changes, and perception changes that occur in human beings over the course of their life span.\nEducational psychology \u2013 study of how humans learn in educational settings, the effectiveness of educational interventions, the psychology of teaching, and the social psychology of schools as organizations.\nEvolutionary psychology \u2013 approach in the social and natural sciences that examines psychological traits such as memory, perception, and language from a modern evolutionary perspective.\nExperimental psychology \u2013 application of experimental methods to the study of behavior and the processes that underlie it.\nForensic psychology \u2013 intersection between psychology and the courtroom\u2014criminal, civil, family and Federal.\nHealth psychology \u2013 concerned with understanding how biological, psychological, environmental, and cultural factors are involved in physical health and illness.\nHumanistic psychology \u2013 psychological perspective which rose to prominence in the mid-20th century in the context of the tertiary sector beginning to produce in the most developed countries in the world more than the secondary sector was producing, for the first time in human history demanding creativity and new understanding of human capital.\nIndustrial and organizational psychology \u2013 scientific study of employees, workplaces, and organizations.\nMusic therapy \u2013 allied health profession and one of the expressive therapies, consisting of an interpersonal process in which a trained music therapist uses music to help clients to improve or maintain their health.\nNeuropsychology \u2013 studies the structure and function of the brain as they relate to specific psychological processes and behaviors.\nPersonality psychology \u2013 branch of psychology that studies personality and individual differences.\nPsychometrics \u2013 field of study concerned with the theory and technique of psychological measurement, which includes the measurement of knowledge, abilities, attitudes, personality traits, and educational measurement.\nPsychology of religion \u2013 application of psychological methods and interpretive frameworks to religious traditions, as well as to both religious and irreligious individuals.\nPsychophysics \u2013 quantitatively investigates the relationship between physical stimuli and the sensations and perceptions they affect.\nSensation and perception psychology \u2013\n\n\nPublic administration \u2013 houses the implementation of government policy and an academic discipline that studies this implementation and that prepares civil servants for this work.\nSocial work \u2013 professional and academic discipline that seeks to improve the quality of life and wellbeing of an individual, group, or community by intervening through research, policy, community organizing, direct practice, and teaching on behalf of those afflicted with poverty or any real or perceived social injustices and violations of their human rights.\nSociology \u2013 studies society using various methods of empirical investigation and critical analysis to understand human social activity, from the micro level of individual agency and interaction to the macro level of systems and social structure.\n\nCriminology \u2013 study of the nature, extent, causes, and control of criminal behavior in both the individual and in society.\nDemography \u2013 statistical study of human populations and sub-populations.\nUrban and rural sociology - the analysis of social life in metropolitan and non-metropolitan areas.\n\n\nSustainable development \u2013 the process of meeting human development goals while sustaining the ability of natural systems to continue to provide the natural resources and natural system services upon which the economy of human society depends.\n\nSustainable agriculture \u2013 farming in sustainable ways based on an understanding of ecosystem services, the study of relationships between organisms and their environment.\n\n\nSustainability studies \u2013 focuses on the interdisciplinary perspective of the sustainability concept. Programs include instruction in sustainable development, geography, environmental policies, ethics, ecology, landscape architecture, city and regional planning, economics, natural resources, sociology, and anthropology, many of which are considered social sciences in their own right.\nHistory of social science[edit]\nHistory of the social sciences\n\nHistory of anthropology\nHistory of archaeology\nHistory of area studies\nHistory of communication studies\nHistory of cultural studies\nHistory of development studies\nHistory of economics\nHistory of education\nHistory of environmental studies\nHistory of gender studies\nHistory of geography\n\nHistory of human geography\n\n\nHistory of history\nHistory of information science\nHistory of journalism\nHistory of law\nHistory of library science\nHistory of linguistics\nHistory of management\nHistory of political science\n\nHistory of international studies\n\nHistory of international relations\n\n\nHistory of political economy\nHistory of public administration\n\n\nHistory of psychology\n\nHistory of social psychology\n\n\nHistory of social work\nHistory of sociology\n\nHistory of criminal justice\nHistory of demography\n\n\nHistory of sustainability\n\n\nEducation and degrees[edit]\nBachelor of Social Science\nBachelor of Science\nBachelor of Arts\nBachelor of Economics\nGeneral social science concepts[edit]\nEthical research in social science\nOpen and closed systems in social science\nPhilosophy of social science\nSocial science organizations[edit]Social science publications[edit]\nList of social science journals\nSocial scientists[edit]\nList of anthropologists\nList of business theorists\nList of developmental psychologists\nList of economists\nList of socialist economists\nList of educational psychologists\nList of geographers\nList of psychologists\nList of political scientists\nList of social psychologists\nList of sociologists\nList of urban planners\nList of urban theorists\nSee also[edit]\nOutline of science\n\nOutline of natural science\n\nOutline of physical science\n\nOutline of earth science\n\n\n\n\nOutline of formal science\nOutline of social science\nOutline of applied science\n\n\nReferences[edit]External links[edit]\nSocial Science Virtual Library\nUC Berkeley Experimental Social Science Laboratory\nIntute: Social Sciences (UK)\nHistory of Social Science\nOn the Social Sciences Critical Essays\npraxeology as the method of the social sciences\nin defense of extreme apriorism\n", "subtitles": ["What ", " of thing is social science?", "Branches of social science", "History of social science", "Education and degrees", "General social science concepts", "Social science organizations", "Social science publications", "Social scientists", "See also", "References", "External links"], "title": "Outline of social science"},
{"content": "This is an index of sociology articles. For a shorter list, see List of basic sociology topics.A[edit]absolute poverty \u2014 achieved status \u2014 acute disease \u2014 adaptation \u2014 Adultism \u2014 affect control theory \u2014 affirmative action \u2014 affluent alienation \u2014 age grade \u2014 age structure \u2014 aging in place \u2014 ageism \u2014 agency \u2014 AGIL Paradigm \u2014 aggregate \u2014 ageism \u2014 agrarian society \u2014 agribusiness \u2014 AIDS \u2014 air pollution \u2014 alcoholism \u2014 alienation \u2014 alien land law \u2014 alternative society \u2014 altruism \u2014 alzheimer's disease \u2014 Amae \u2014 amalgamation \u2014 Americanization \u2014 Anabaptist \u2014 anarchy \u2014 androgyny \u2014 animal abuse \u2014 animism \u2014 anomia \u2014 anomie \u2014 anthropology \u2014 antisemitism \u2014 apartheid \u2014 apollonian \u2014 applied science \u2014 approach \u2014 appropriate technology \u2014 The Archaeology of Knowledge \u2014 arms race \u2014 arms trade \u2014 arranged marriage \u2014 asceticism \u2014 Asch conformity experiments \u2014 ascribed status \u2014 assimilation \u2014 assisted living \u2014 attribution theory \u2014 autarky \u2014 authentic act \u2014 authoritarian personality \u2014 authoritarianism \u2014 authority \u2014 autocracy \u2014 automation \u2014 avant-garde \u2014 abortionB[edit]baby boomer \u2014 \u2014 balance of power \u2014 base and superstructure \u2014 The Bell Curve \u2014 belonging \u2014 berdache \u2014 biological determinism \u2014 bioethics \u2014 biosocial theory \u2014 Black Power \u2014 Black Panther Party \u2014 blended family \u2014 boomerang generation \u2014 bourgeoisie \u2014 brainwashing \u2014 bricolage \u2014 bureaucracy \u2014 bureaucratic collectivism \u2014 bureaucratization \u2014 bystander effectC[edit]capitalism \u2014 capitalists \u2014 carrying capacity \u2014 cash crop \u2014 caste \u2014 caste system \u2014 Catholic Worker \u2014 Catholicism \u2014 causation \u2014 cause marketing \u2014 charismatic movement \u2014 CBD \u2014 Chicago Area Project \u2014 Chicago school \u2014 Chicano \u2014 child labor \u2014 chronic disease \u2014 church \u2014 citizen \u2014 citizenship \u2014 civil disorder \u2014 civil inattention \u2014 civil religion \u2014 civil rights \u2014 civil society \u2014 clan \u2014 class \u2014 class conflict \u2014 class consciousness \u2014 class structure \u2014 classism \u2014 cognition \u2014 cohabitation \u2014 cold war \u2014 collective action \u2014 collective behavior \u2014 collective consciousness \u2014collective punishment \u2014 collective representation \u2014 collective violence \u2014 colonialism \u2014 commodity \u2014 commodity chain \u2014 commodity fetishism \u2014 communal riot \u2014 communication \u2014 communism \u2014 community \u2014 community care \u2014 comparable worth \u2014 comparative sociology \u2014 complex society \u2014 computational sociology \u2014 conflict theory \u2014 conformity \u2014 conglomerates \u2014 conscience collective \u2014 consciousness \u2014 consensus \u2014 consensus decision-making \u2014 consumerism \u2014 content analysis \u2014 contingent work \u2014 contradiction \u2014 conversation analysis \u2014 core countries \u2014 corporation \u2014 correlation \u2014 corruption \u2013 Counterculture \u2014 counter-revolutionary \u2014 coup d'e\u0301tat \u2014 created environment \u2014 creole language \u2014 crime \u2014 critical theory \u2014 crowd psychology \u2014 crude birth rate \u2014 crude death rate \u2014 cult \u2014 cultural capital \u2014 cultural deprivation \u2014 cultural imperialism \u2014 cultural lag \u2014 cultural materialism \u2014 cultural pluralism \u2014 cultural relativism \u2014 cultural reproduction \u2014 cultural system \u2014 cultural transmission \u2014 cultural universal \u2014 culture \u2014 culture of poverty \u2014 curative medicineD[edit]Darwinism \u2014 death \u2014 debt bondage \u2014 deconstruction \u2014 defensive medicine \u2014 deforestation \u2014 deinstitutionalisation \u2014 democracy \u2014 demographic transition \u2014 demography \u2014 dependency theory \u2014 dependent variable \u2014 depletion \u2014 desertification \u2014 deskilling \u2014 destratification \u2014 deterrence theory \u2014 devaluation \u2014 developmental state \u2014 deviance \u2014 deviance amplification \u2014 deviant subculture \u2014 dialectic \u2014 diaspora \u2014 differential association \u2014 differentiation \u2014 diffusion \u2014 dionysian \u2014 discourse \u2014 discrimination \u2014 division of labour \u2014 division of labour \u2014 domestic worker \u2014 domestic violence \u2014 double standard \u2014 doubling time \u2014 dramaturgical perspective \u2014 Disneyfication \u2014 dyad \u2014 dysfunction \u2014 dystopiaE[edit]ecologism \u2014 ecology \u2014 economic determinism \u2014 ecological modernization \u2014 economic interdependence \u2014 economies of scale \u2014 economy \u2014 ecosystem \u2014 education \u2014 education system \u2014 egalitarianism \u2014 elder abuse \u2014 elite \u2014 embourgeoisement thesis \u2014 emigration \u2014 empirical studies \u2014 encounter \u2014 endogamy \u2014 entrepreneur \u2014 entropy \u2014 environmentalism \u2014 environmental sociology \u2014 epistemology \u2014 estate \u2014 ethnic group \u2014 ethnic minority \u2014 ethnicity \u2014 ethnocentrism \u2014 ethnography \u2014 ethnomethodology \u2014 eutrophication \u2014 evolution \u2014 evolutionary sociology \u2014 evolutionism \u2014 exclusivist \u2014 existentialism \u2014 exogamy \u2014 experiment \u2014 exponential growth \u2014 export-processing zone \u2014 extended family \u2014F[edit]false consciousness \u2014 family \u2014 fascism \u2014 fecundity \u2014 feedback \u2014 femininity \u2014 feminism \u2014 Ferdinand To\u0308nnies Society \u2014 fertility \u2014 fetishism \u2014 feudalism \u2014 First World \u2014 flextime plan \u2014 forces of production \u2014 Fordism \u2014 forms of activity and interpersonal relations \u2014 functionalism \u2014 functions \u2014 fundamentalism \u2014 futures studies \u2013 futurist \u2014 futurist \u2014 futurology \u2014G[edit]gang \u2014 GDP \u2014 gemeinschaft \u2014 Gemeinschaft and Gesellschaft \u2013 gender \u2014 gendered division of labour \u2014 gendering \u2014 genealogy of power/knowledge \u2014 generalized other \u2014 generalized other \u2014 genetic engineering \u2014 genocide \u2014 gentrification \u2014 geopolitics \u2014 German Society for Sociology \u2014 gesellschaft \u2014 gestalt psychology \u2014 ghetto \u2014 globalization \u2014 glocalisation \u2014 GNP \u2014 government \u2014 Great Depression \u2014 group action \u2014 group behaviour \u2014 group dynamics \u2014 The Great Transformation \u2014 Green Revolution \u2014 greenhouse effect \u2014 gross domestic product \u2014 gross national product \u2014 guerrilla movement \u2014H[edit]habitus \u2014 health maintenance organization \u2014 hegemony \u2014 heterophobia \u2014 heterosexuality \u2014 hidden curriculum \u2014 higher circles \u2014 higher education \u2014 Hispanic \u2013 historical materialism \u2014 historical sociology \u2014 holocaust \u2014 homelessness \u2014 homophobia \u2014 homosexuality \u2014 house work \u2014 hunter-gatherer \u2014 human ecology \u2014 hybridity \u2014 hyperreality \u2014 hypothesis \u2013 honour killingI[edit]'I' and the 'me' \u2014 iatrogenesis \u2014 ideal type \u2014 identity \u2014 identity politics \u2014 ideology \u2014 imagined communities \u2014 immigration \u2014 imperialism \u2014 ingroup \u2014 income \u2014 independent variable \u2014 industrial democracy \u2014 industrial production \u2014 industrial society \u2014 industrial sociology \u2014 industrialisation \u2014 industrialization of war \u2014 infant mortality\u2014 informal economy \u2014 information technology \u2014infrastructure \u2014 inner city \u2014 instinct \u2014 institution \u2014 institutional discrimination \u2014 institutional racism \u2014 insurgency \u2014 intelligence \u2014 intelligence quotient \u2014 intelligentsia \u2014 intentional community \u2014 interaction \u2014 interest group \u2014 intergenerational mobility \u2014 internal colonialism \u2014 international division of labor \u2014 interpersonal violence \u2014 interpretiveJ[edit]Japanization \u2014 Jim Crow laws \u2014 jingoism \u2014 Judaism \u2014 justice, distributive \u2014 juvenile delinquency \u2014K[edit]kin selection \u2014 kinship \u2014 kindnessL[edit]labeling theory \u2014 labour power \u2014 laissez-faire \u2014 late modernity \u2014 Latino/a \u2013 latent function \u2014 law \u2014 legitimacy \u2014 legitimation crisis \u2014 Leipzig school \u2014 lesbianism \u2014 liberal democracy \u2014 life-course \u2014 life-cycle \u2014 life expectancy \u2014 lifeworld \u2014 limited war \u2014 literacy \u2014 local knowledge \u2014 longevity \u2014 longitudinal study \u2014 looking-glass self \u2014 love \u2014 luddism \u2014 luddite \u2014 lumpenproletariat \u2014M[edit]macrosociology \u2014 malthusianism \u2014 managed care \u2014managerial class \u2014 manifest function \u2014 marginalization \u2014 marriage \u2014 Marxism \u2014 masculinity \u2014 mass action \u2014 mass media \u2014 mass society \u2014 master status \u2014 materialism \u2014 matriarchy \u2014 matrilineality \u2014 matrilocal residence \u2014 McDonaldization \u2014 mean \u2014 means of production \u2014 mechanical solidarity \u2014 mechanization \u2014 median \u2014 medicaid \u2014 medical gaze \u2014 medical model \u2014 medicalization \u2014 medicare \u2014 megalopolis \u2014 mental disorder \u2014mercantilism \u2014 medical sociology \u2014 meritocracy \u2014 metanarrative \u2014 methodology \u2014 microsociology \u2014 middle class \u2014 militarism \u2014 military-industrial complex \u2014 millenarianism \u2014 minority group \u2014 mixed economy \u2014 mode \u2014 mode of production \u2014 mode of reproduction \u2014 modernity \u2014 modernization \u2014 monogamy \u2014 monopoly \u2014 monotheism \u2014 moral panic \u2014 mores \u2014 mortality rate \u2014 multiculturalism \u2014 multilineal evolution \u2014 multinational corporation \u2014 murderN[edit]nation state \u2014 nationalism \u2014 nature \u2014 neocolonialism \u2014 neoliberalism \u2014 neo-locality \u2014 new international division of labour \u2014 non-state actor \u2014 non-tariff barriers to trade \u2014 norm \u2014 normal type \u2014 normlessness \u2014 nuclear familyO[edit]objectivity \u2014 oligarchy \u2014 oligopoly \u2014 ontological security \u2014 ontology \u2014 organic solidarity \u2014 organization \u2014 organizational behavior \u2014 organizational studies \u2014 organized crimeP[edit]paradigm \u2014 participant observation \u2014 participatory democracy \u2014 pastoral society \u2014 patient dumping \u2014 patriarchal \u2014 patriarchy \u2014 patrilineality \u2014 peasant \u2014 peer group \u2014 periphery countries \u2014 phenomenology \u2014 Physician Assistant \u2014 plea bargaining \u2014 pluralism \u2014 pluralist theory \u2014 police brutality \u2014 political economy \u2014 political party \u2014 politics \u2014 pollution \u2014 polyandry \u2014 polyarchy \u2014 polygamy \u2014 polygyny \u2014 polylogism \u2014 polytheism \u2014 popular culture \u2014 positivism \u2014 post-Fordism \u2014 post-structuralism \u2014 post-industrial society \u2014 postmodernism \u2014 postmodernity \u2014 poverty \u2014 power \u2014 power elite \u2014 powerlessness \u2014 pragmatism \u2014 on pragmatic sociology, for now, see: George Herbert Mead \u2014 prejudice \u2014 primary deviance \u2014 Primary and secondary groups \u2014 primary labor market \u2014 primary sector \u2014 private health care \u2014 privatism \u2014 profanity \u2014 professionalism \u2014 profession \u2014 proletariat \u2014 prostitution \u2014 proto-globalization \u2014 psychopathy \u2014 psychosis \u2014 public order crime \u2014 public health \u2014 public sphere \u2014 purchasing power parity \u2014Q[edit]qualitative research \u2014 quantitative researchR[edit]race \u2014 racism \u2014 radical \u2014 rape \u2014 rationalisation process \u2014 rationality \u2014 rationalization \u2014 realism \u2014 rebellion \u2014 recidivism \u2014 reciprocity \u2014 reductionism \u2014 reflexive \u2014 reflexivity \u2014 reform movement \u2014 reify \u2014 relations of production \u2014 relative deprivation \u2014 relative poverty \u2014 relativism \u2014 religion \u2014 representative democracy \u2014 research methods \u2014 reserve army of labour \u2014 resocialization \u2014 retirement home \u2014 revolution \u2014 riot \u2014 risk \u2014 rite of passage \u2014 ritual \u2014 role \u2014 role conflict \u2014 rural sociology \u2014 ruling class \u2014 ruling eliteS[edit]sacred \u2014 sampling \u2014 sampling frame \u2014 sanction \u2014 Sapir\u2013Whorf hypothesis \u2014 scapegoating \u2014 schizophrenia \u2014 science \u2014 Second World \u2014 secondary data \u2014 secondary deviance \u2014 secondary group \u2014 secondary labor market \u2014 sect \u2014 secularization \u2014 self \u2014 self-consciousness \u2014 semi-periphery countries \u2014 semiotics \u2014 serial monogamy \u2014 serial reciprocity \u2014 sex \u2014 sex role \u2014 sexism \u2014 sexual harassment \u2014 sexual script \u2014 sick role \u2014 significant other \u2014 simulation \u2014 snowball sampling \u2014 for entries beginning with social, see sections below \u2014 socialism \u2014 socialization \u2014 society \u2014 sociobiology \u2014 sociocultural context \u2014 sociocultural evolution \u2014 for entries beginning with sociological, see sections below \u2014 sociology \u2014 for entries beginning with sociology of, see sections below \u2014 solid waste \u2014 solidarity \u2014 sovereignty \u2014 split labor market theory \u2014 standing army \u2014 state (polity) \u2014 stateless nation \u2014 status \u2014 status group \u2014 status inconsistency \u2014 status offense \u2014 stem cell \u2014 stepfamily \u2014 stereotype \u2014 stigma \u2014 stigmatise \u2014 Strategic Defense Initiative \u2014 stratification \u2014 strike \u2014 structural unemployment \u2014 structuration \u2014 structure \u2014 subculture \u2014 suburbanization \u2014 surplus value \u2014 surveillance \u2014 survey \u2014 symbol \u2014 Symbolic Convergence Theory \u2014 symbolic interactionism \u2014 symbolic system \u2014 systems theorySociological[edit]sociology books \u2014 sociological framework \u2014 sociological imagination \u2014 sociological naturalism \u2014 sociological paradigm \u2014 sociological perspective \u2014 sociological positivism \u2014 sociological theorySociology of[edit]\nSee Subfields of sociology for the full list of subfields of sociology\nsociology of aging \u2014 sociology of architecture \u2014 sociology of art \u2014 sociology of the body \u2014 sociology of childhood \u2014 sociology of conflict \u2014 sociology of deviance \u2014 sociology of disaster \u2014 sociology of education \u2014 sociology of emotions \u2014 sociology of the family \u2014 sociology of fatherhood \u2014 sociology of film \u2014 sociology of food \u2014 sociology of gender \u2014 sociology of government \u2014 sociology of health and illness \u2014 sociology of the history of science \u2014 sociology of immigration \u2014 sociology of knowledge \u2014 sociology of language \u2014 sociology of law \u2014 sociology of leisure \u2014 sociology of markets \u2014 sociology of medicine \u2014 sociology of the military \u2014 sociology of music \u2014 sociology of politics \u2014 sociology of punishment \u2014 sociology of race \u2014 sociology of religion \u2014 sociology of science and technology \u2014 sociology of sport \u2014 sociology of terrorism \u2014 sociology of work-sociology of motherhoodT[edit]taboo \u2014 Scientific management \u2014 Taylorization \u2014 technology \u2014 terrorism \u2014 tertiary sector of economic activity \u2014 the Enlightenment \u2014 the Renaissance \u2014 theoretical approach \u2014 theory \u2014 Third World \u2014 total institution \u2014 total war \u2014 totalitarianism \u2014 totemism \u2014 totem \u2014 trading network \u2014 traditional state \u2014 transnational company \u2014 transsexualism \u2014 trust \u2014 temperamentU[edit]unconscious \u2014 underclass \u2014 underdevelopment \u2014 unemployment \u2014 unilineal evolution \u2014 unintended consequences \u2014 unions \u2014 universal health care \u2014 upper class \u2014 urban ecology \u2014 urban renewal \u2014 urbanism \u2014 urbanization \u2014 urban sociologyV[edit]value \u2014 value-added theory \u2014 verstehen \u2014 vertical mobility \u2014 Vested interest (communication theory) \u2014 victimless crime \u2014 violence \u2014 visual sociologyW[edit]wealth \u2013 wealthfare \u2013 welfare \u2013 welfare state \u2013 whisper campaign \u2013 white-collar crime \u2013 white flight \u2013 white guilt \u2013 white privilege \u2013 women's liberation movement \u2013 work \u2013 working class \u2013 world-systems theory \u2013X[edit]xenophobia \u2014 xenocentrismY[edit]youth, youth subculture, youth welfareZ[edit]zero population growthPlease help the Wikipedia sociology project by adding relevant articles to this list. Articles marked red are yet to be created.", "subtitles": [], "title": "Index of sociology articles"},
{"content": "In computing, type introspection is the ability of a program to examine the type or properties of an object at runtime. Some programming languages possess this capability.Introspection should not be confused with reflection, which goes a step further and is the ability for a program to manipulate the values, meta-data, properties and/or functions of an object at runtime. Some programming languages - e.g. Java, Python and Go - also possess that capability.Examples[edit]Ruby[edit]Type introspection is a core feature of Ruby. In Ruby, the Object class (ancestor of every class) provides Object#instance_of? and Object#kind_of? methods for checking the instance's class. The latter returns true when the particular instance the message was sent to is an instance of a descendant of the class in question. For example, consider the following example code (you can immediately try this with the Interactive Ruby Shell):In the example above, the Class class is used as any other class in Ruby. Two classes are created, A and B, the former is being a superclass of the latter, then one instance of each class is checked. The last expression gives true because A is a superclass of the class of b.Further, you can directly ask for the class of any object, and compare them (code below assumes having executed the code above):Objective-C[edit]In Objective-C, for example, both the generic Object and NSObject (in Cocoa/OpenStep) provide the method isMemberOfClass: which returns true if the argument to the method is an instance of the specified class. The method isKindOfClass: analogously returns true if the argument inherits from the specified class.For example, say we have an Apple and Orange class inheriting from Fruit.Now, in the eat method we can writeNow, when eat is called with a generic object (an id), the function will behave correctly depending on the type of the generic object.C++[edit]C++ supports type introspection via the run-time type information (RTTI) typeid and dynamic_cast keywords. The dynamic_cast expression can be used to determine whether a particular object is of a particular derived class. For instance:The typeid operator retrieves a std::type_info object describing the most derived type of an object:Object Pascal[edit]Type introspection has been a part of Object Pascal since the original release of Delphi, which uses RTTI heavily for visual form design. In Object Pascal, all classes descend from the base TObject class, which implements basic RTTI functionality. Every class's name can be referenced in code for RTTI purposes; the class name identifier is implemented as a pointer to the class's metadata, which can be declared and used as a variable of type TClass. The language includes an is operator, to determine if an object is or descends from a given class, an as operator, providing a type-checked typecast, and several TObject methods. More deeper introspection (enumerating fields and methods) are traditionally only supported for objects declared in the $M+ (a pragma) state, typically TPersistent, and only for symbols defined in the published section. Delphi 2010 increased this to nearly all symbols.Java[edit]The simplest example of type introspection in Java is the instanceof[1] operator. The instanceof operator determines whether a particular object belongs to a particular class (or a subclass of that class, or a class that implements that interface). For instance:The java.lang.Class[2] class is the basis of more advanced introspection.For instance, if it is desirable to determine the actual class of an object (rather than whether it is a member of a particular class), Object.getClass() and Class.getName() can be used:PHP[edit]In PHP introspection can be done using instanceof operator. For instance:Perl[edit]Introspection can be achieved using the ref and isa functions in Perl.We can introspect the following classes and their corresponding instances:using:Meta-Object Protocol[edit]Much more powerful introspection in Perl can be achieved using the Moose object system[3] and the Class::MOP meta-object protocol,[4] for example this is how you can check if a given object does a role X:This is how you can list fully qualified names of all of the methods that can be invoked on the object, together with the classes in which they were defined:Python[edit]The most common method of introspection in Python is using the dir function to detail the attributes of an object. For example:Also, the built-in functions type and isinstance can be used to determine what an object is while hasattr can determine what an object does. For example:In Python 2 but not Python 3, declaring class Foo instead of class Foo(object) will result in type returning the generic instance type instead of the class.[5]ActionScript (as3)[edit]In ActionScript the function flash.utils.getQualifiedClassName can be used to retrieve the Class/Type name of an arbitrary Object.Or alternatively in actionscipt the operator is can be used to determine if an object is of a specific typeThis second function can be used to test class inheritance parents as wellMeta-Type introspection[edit]Like perl, actionscript can go further than getting the Class Name, but all the metadata, functions and other elements that make up an object using the flash.utils.describeType function, this is used when implementing reflection in actionscript.See also[edit]\nReflection (computer science)\nReification (computer science)\nReferences[edit]External links[edit]\nIntrospection on Rosetta Code\n", "subtitles": ["Examples", "See also", "References", "External links"], "title": "Type introspection"},
{"content": "Reification is the process by which an abstract idea about a computer program is turned into an explicit data model or other object created in a programming language. A computable/addressable object \u2014 a resource \u2014 is created in a system as a proxy for a non computable/addressable object. By means of reification, something that was previously implicit, unexpressed, and possibly inexpressible is explicitly formulated and made available to conceptual (logical or computational) manipulation. Informally, reification is often referred to as making something a first-class citizen within the scope of a particular system. Some aspect of a system can be reified at language design time, which is related to reflection in programming languages. It can be applied as a stepwise refinement at system design time. Reification is one of the most frequently used techniques of conceptual analysis and knowledge representation.Reflective programming languages[edit]In the context of programming languages, reification is the process by which a user program or any aspect of a programming language that was implicit in the translated program and the run-time system, are expressed in the language itself. This process makes it available to the program, which can inspect all these aspects as ordinary data. In reflective languages, reification data is causally connected to the related reified aspect such that a modification to one of them affects the other. Therefore, the reification data is always a faithful representation of the related reified aspect [clarification needed]. Reification data is often said to be made a first class object[citation needed]. Reification, at least partially, has been experienced in many languages to date: in early Lisp dialects and in current Prolog dialects, programs have been treated as data, although the causal connection has often been left to the responsibility of the programmer. In Smalltalk-80, the compiler from the source text to bytecode has been part of the run-time system since the very first implementations of the language.[1]\nThe C programming language reifies the low-level detail of memory addresses.\n\nMany programming language designs encapsulate the details of memory allocation in the compiler and the run-time system. In the design of the C programming language, the memory address is reified and is available for direct manipulation by other language constructs. For example, the following code may be used when implementing a memory-mapped device driver. The buffer pointer is a proxy for the memory address 0xB800000.\n\n\n\n char* buffer = (char*) 0xB800000;\n buffer[0] = 10;\n\n\nFunctional programming languages based on lambda-calculus reify the concept of a procedure abstraction and procedure application in the form of the Lambda expression.\nThe Scheme programming language reifies continuations (approximately, the call stack).\nIn C#, reification is used to make parametric polymorphism implemented as generics as a first-class feature of the language.\nIn the Java programming language, there exist reifiable types that are completely available at run time (i.e. their information is not erased during compilation).[2]\nREBOL reifies code as data and vice versa.\nMany languages, such as Lisp, JavaScript, and Curl, provide an eval or evaluate procedure that effectively reifies the language interpreter.\nThe Logtalk framework for Prolog offers a means to explore reification in the context of logic programming.\nSmalltalk and Actor languages permit the reification of blocks and messages,[3] which are equivalent of lambda expressions in Lisp, and thisContext which is a reification of the current executing block.\nHomoiconic languages reify the syntax of the language itself in the form of an abstract syntax tree, typically together with eval.\nData reification vs. data refinement[edit]Data reification (stepwise refinement) involves finding a more concrete representation of the abstract data types used in a formal specification.Data reification is the terminology of the Vienna Development Method (VDM) that most other people would call data refinement. An example is taking a step towards an implementation by replacing a data representation without a counterpart in the intended implementation language, such as sets, by one that does have a counterpart (such as maps with fixed domains that can be implemented by arrays), or at least one that is closer to having a counterpart, such as sequences. The VDM community prefers the word reification over refinement, as the process has more to do with concretising an idea than with refining it.[4]For similar usages, see Reification (linguistics).In conceptual modeling[edit]Reification is widely used in conceptual modeling.[5] Reifying a relationship means viewing it as an entity. The purpose of reifying a relationship is to make it explicit, when additional information needs to be added to it. Consider the relationship type IsMemberOf(member:Person, Committee). An instance of IsMemberOf is a relationship that represents the fact that a person is a member of a committee. The figure below shows an example population of IsMemberOf relationship in tabular form. Person P1 is a member of committees C1 and C2. Person P2 is a member of committee C1 only.The same fact, however, could also be viewed as an entity. Viewing a relationship as an entity, one can say that the entity reifies the relationship. This is called reification of a relationship. Like any other entity, it must be an instance of an entity type. In the present example, the entity type has been named Membership. For each instance of IsMemberOf, there is one and only one instance of Membership, and vice versa. Now, it becomes possible to add more information to the original relationship. As an example, we can express the fact that person p1 was nominated to be the member of committee c1 by person p2. Reified relationship Membership can be used as the source of a new relationship IsNominatedBy(Membership, Person).For related usages see Reification (knowledge representation).In Unified Modeling Language (UML)[edit]UML provides an association class construct for defining reified relationship types. The association class is a single model element that is both a kind of association and a kind of a class.[6] The association and the entity type that reifies are both the same model element. Note that attributes cannot be reified.On Semantic Web[edit]RDF and OWL[edit]In Semantic Web languages, such as Resource Description Framework (RDF) and Web Ontology Language (OWL), a statement is a binary relation. It is used to link two individuals or an individual and a value. Applications sometimes need to describe other RDF statements, for instance, to record information like when statements were made, or who made them, which is sometimes called provenance information. As an example, we may want to represent properties of a relation, such as our certainty about it, severity or strength of a relation, relevance of a relation, and so on.The example from the conceptual modeling section describes a particular person with URIref person:p1, who is a member of the committee:c1. The RDF triple from that description isConsider to store two further facts: (i) to record who nominated this particular person to this committee (a statement about the membership itself), and (ii) to record who added the fact to the database (a statement about the statement).The first case is a case of classical reification like above in UML: reify the membership and store its attributes and roles etc.:Additionally, RDF provides a built-in vocabulary intended for describing RDF statements. A description of a statement using this vocabulary is called a reification of the statement. The RDF reification vocabulary consists of the type rdf:Statement, and the properties rdf:subject, rdf:predicate, and rdf:object.[7]Using the reification vocabulary, a reification of the statement about the person's membership would be given by assigning the statement a URIref such as committee:membership12345 so that describing statements can be written as follows:These statements say that the resource identified by the URIref committee:membership12345Stat is an RDF statement, that the subject of the statement refers to the resource identified by person:p1, the predicate of the statement refers to the resource identified by committee:isMemberOf, and the object of the statement refers to the resource committee:c1. Assuming that the original statement is actually identified by committee:membership12345, it should be clear by comparing the original statement with the reification that the reification actually does describe it. The conventional use of the RDF reification vocabulary always involves describing a statement using four statements in this pattern. Therefore, they are sometimes referred to as the reification quad.[7]Using reification according to this convention, we could record the fact that person:p3 added the statement to the database byIt is important to note that in the conventional use of reification, the subject of the reification triples is assumed to identify a particular instance of a triple in a particular RDF document, rather than some arbitrary triple having the same subject, predicate, and object. This particular convention is used because reification is intended for expressing properties such as dates of composition and source information, as in the examples given already, and these properties need to be applied to specific instances of triples. Note that the described triple (subject predicate object) itself is not implied by such a reification quad (and it is not necessary that it actually exists in the database). This allows also to use this mechanism to express which triples do not hold.The power of the reification vocabulary in RDF is restricted by the lack of a built-in means for assigning URIrefs to statements, so in order to express provenance information of this kind in RDF, one has to use some mechanism (outside of RDF) to assign URIs to individual RDF statements, then make further statements about those individual statements, using their URIs to identify them.[7]In Topic Maps[edit]In an XML Topic Map (XTM), only a topic can have a name or play a role in an association. One may use an association to make an assertion about a topic, but one cannot directly make assertions about that assertion. However, it is possible to create a topic that reifies a non-topic construct in a map, thus enabling the association to be named and treated as a topic itself.[8]n-ary relations[edit]In Semantic Web languages, such as RDF and OWL, a property is a binary relation used to link two individuals or an individual and a value. However, in some cases, the natural and convenient way to represent certain concepts is to use relations to link an individual to more than just one individual or value. These relations are called n-ary relations. Examples are representing relations among multiple individuals, such as a committee, a person who is a committee member and another person who has nominated the first person to become the committee member, or a buyer, a seller, and an object that was bought when describing a purchase of a book.A more general approach to reification is to create an explicit new class and n new properties to represent an n-ary relation, making an instance of the relation linking the n individuals an instance of this class. This approach can also be used to represent provenance information and other properties for an individual relation instance.[9]Vs. quotation[edit]It is also important to note that the reification described here is not the same as quotation found in other languages. Instead, the reification describes the relationship between a particular instance of a triple and the resources the triple refers to. The reification can be read intuitively as saying this RDF triple talks about these things, rather than (as in quotation) this RDF triple has this form. For instance, in the reification example used in this section, the triple:describing the rdf:subject of the original statement says that the subject of the statement is the resource (the person) identified by the URIref person:p1. It does not state that the subject of the statement is the URIref itself (i.e., a string beginning with certain characters), as quotation would.See also[edit]\nDenotational semantics\nFormal semantics of programming languages\nMeta-circular evaluator\nMetamodeling\nMetaobject\nMetaprogramming\nNormalization by evaluation\nOperational semantics\nReflection (computer science)\nResource Description Framework\nSelf-interpreter\nTopic Maps\nReferences[edit]", "subtitles": ["Reflective programming languages", "Data reification vs. data refinement", "In conceptual modeling", "In Unified Modeling Language (UML)", "On Semantic Web", "See also", "References"], "title": "Reification (computer science)"},
{"content": "Role-oriented programming as a form of computer programming aims at expressing things in terms that are analogous to human conceptual understanding of the World. This should make programs easier to understand and maintain.[citation needed]The main idea of role-oriented programming is that humans think in terms of roles. This claim is often backed up by examples of social relations. For example, a student attending a class and the same student at a party are the same person, yet that person plays two different roles. In particular, the interactions of this person with the outside world depend on his current role. The roles typically share features, e.g., the intrinsic properties of being a person. This sharing of properties is often handled by the delegation mechanism.In the older literature and in the field of databases, it seems[original research?] that there has been little consideration for the context in which roles interplay with each other. Such a context is being established in newer role- and aspect-oriented programming languages such as Object Teams. Compare the use of role as a set of software programs (services) that enable a server to perform specific functions for users or computers on the network in Windows Server jargon.[1]Many[quantify] researchers have argued the advantages of roles in modeling and implementation. Roles allow objects to evolve over time, they enable independent and concurrently existing views (interfaces) of the object, explicating the different contexts of the object, and separating concerns. Generally roles are a natural element of human daily concept-forming. Roles in programming languages enable objects to have changing interfaces, as we see in real life - things change over time, are used differently in different contexts, etc.Authors of role literature[edit]\nBarbara Pernici\nBent Bruun Kristensen[2]\nBruce Wallace\nCharles Bachman[3]\nFriedrich Steimann\nGeorg Gottlob\nKasper B. Graversen\nKasper \u00d8sterbye\nStephan Herrmann\nTrygve Reenskaug[4]\nThomas Ku\u0308hn\nProgramming languages with explicit support for roles[edit]\nChameleon\nEpsilonJ\nJavaScript Delegation - Functions as Roles (Traits and Mixins)\nObject Teams\nPerl 5 (Moose)\nPerl 6\npowerJava\nSCala ROLes Language\nSee also[edit]\nAspect-oriented programming\nData, context and interaction\nObject Oriented Role Analysis Method\nObject-role modeling\nSubject (programming)\nSubject-oriented programming\nTraits (computer science)\nReferences[edit]External links[edit]\nAdaptive Plug-and-Play Components for Evolutionary Software Development, by Mira Mezini and Karl Lieberherr\nContext Aspect Sensitive Services\nOverview and taxonomy of Role languages\nROPE: Role Oriented Programming Environment for Multiagent Systems\n", "subtitles": ["Authors of role literature", "Programming languages with explicit support for roles", "See also", "References", "External links"], "title": "Role-oriented programming"}]